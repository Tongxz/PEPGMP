# 性能优化与自适应指南

本指南是项目性能调优的权威参考，旨在帮助开发者理解系统的核心优化策略，并根据不同硬件和场景进行有效配置。

## 1. 性能调优理念：配置优于编码

本项目的核心性能调优理念是 **“通过配置而非修改代码来适应不同环境”**。绝大多数性能相关的调整都可以通过修改统一配置文件 `config/unified_params.yaml` 和提供命令行（CLI）参数来完成。

调优主要围绕两大核心机制展开：

1.  **性能档位 (Performance Profiles)**：为不同场景预设的参数集合。
2.  **硬件自适应 (Hardware Adaptation)**：自动检测并使用最佳的计算设备和模型后端。

## 2. 核心优化策略与配置

以下是系统内置的主要优化策略，以及如何在 `config/unified_params.yaml` 中配置它们。

### 2.1 性能档位 (Performance Profiles)

系统预设了三种性能档位，以快速适应不同需求。通过修改 `inference.profile` 的值或使用 `--profile` 命令行参数即可切换。

-   **`fast` (速度优先)**
    *   **用途**: 开发、调试、或在低功耗硬件（如笔记本CPU、Apple M系列芯片）上运行。
    *   **配置**: 使用最轻量的模型 (如 `yolov8n.pt`)、较小的图像尺寸 (`imgsz`) 和较低的检测阈值。

-   **`balanced` (平衡模式)**
    *   **用途**: 在中端GPU（如GTX 1660, RTX 3060）上实现性能和精度的平衡。
    *   **配置**: 使用中等大小的模型 (如 `yolov8s.pt`) 和标准的图像尺寸。

-   **`accurate` (精度优先)**
    *   **用途**: 在高端GPU（如RTX 3080/4090）上追求最高检测精度。
    *   **配置**: 使用更大、更精确的模型 (如 `yolov8m.pt`) 和更大的图像尺寸。

**配置示例 (`unified_params.yaml`)**:
```yaml
inference:
  profile: fast  # 默认档位

profiles:
  fast: {}
  balanced:
    human_detection:
      model_path: models/yolo/yolov8s.pt
      imgsz: 640
  accurate:
    human_detection:
      model_path: models/yolo/yolov8m.pt
      imgsz: 736
```
**命令行覆盖**: `python main.py --profile balanced`

### 2.2 硬件设备自适应 (Hardware Device Adaptation)

这是系统性能自适应的基石。

-   **设备自动选择**: 对于所有检测器，当 `device: auto` 时，系统会按 **`mps` → `cuda` → `cpu`** 的优先级自动选择可用的最佳设备，并在启动时打印日志。这解决了之前在有GPU的机器上仍使用CPU的性能瓶颈。

-   **后端自动选择**: 特别是对于姿态检测器，当 `pose_detection.backend: auto` 时，系统会：
    *   在 `cuda` 或 `mps` 设备上，自动选择性能更优的 **`yolov8`** 后端。
    *   在 `cpu` 设备上，自动选择对CPU更友好的 **`mediapipe`** 后端。

-   **强制指定设备**: 你可以通过命令行强制使用特定设备，用于调试或测试。
    ```bash
    # 强制使用CPU运行
    python main.py --device cpu
    ```

### 2.3 批处理 (Batch Processing)

批处理是提升GPU利用率、最大化吞吐量的最有效手段。

-   **启用**: `system.enable_batch_processing: true`
-   **批次大小**: `system.batch_size: 8`

**配置建议**: `batch_size` 的大小应根据GPU显存调整。高端卡（≥12GB）可尝试16或32，中端卡（6-8GB）建议4或8，低端卡（≤4GB）建议2或4。

### 2.4 自动混合精度 (AMP)

对于支持Tensor Core的NVIDIA GPU（GTX 16系列及以后），AMP可以通过使用半精度（FP16）计算来显著提速，同时几乎不损失精度。

-   **启用**: `system.enable_amp: true`

### 2.5 其他优化

-   **跳帧处理**: 对于视频文件，可通过 `runtime.frame_skip: N` 设置每N帧处理一次，以在不严重影响检测效果的情况下提升处理速度。
-   **渲染开销**: 在进行性能测试或后台部署时，使用 `--no-window` 和 `--osd-minimal` 参数可以完全关闭或简化渲染，消除I/O和绘图带来的性能开销。

## 3. 实践指南：如何调优

根据你的目标和硬件，可以参考以下场景进行配置。

#### 场景1：我需要在笔记本/CPU/Mac上进行开发和调试
- **方案**: 使用 `fast` profile，它为低功耗设备优化。
- **命令**: `python main.py --profile fast`
- **预期**: 系统会自动选择 `cpu` 或 `mps`，使用 `yolov8n` 和 `mediapipe`，保证流畅运行。

#### 场景2：我有一张中端NVIDIA显卡（如 RTX 3060），希望平衡速度和精度
- **方案**: 使用 `balanced` profile，并根据显存（如8GB）在 `unified_params.yaml` 中设置 `batch_size: 8`。
- **命令**: `python main.py --profile balanced`
- **预期**: 系统自动使用 `cuda` 设备和 `yolov8s` 模型，达到良好的实时检测效果。

#### 场景3：我有一张高端NVIDIA显卡（如 RTX 4090），追求最高精度
- **方案**: 使用 `accurate` profile，并可进一步调高 `batch_size`（如16或32），甚至启用 `cascade` 级联检测。
- **命令**: `python main.py --profile accurate`
- **预期**: 系统将使用最大、最准的模型，充分利用GPU性能。

#### 场景4：我需要进行纯后台的性能基准测试
- **方案**: 关闭所有渲染和非必要的日志。
- **命令**: `python main.py --profile <your_profile> --no-window --log-level WARNING`

## 4. 后续优化路线图 (Future Optimization Roadmap)

当前系统已通过“性能档位”和“硬件自适应”建立了一个坚实的性能基础。以下路线图旨在指导未来的深度优化工作，以应对更高的性能要求和更复杂的场景。

### 阶段一：系统级并行化 (System-Level Parallelization)

**目标**: 进一步挖掘CPU与GPU的并行潜力，通过异步化和流水线作业，最大限度地减少I/O等待和数据拷贝延迟。

*   **实现并行处理流水线 (Implement Parallel Processing Pipeline)**
    *   **描述**: 将当前单线程的“解码→推理→后处理”流程，重构为多线程流水线架构。
    *   **方案**: 创建独立的“视频解码线程”、“模型推理线程”和“结果处理/可视化线程”，它们之间通过线程安全的队列（`Queue`）进行数据交换。这样可以让CPU的解码工作、GPU的推理工作和CPU的后处理工作实现并行，隐藏等待时间。

*   **集成硬件视频解码 (Integrate Hardware Video Decoding)**
    *   **描述**: 将视频解码的负载从CPU上卸载，释放更多CPU资源给业务逻辑。
    *   **方案**: 引入支持硬件加速的视频处理库（如 `PyAV`），利用NVIDIA的 `NVDEC` 或macOS的 `VideoToolbox` 进行视频解码。

*   **优化数据传输 (Optimize Data Transfer)**
    *   **描述**: 减少CPU和GPU之间的数据传输耗时。
    *   **方案**: 在数据预处理阶段，使用锁页内存（Pinned Memory），以实现更高效的异步数据传输。

### 阶段二：模型与推理引擎深度优化 (Deep Model & Inference Engine Optimization)

**目标**: 极致地压缩模型本身的推理时间，这是提升FPS上限的关键。

*   **TensorRT 集成 (TensorRT Integration)**
    *   **描述**: 对于NVIDIA GPU部署环境，使用TensorRT是实现极致推理性能的业界标准。
    *   **方案**: 将PyTorch模型（`.pt`）首先转换为通用的ONNX格式，然后使用TensorRT将其编译为高度优化的推理引擎。可配合使用INT8量化，通常能带来 **2-3倍** 的推理速度提升。

*   **模型融合 (Model Fusion)**
    *   **描述**: 当前系统需要依次调用人体检测、姿态检测等多个模型。将这些模型融合成一个单一的多头（Multi-head）模型，可以显著减少模型调用和数据传输的开销。
    *   **方案**: 设计并训练一个新的网络，共享一个骨干网络（Backbone），但有多个不同的输出头，分别用于人体、姿态等不同任务。

### 阶段三：极限与分布式优化 (Extreme & Distributed Optimization)

**目标**: 应对超高吞吐量或多路视频流的终极场景。

*   **动态批处理 (Dynamic Batching)**
    *   **描述**: 当前的批处理机制是固定大小的。动态批处理可以根据请求的实时负载，在“延迟”和“吞吐量”之间取得更优的平衡。
    *   **方案**: 实现一个智能批处理协调器，它不仅在批次大小达到阈值时触发推理，也会在一个微小的等待超时（如5-10毫秒）后触发，确保低负载时延迟也极低。

*   **多GPU支持 (Multi-GPU Support)**
    *   **描述**: 在单机多卡的服务器上，将负载平均分配给所有可用的GPU。
    *   **方案**: 最简单的方式是使用PyTorch内置的 `DataParallel`，它能自动将一个大批次分割到多个GPU上。

*   **自定义CUDA核 (Custom CUDA Kernels)**
    *   **描述**: 对于一些Python或PyTorch标准库中性能不佳的特定操作（如复杂的NMS后处理），用C++/CUDA语言重写，并编译为Python可调用的模块。
    *   **方案**: 使用 `torch.utils.cpp_extension` 来加载自定义的CUDA代码，替换性能瓶颈函数。

## 5. 高级调优：环境变量

对于追求极致性能的高端NVIDIA GPU（如RTX 30/40系列），除了调整配置文件，还可以通过设置特定的环境变量来进一步压榨GPU性能。这些配置是从旧的启动脚本中提取的最佳实践。

**推荐配置 (在启动脚本前执行):**

**Linux/macOS:**
```bash
# 优化PyTorch的CUDA内存分配器，减少内存碎片
export PYTORCH_CUDA_ALLOC_CONF='max_split_size_mb:512'

# 异步执行CUDA核，提高并行度
export CUDA_LAUNCH_BLOCKING=0

# 限制并行计算库的线程数，避免CPU竞争
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
```

**Windows (PowerShell):**
```powershell
# 优化PyTorch的CUDA内存分配器
$env:PYTORCH_CUDA_ALLOC_CONF='max_split_size_mb:512'

# 异步执行CUDA核
$env:CUDA_LAUNCH_BLOCKING=0

# 限制并行计算库的线程数
$env:OMP_NUM_THREADS=8
$env:MKL_NUM_THREADS=8
```

**说明:**
- `PYTORCH_CUDA_ALLOC_CONF`: 调整PyTorch的内存分配策略。`max_split_size_mb`可以限制内存碎片，对于长期运行的服务有帮助。
- `CUDA_LAUNCH_BLOCKING=0`: 确保CUDA核的执行是异步的，这是获取高性能的默认和推荐方式。
- `OMP_NUM_THREADS` / `MKL_NUM_THREADS`: 在多核CPU上，限制这些库的线程数有时可以避免它们与Python主线程或数据加载线程竞争资源，从而稳定甚至提高性能。

在进行基准测试或生产部署时，可以尝试应用这些环境变量。
