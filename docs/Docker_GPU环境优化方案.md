# Docker + GPUç¯å¢ƒä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

æœ¬æ–¹æ¡ˆä¸“é—¨é’ˆå¯¹Docker+GPUç”Ÿäº§ç¯å¢ƒï¼Œæä¾›CUDAã€cuDNNã€TensorRTçš„å®Œæ•´ä¼˜åŒ–é…ç½®å’Œä½¿ç”¨æŒ‡å—ã€‚

### æ ¸å¿ƒä¼˜åŠ¿
- âœ… **Dockerå®¹å™¨åŒ–**: ç¯å¢ƒä¸€è‡´æ€§å’Œå¯ç§»æ¤æ€§
- âœ… **GPUåŠ é€Ÿ**: CUDAã€cuDNNã€TensorRTå®Œæ•´æ”¯æŒ
- âœ… **ç”Ÿäº§å°±ç»ª**: ä¼˜åŒ–çš„Dockerfileå’Œé…ç½®
- âœ… **æ€§èƒ½æå‡**: 5-10å€æ¨ç†é€Ÿåº¦æå‡

---

## ğŸ¯ ä¸€ã€Docker + GPUç¯å¢ƒæ¶æ„

### 1.1 ç¯å¢ƒç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Host (Ubuntu 22.04)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   NVIDIA Driver (â‰¥470.57.02)         â”‚  â”‚
â”‚  â”‚   CUDA Toolkit (12.4.0)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   NVIDIA Container Toolkit           â”‚  â”‚
â”‚  â”‚   - nvidia-container-runtime         â”‚  â”‚
â”‚  â”‚   - nvidia-container-cli             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Docker Engine (20.10+)             â”‚  â”‚
â”‚  â”‚   - GPU Support                      â”‚  â”‚
â”‚  â”‚   - CUDA Runtime                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Application Container              â”‚  â”‚
â”‚  â”‚   - PyTorch + CUDA                   â”‚  â”‚
â”‚  â”‚   - TensorRT                         â”‚  â”‚
â”‚  â”‚   - cuDNN                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒæŠ€æœ¯æ ˆ

| ç»„ä»¶ | ç‰ˆæœ¬ | ä½œç”¨ |
|------|------|------|
| **NVIDIA Driver** | â‰¥470.57.02 | GPUé©±åŠ¨ |
| **CUDA Toolkit** | 12.4.0 | GPUè®¡ç®—å¹³å° |
| **cuDNN** | 8.9.0 | æ·±åº¦å­¦ä¹ åŠ é€Ÿ |
| **TensorRT** | 8.6.0 | æ¨ç†ä¼˜åŒ–å¼•æ“ |
| **PyTorch** | 2.2.0+ | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| **Docker** | 20.10+ | å®¹å™¨åŒ–å¹³å° |
| **NVIDIA Container Toolkit** | 1.13+ | GPUå®¹å™¨æ”¯æŒ |

---

## ğŸ³ äºŒã€Dockerfileä¼˜åŒ–

### 2.1 åŸºç¡€é•œåƒé€‰æ‹©

#### æ–¹æ¡ˆAï¼šå®˜æ–¹CUDAé•œåƒï¼ˆæ¨èï¼‰

```dockerfile
# ä½¿ç”¨NVIDIAå®˜æ–¹CUDAé•œåƒ
ARG CUDA_VERSION=12.4.0
ARG UBUNTU_VERSION=22.04
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu${UBUNTU_VERSION}

# ä¼˜åŠ¿ï¼š
# - å®˜æ–¹ç»´æŠ¤ï¼Œç¨³å®šå¯é 
# - é¢„è£…CUDA Runtime
# - æ”¯æŒå¤šæ¶æ„ï¼ˆamd64, arm64ï¼‰
# - è‡ªåŠ¨å¤„ç†GPUè®¾å¤‡æ˜ å°„
```

#### æ–¹æ¡ˆBï¼šPyTorchå®˜æ–¹é•œåƒ

```dockerfile
# ä½¿ç”¨PyTorchå®˜æ–¹é•œåƒ
FROM pytorch/pytorch:2.2.0-cuda12.4-cudnn8-runtime

# ä¼˜åŠ¿ï¼š
# - é¢„è£…PyTorchå’ŒCUDA
# - åŒ…å«cuDNN
# - å¼€ç®±å³ç”¨
```

### 2.2 ä¼˜åŒ–çš„Dockerfile

```dockerfile
# =================================================================
# é˜¶æ®µ1: æ„å»ºå™¨ (Builder)
# =================================================================
ARG CUDA_VERSION=12.4.0
ARG UBUNTU_VERSION=22.04
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION} AS builder

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3.10-venv \
    python3-pip \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
RUN python3.10 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å‡çº§pip
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt pyproject.toml ./

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å®‰è£…TensorRT
RUN pip install --no-cache-dir nvidia-tensorrt

# =================================================================
# é˜¶æ®µ2: è¿è¡Œæ—¶é•œåƒ (Runtime)
# =================================================================
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu${UBUNTU_VERSION}

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    libpython3.10 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 appuser

WORKDIR /app

# ä»æ„å»ºå™¨å¤åˆ¶è™šæ‹Ÿç¯å¢ƒ
COPY --from=builder --chown=appuser:appuser /opt/venv /opt/venv

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=appuser:appuser . .

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER appuser

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PATH="/opt/venv/bin:$PATH"
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# CUDAç¯å¢ƒå˜é‡
ENV CUDA_VISIBLE_DEVICES=0
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# PyTorch CUDAä¼˜åŒ–
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,roundup_power2_divisions:16
ENV TORCH_CUDNN_V8_API_ENABLED=1

# TensorRTä¼˜åŒ–
ENV TENSORRT_LOGGER_LEVEL=WARNING

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "main.py", "--mode", "api", "--host", "0.0.0.0"]
```

### 2.3 å¤šé˜¶æ®µæ„å»ºä¼˜åŒ–

```dockerfile
# =================================================================
# é˜¶æ®µ1: åŸºç¡€ç¯å¢ƒ
# =================================================================
FROM nvidia/cuda:12.4.0-devel-ubuntu22.04 AS base

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    python3.10-venv \
    python3-pip \
    build-essential \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# =================================================================
# é˜¶æ®µ2: ä¾èµ–æ„å»º
# =================================================================
FROM base AS dependencies

WORKDIR /app

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
RUN python3.10 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt pyproject.toml ./

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir -r requirements.txt

# =================================================================
# é˜¶æ®µ3: TensorRTæ„å»º
# =================================================================
FROM dependencies AS tensorrt

# å®‰è£…TensorRT
RUN pip install --no-cache-dir nvidia-tensorrt && \
    pip install --no-cache-dir torch-tensorrt

# =================================================================
# é˜¶æ®µ4: æœ€ç»ˆé•œåƒ
# =================================================================
FROM nvidia/cuda:12.4.0-runtime-ubuntu22.04

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 \
    libpython3.10 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºç”¨æˆ·
RUN useradd -m -u 1000 appuser

WORKDIR /app

# å¤åˆ¶è™šæ‹Ÿç¯å¢ƒ
COPY --from=tensorrt --chown=appuser:appuser /opt/venv /opt/venv

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=appuser:appuser . .

# åˆ‡æ¢ç”¨æˆ·
USER appuser

# ç¯å¢ƒå˜é‡
ENV PATH="/opt/venv/bin:$PATH"
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,roundup_power2_divisions:16
ENV TORCH_CUDNN_V8_API_ENABLED=1

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "main.py", "--mode", "api", "--host", "0.0.0.0"]
```

---

## ğŸš€ ä¸‰ã€Docker Composeé…ç½®

### 3.1 GPUæ”¯æŒé…ç½®

```yaml
version: "3.8"

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: 12.4.0
        UBUNTU_VERSION: 22.04

    # GPUæ”¯æŒé…ç½®
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # ç¯å¢ƒå˜é‡
    environment:
      - ENVIRONMENT=production
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,roundup_power2_divisions:16
      - TORCH_CUDNN_V8_API_ENABLED=1
      - TENSORRT_LOGGER_LEVEL=WARNING

    ports:
      - "8000:8000"

    volumes:
      - ./config:/app/config:ro
      - ./logs:/app/logs
      - ./output:/app/output
      - ./models:/app/models:ro

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    restart: unless-stopped
```

### 3.2 å®Œæ•´ç”Ÿäº§é…ç½®

```yaml
version: "3.8"

networks:
  pyt-prod-network:
    driver: bridge

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local

services:
  # PostgreSQLæ•°æ®åº“
  database:
    image: postgres:16-alpine
    container_name: pyt-postgres-prod
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-pyt_production}
      POSTGRES_USER: ${POSTGRES_USER:-pyt_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_me}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    networks:
      - pyt-prod-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-pyt_user} -d ${POSTGRES_DB:-pyt_production}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: pyt-redis-prod
    command: >
      redis-server
      --appendonly yes
      --requirepass ${REDIS_PASSWORD:-change_me}
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "${REDIS_PORT:-6379}:6379"
    networks:
      - pyt-prod-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  # åç«¯API (GPUåŠ é€Ÿ)
  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: ${CUDA_VERSION:-12.4.0}
        UBUNTU_VERSION: ${UBUNTU_VERSION:-22.04}

    container_name: pyt-api-prod

    # GPUæ”¯æŒ
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    environment:
      # åº”ç”¨é…ç½®
      - ENVIRONMENT=production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

      # æ•°æ®åº“é…ç½®
      - DATABASE_URL=postgresql://${POSTGRES_USER:-pyt_user}:${POSTGRES_PASSWORD:-change_me}@database:5432/${POSTGRES_DB:-pyt_production}

      # Redisé…ç½®
      - REDIS_URL=redis://:${REDIS_PASSWORD:-change_me}@redis:6379/0

      # å®‰å…¨é…ç½®
      - SECRET_KEY=${SECRET_KEY:-change_me}
      - JWT_SECRET=${JWT_SECRET:-change_me}

      # CUDAé…ç½®
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # PyTorch CUDAä¼˜åŒ–
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,roundup_power2_divisions:16
      - TORCH_CUDNN_V8_API_ENABLED=1

      # TensorRTé…ç½®
      - TENSORRT_LOGGER_LEVEL=WARNING
      - TENSORRT_ENABLED=${TENSORRT_ENABLED:-true}
      - TENSORRT_PRECISION=${TENSORRT_PRECISION:-fp16}

      # æ€§èƒ½ä¼˜åŒ–
      - BATCH_SIZE=${BATCH_SIZE:-8}
      - ENABLE_AMP=${ENABLE_AMP:-true}
      - ENABLE_TTA=${ENABLE_TTA:-false}

    ports:
      - "${API_PORT:-8000}:8000"

    volumes:
      - ./config:/app/config:ro
      - ./logs:/app/logs
      - ./output:/app/output
      - ./models:/app/models:ro
      - ./data:/app/data

    networks:
      - pyt-prod-network

    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    restart: unless-stopped

  # å‰ç«¯
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
      args:
        VITE_API_BASE: ${VITE_API_BASE:-/api/v1}
        BASE_URL: ${BASE_URL:-/}

    container_name: pyt-frontend-prod

    ports:
      - "${FRONTEND_PORT:-8080}:80"

    networks:
      - pyt-prod-network

    depends_on:
      api:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

    restart: unless-stopped
```

---

## âš¡ å››ã€CUDAä¼˜åŒ–é…ç½®

### 4.1 ç¯å¢ƒå˜é‡ä¼˜åŒ–

```bash
# .env æ–‡ä»¶
# ============================================
# CUDAé…ç½®
# ============================================
CUDA_VISIBLE_DEVICES=0
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# PyTorch CUDAä¼˜åŒ–
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,roundup_power2_divisions:16
TORCH_CUDNN_V8_API_ENABLED=1

# CUDAæµä¼˜åŒ–
CUDA_LAUNCH_BLOCKING=0
CUDA_MODULE_LOADING=LAZY

# cuBLASä¼˜åŒ–
CUBLAS_WORKSPACE_CONFIG=:16:8

# TensorRTé…ç½®
TENSORRT_LOGGER_LEVEL=WARNING
TENSORRT_ENABLED=true
TENSORRT_PRECISION=fp16
```

### 4.2 è¿è¡Œæ—¶ä¼˜åŒ–

```python
# src/utils/gpu_acceleration.py
import torch
import os

def configure_cuda_optimizations():
    """é…ç½®CUDAä¼˜åŒ–"""
    if not torch.cuda.is_available():
        return

    # 1. ç¯å¢ƒå˜é‡è®¾ç½®
    os.environ.setdefault("CUDA_LAUNCH_BLOCKING", "0")
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:512,roundup_power2_divisions:16")
    os.environ.setdefault("TORCH_CUDNN_V8_API_ENABLED", "1")

    # 2. CuDNNä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

    # 3. TF32ä¼˜åŒ– (Ampereæ¶æ„)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # 4. å†…å­˜ç®¡ç†
    torch.cuda.empty_cache()

    print(f"âœ… CUDAä¼˜åŒ–é…ç½®å®Œæˆ")
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
```

---

## ğŸ”¥ äº”ã€TensorRTé›†æˆ

### 5.1 TensorRTä¼˜åŒ–å™¨

```python
# src/optimization/tensorrt_optimizer.py
import torch
import torch_tensorrt
import logging
from typing import Optional, Dict, Any
import os

logger = logging.getLogger(__name__)

class TensorRTOptimizer:
    """TensorRTä¼˜åŒ–å™¨"""

    def __init__(self, model, input_shape=(1, 3, 640, 640)):
        self.model = model
        self.input_shape = input_shape
        self.optimized_model = None
        self.enabled = os.getenv("TENSORRT_ENABLED", "false").lower() == "true"
        self.precision = os.getenv("TENSORRT_PRECISION", "fp16")

    def optimize(self, precision: Optional[str] = None, workspace_size: int = 1<<30):
        """ä¼˜åŒ–æ¨¡å‹"""
        if not self.enabled:
            logger.info("TensorRTæœªå¯ç”¨ï¼Œè·³è¿‡ä¼˜åŒ–")
            return None

        precision = precision or self.precision
        logger.info(f"å¼€å§‹TensorRTä¼˜åŒ–ï¼Œç²¾åº¦: {precision}")

        try:
            # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
            example_input = torch.randn(*self.input_shape).cuda()

            # ç¼–è¯‘é…ç½®
            enabled_precisions = set()
            if precision == 'fp16':
                enabled_precisions = {torch.half}
            elif precision == 'int8':
                enabled_precisions = {torch.int8}

            # ç¼–è¯‘æ¨¡å‹
            self.optimized_model = torch_tensorrt.compile(
                self.model,
                inputs=[example_input],
                enabled_precisions=enabled_precisions,
                workspace_size=workspace_size,
                min_block_size=7,
                truncate_long_and_double=True,
            )

            logger.info(f"âœ… TensorRTä¼˜åŒ–å®Œæˆï¼Œç²¾åº¦: {precision}")
            return self.optimized_model

        except Exception as e:
            logger.error(f"TensorRTä¼˜åŒ–å¤±è´¥: {e}")
            return None

    def save(self, path: str):
        """ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹"""
        if self.optimized_model is not None:
            torch.jit.save(self.optimized_model, path)
            logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
        else:
            logger.error("æ¨¡å‹æœªä¼˜åŒ–ï¼Œæ— æ³•ä¿å­˜")

    def load(self, path: str):
        """åŠ è½½ä¼˜åŒ–åçš„æ¨¡å‹"""
        self.optimized_model = torch.jit.load(path)
        logger.info(f"æ¨¡å‹å·²ä»{path}åŠ è½½")
        return self.optimized_model
```

### 5.2 é›†æˆåˆ°æ£€æµ‹å™¨

```python
# src/detection/detector.py
from src.optimization.tensorrt_optimizer import TensorRTOptimizer

class HumanDetector(BaseDetector):
    """äººä½“æ£€æµ‹å™¨ - æ”¯æŒTensorRTä¼˜åŒ–"""

    def __init__(self, model_path: Optional[str] = None, device: str = "auto"):
        super().__init__(model_path or "models/yolo/yolov8s.pt", device)

        # TensorRTä¼˜åŒ–
        self.tensorrt_optimizer = TensorRTOptimizer(
            self.model,
            input_shape=(1, 3, 640, 640)
        )

        if self.tensorrt_optimizer.enabled:
            logger.info("å¯ç”¨TensorRTä¼˜åŒ–")
            self.model = self.tensorrt_optimizer.optimize()

    def detect(self, image: np.ndarray) -> List[Dict]:
        """æ‰§è¡Œæ£€æµ‹"""
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")

        # ä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡å‹è¿›è¡Œæ¨ç†
        with torch.no_grad():
            results = self.model(image)

        # åå¤„ç†
        detections = self._postprocess(results)

        return detections
```

### 5.3 è‡ªåŠ¨ä¼˜åŒ–è„šæœ¬

```python
# scripts/optimization/auto_tensorrt_optimization.py
import torch
from src.optimization.tensorrt_optimizer import TensorRTOptimizer
from src.detection.detector import HumanDetector
from src.detection.yolo_hairnet_detector import YOLOHairnetDetector
import os

def auto_optimize_models():
    """è‡ªåŠ¨ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è‡ªåŠ¨TensorRTä¼˜åŒ–...")

    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["TENSORRT_ENABLED"] = "true"
    os.environ["TENSORRT_PRECISION"] = "fp16"

    # 1. ä¼˜åŒ–äººä½“æ£€æµ‹æ¨¡å‹
    print("\n1. ä¼˜åŒ–äººä½“æ£€æµ‹æ¨¡å‹...")
    human_detector = HumanDetector()
    if human_detector.model is not None:
        optimizer = TensorRTOptimizer(
            human_detector.model,
            input_shape=(1, 3, 640, 640)
        )
        optimized_model = optimizer.optimize(precision='fp16')
        if optimized_model:
            optimizer.save("models/tensorrt/human_detection_fp16.trt")
            print("   âœ… äººä½“æ£€æµ‹æ¨¡å‹ä¼˜åŒ–å®Œæˆ")

    # 2. ä¼˜åŒ–å‘ç½‘æ£€æµ‹æ¨¡å‹
    print("\n2. ä¼˜åŒ–å‘ç½‘æ£€æµ‹æ¨¡å‹...")
    hairnet_detector = YOLOHairnetDetector()
    if hairnet_detector.model is not None:
        optimizer = TensorRTOptimizer(
            hairnet_detector.model,
            input_shape=(1, 3, 224, 224)
        )
        optimized_model = optimizer.optimize(precision='fp16')
        if optimized_model:
            optimizer.save("models/tensorrt/hairnet_detection_fp16.trt")
            print("   âœ… å‘ç½‘æ£€æµ‹æ¨¡å‹ä¼˜åŒ–å®Œæˆ")

    print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹ä¼˜åŒ–å®Œæˆï¼")

if __name__ == "__main__":
    auto_optimize_models()
```

---

## ğŸ“Š å…­ã€æ€§èƒ½åŸºå‡†æµ‹è¯•

### 6.1 åŸºå‡†æµ‹è¯•è„šæœ¬

```python
# scripts/benchmark/gpu_benchmark.py
import time
import torch
from src.utils.gpu_acceleration import initialize_gpu_acceleration
from src.detection.detector import HumanDetector
from src.optimization.tensorrt_optimizer import TensorRTOptimizer

def benchmark_all():
    """å…¨é¢åŸºå‡†æµ‹è¯•"""
    results = {}

    # åˆå§‹åŒ–GPU
    gpu_info = initialize_gpu_acceleration()
    print(f"GPU: {gpu_info['gpu_name']}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print()

    # å‡†å¤‡è¾“å…¥
    input_shape = (1, 3, 640, 640)
    input_data = torch.randn(*input_shape).cuda()

    # æµ‹è¯•1: PyTorch FP32
    print("æµ‹è¯• PyTorch FP32...")
    model_fp32 = HumanDetector().model.cuda()
    model_fp32.eval()

    avg_time, fps = benchmark_model(model_fp32, input_data)
    results['PyTorch FP32'] = {'time': avg_time, 'fps': fps}
    print(f"  å»¶è¿Ÿ: {avg_time:.2f}ms, FPS: {fps:.1f}")

    # æµ‹è¯•2: PyTorch FP16
    print("æµ‹è¯• PyTorch FP16...")
    model_fp16 = model_fp32.half()

    avg_time, fps = benchmark_model(model_fp16, input_data.half())
    results['PyTorch FP16'] = {'time': avg_time, 'fps': fps}
    print(f"  å»¶è¿Ÿ: {avg_time:.2f}ms, FPS: {fps:.1f}")

    # æµ‹è¯•3: TensorRT FP32
    print("æµ‹è¯• TensorRT FP32...")
    optimizer_fp32 = TensorRTOptimizer(model_fp32)
    model_trt_fp32 = optimizer_fp32.optimize(precision='fp32')

    if model_trt_fp32:
        avg_time, fps = benchmark_model(model_trt_fp32, input_data)
        results['TensorRT FP32'] = {'time': avg_time, 'fps': fps}
        print(f"  å»¶è¿Ÿ: {avg_time:.2f}ms, FPS: {fps:.1f}")

    # æµ‹è¯•4: TensorRT FP16
    print("æµ‹è¯• TensorRT FP16...")
    optimizer_fp16 = TensorRTOptimizer(model_fp32)
    model_trt_fp16 = optimizer_fp16.optimize(precision='fp16')

    if model_trt_fp16:
        avg_time, fps = benchmark_model(model_trt_fp16, input_data)
        results['TensorRT FP16'] = {'time': avg_time, 'fps': fps}
        print(f"  å»¶è¿Ÿ: {avg_time:.2f}ms, FPS: {fps:.1f}")

    # ç”ŸæˆæŠ¥å‘Š
    print("\næ€§èƒ½å¯¹æ¯”æŠ¥å‘Š:")
    print("-" * 60)
    print(f"{'æ¨¡å‹':<20} {'å»¶è¿Ÿ(ms)':<15} {'FPS':<15} {'é€Ÿåº¦æå‡':<15}")
    print("-" * 60)

    baseline_fps = results['PyTorch FP32']['fps']
    for name, metrics in results.items():
        speedup = metrics['fps'] / baseline_fps
        print(f"{name:<20} {metrics['time']:<15.2f} {metrics['fps']:<15.1f} {speedup:<15.2f}x")

    return results

def benchmark_model(model, input_data, num_iterations=100):
    """åŸºå‡†æµ‹è¯•æ¨¡å‹"""
    # é¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            _ = model(input_data)

    torch.cuda.synchronize()

    # æµ‹è¯•
    start = time.time()
    for _ in range(num_iterations):
        with torch.no_grad():
            _ = model(input_data)
    torch.cuda.synchronize()
    end = time.time()

    avg_time = (end - start) / num_iterations * 1000  # ms
    fps = 1000 / avg_time

    return avg_time, fps

if __name__ == "__main__":
    results = benchmark_all()
```

### 6.2 é¢„æœŸæ€§èƒ½

| ä¼˜åŒ–æ–¹æ¡ˆ | å»¶è¿Ÿ (ms) | FPS | é€Ÿåº¦æå‡ | ç²¾åº¦å½±å“ |
|----------|-----------|-----|----------|----------|
| PyTorch FP32 | 35 | 28.6 | 1.0x | åŸºå‡† |
| PyTorch FP16 | 22 | 45.5 | 1.6x | Â±0.1% |
| TensorRT FP32 | 12 | 83.3 | 2.9x | æ—  |
| TensorRT FP16 | 6 | 166.7 | 5.8x | Â±0.1% |
| TensorRT INT8 | 4 | 250.0 | 8.7x | -1-3% |

---

## ğŸš€ ä¸ƒã€éƒ¨ç½²æµç¨‹

### 7.1 å®Œæ•´éƒ¨ç½²æ­¥éª¤

```bash
# 1. æ£€æŸ¥GPUç¯å¢ƒ
nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.4.0-runtime-ubuntu22.04 nvidia-smi

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶

# 3. æ„å»ºé•œåƒ
docker compose -f docker-compose.prod.full.yml build

# 4. å¯åŠ¨æœåŠ¡
docker compose -f docker-compose.prod.full.yml up -d

# 5. æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose.prod.full.yml logs -f api

# 6. éªŒè¯æœåŠ¡
curl http://localhost:8000/health

# 7. è¿è¡ŒåŸºå‡†æµ‹è¯•
docker exec pyt-api-prod python scripts/benchmark/gpu_benchmark.py
```

### 7.2 ä¼˜åŒ–æ¨¡å‹

```bash
# 1. è¿›å…¥å®¹å™¨
docker exec -it pyt-api-prod bash

# 2. è¿è¡Œè‡ªåŠ¨ä¼˜åŒ–
python scripts/optimization/auto_tensorrt_optimization.py

# 3. éªŒè¯ä¼˜åŒ–åçš„æ¨¡å‹
python scripts/benchmark/gpu_benchmark.py

# 4. é‡å¯æœåŠ¡
exit
docker compose -f docker-compose.prod.full.yml restart api
```

---

## ğŸ“ˆ å…«ã€ç›‘æ§å’Œè°ƒä¼˜

### 8.1 GPUç›‘æ§

```bash
# å®æ—¶ç›‘æ§GPU
watch -n 1 nvidia-smi

# Dockerå®¹å™¨GPUä½¿ç”¨
docker stats pyt-api-prod
```

### 8.2 æ€§èƒ½è°ƒä¼˜

```yaml
# docker-compose.prod.full.yml
api:
  environment:
    # æ‰¹å¤„ç†å¤§å°
    - BATCH_SIZE=8

    # æ··åˆç²¾åº¦
    - ENABLE_AMP=true

    # TensorRT
    - TENSORRT_ENABLED=true
    - TENSORRT_PRECISION=fp16

    # æµ‹è¯•æ—¶å¢å¼º
    - ENABLE_TTA=false
```

---

## ğŸ¯ ä¹ã€æœ€ä½³å®è·µ

### 9.1 Dockeré•œåƒä¼˜åŒ–

- âœ… ä½¿ç”¨å¤šé˜¶æ®µæ„å»º
- âœ… ä½¿ç”¨å®˜æ–¹CUDAé•œåƒ
- âœ… æœ€å°åŒ–é•œåƒå¤§å°
- âœ… ä½¿ç”¨érootç”¨æˆ·
- âœ… è®¾ç½®å¥åº·æ£€æŸ¥

### 9.2 GPUä¼˜åŒ–

- âœ… å¯ç”¨cuDNN benchmark
- âœ… ä½¿ç”¨TF32ç²¾åº¦
- âœ… ä¼˜åŒ–å†…å­˜åˆ†é…
- âœ… å¯ç”¨TensorRT
- âœ… ä½¿ç”¨FP16æ¨ç†

### 9.3 ç”Ÿäº§é…ç½®

- âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
- âœ… å¯ç”¨æ—¥å¿—è½®è½¬
- âœ… è®¾ç½®èµ„æºé™åˆ¶
- âœ… é…ç½®å¥åº·æ£€æŸ¥
- âœ… å¯ç”¨è‡ªåŠ¨é‡å¯

---

## ğŸ‰ æ€»ç»“

### å¿«é€Ÿéƒ¨ç½²å‘½ä»¤

```bash
# 1. é…ç½®ç¯å¢ƒ
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶

# 2. å¯åŠ¨æœåŠ¡
docker compose -f docker-compose.prod.full.yml up -d

# 3. ä¼˜åŒ–æ¨¡å‹
docker exec pyt-api-prod python scripts/optimization/auto_tensorrt_optimization.py

# 4. éªŒè¯æ€§èƒ½
docker exec pyt-api-prod python scripts/benchmark/gpu_benchmark.py

# 5. æŸ¥çœ‹æ—¥å¿—
docker compose -f docker-compose.prod.full.yml logs -f api
```

### æ€§èƒ½æå‡æ€»ç»“

| ä¼˜åŒ–é¡¹ | æ€§èƒ½æå‡ | å®æ–½éš¾åº¦ |
|--------|----------|----------|
| CUDAåŸºç¡€ä¼˜åŒ– | 2-3å€ | ä½ |
| cuDNNä¼˜åŒ– | +30-50% | ä½ |
| TensorRT FP16 | 5-10å€ | ä¸­ |
| å®Œæ•´ä¼˜åŒ– | 10-20å€ | ä¸­ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-15
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ
