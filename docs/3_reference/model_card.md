# 模型卡 (Model Cards)

本文档为项目中所使用的核心AI模型提供标准化的说明，旨在提高模型的透明度、可理解性和可复现性。每一张“模型卡”都是一个模型的“身份证”。

---

## 1. 人体检测模型 (Human Detector)

- **模型名称**: YOLOv8 (主要使用 Small/Medium 版本)
- **模型种类**: 目标检测 (Object Detection)
- **版本**: v8
- **文件路径**: `models/yolo/yolov8s.pt` (默认), `yolov8m.pt` (高精度)

### 1.1. 用途与范围

- **主要用途**: 在输入的图像或视频帧中，实时检测并定位所有“人”的位置。
- **下游依赖**: 此模型是整个检测管线的第一步，后续的发网检测、姿态估计和行为识别都强依赖于此模型的输出。
- **适用场景**: 适用于各种室内场景，如工厂车间、厨房、实验室等。

### 1.2. 性能指标

| 性能档位 | 模型版本 | mAP@0.5 | 速度 (GPU, T4) | 适用场景 |
| :--- | :--- | :--- | :--- | :--- |
| `fast` | YOLOv8s | ~92% | ~30-35 FPS | 需要高吞吐量的实时监控 |
| `balanced` | YOLOv8s | ~93% | ~25-30 FPS | 性能与精度的最佳平衡 |
| `accurate` | YOLOv8m | ~95% | ~15-20 FPS | 对检测精度有极高要求 |

### 1.3. 训练数据

- **基础模型**: 在大规模公开数据集（如 COCO）上进行了预训练。
- **微调**: 可能在项目的特定场景数据上进行了微调，以提高对特定角度、光照或遮挡情况的鲁棒性。

### 1.4. 限制与偏见

- **遮挡问题**: 对于严重被遮挡的人体，检测置信度可能会下降或导致漏检。
- **小目标**: 对于在图像中占比较极小的远距离人体，可能无法有效检测。
- **人群密集**: 在极端拥挤的场景下，检测框可能出现重叠或合并，影响计数准确性。

---

## 2. 发网检测模型 (Hairnet Detector)

- **模型名称**: YOLOv8-Custom
- **模型种类**: 目标检测 (Object Detection)
- **版本**: 自定义训练 v1.1
- **文件路径**: `models/hairnet_detection/hairnet_detection.pt`

### 2.1. 用途与范围

- **主要用途**: 检测指定区域（通常是人头区域）是否佩戴了发网。
- **检测类别**: `hairnet` (发网), `head` (头部), `person` (人员)。模型被训练为端到端检测，但主要使用其`hairnet`类别的输出来判断合规性。
- **适用场景**: 主要用于需要佩戴发网的洁净室、食品加工车间等合规性检查场景。

### 2.2. 性能指标

- **准确率 (Accuracy)**: 85% - 90%
- **精确率 (Precision)**: 88% - 92%
- **召回率 (Recall)**: 82% - 88%
- **处理速度 (GPU, T4)**: ~15-25 FPS (在头部区域的小图上进行推理，速度很快)

### 2.3. 训练数据

- **数据集**: 使用项目内部收集和标注的私有数据集进行训练。
- **数据增强**: 训练过程中应用了包括旋转、缩放、色彩抖动在内的多种数据增强技术，以提高模型的泛化能力。
- **标注规范**: 遵循YOLOv8格式，对`hairnet`, `head`, `person`三个类别进行标注。

### 2.4. 限制与偏见

- **颜色和样式**: 模型可能对训练数据中未出现过的、颜色或样式特别的发网敏感度较低。
- **头部姿态**: 在极端的头部俯仰或旋转角度下，检测性能可能会下降。
- **与头发颜色的混淆**: 在某些光照下，浅色头发可能会对检测结果产生轻微干扰。

---

## 3. 行为识别模型 (Behavior Recognizer)

- **模型名称**: XGBoost Classifier
- **模型种类**: 时序数据分类器 (Time-series Classifier)
- **版本**: v2.0
- **文件路径**: `models/handwash_xgb.json`

### 3.1. 用途与范围

- **主要用途**: 基于上游模型输出的人体姿态关键点序列，识别特定行为，如“洗手”、“消毒”。
- **输入特征**: **不直接处理图像**。其输入是经过处理的、连续30帧的姿态关键点时序特征（如手部速度、加速度、相对位置等）。
- **适用场景**: 用于需要进行过程合规性判断的场景，例如进入车间前的洗手、消毒流程。

### 3.2. 性能指标

- **洗手识别准确率**: 80% - 88%
- **消毒识别准确率**: 75% - 85%
- **F1 分数**: 0.80 - 0.87
- **处理速度**: 极快，通常在1ms以内，因为只处理特征数据，不涉及图像计算。

### 3.3. 训练数据

- **数据集**: 基于视频标注数据生成。先通过姿态估计算法提取所有视频帧的关键点，然后将这些时序特征与人工标注的行为（洗手、消毒、无行为）进行匹配，形成训练样本。

### 3.4. 限制与偏见

- **上游依赖性**: 模型的性能**严重依赖**上游姿态估计模型（YOLOv8-Pose / MediaPipe）的准确性。如果关键点检测错误或抖动，将直接导致行为误判。
- **动作幅度**: 对于动作幅度过小或过快的非标准行为，识别准确率会下降。
- **场景泛化**: 模型可能对特定场景下的动作模式学习存在过拟合，在新场景下可能需要微调。
- **融合策略**: 当前采用机器学习（XGBoost）与规则判断相融合的策略，`ml_fusion_alpha`参数的设置会影响最终决策的偏向。
