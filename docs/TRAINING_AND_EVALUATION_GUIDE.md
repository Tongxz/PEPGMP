# æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [å‘ç½‘æ£€æµ‹æ¨¡å‹è®­ç»ƒ](#å‘ç½‘æ£€æµ‹æ¨¡å‹è®­ç»ƒ)
2. [æ‰‹éƒ¨æ£€æµ‹æ¨¡å‹è®­ç»ƒ](#æ‰‹éƒ¨æ£€æµ‹æ¨¡å‹è®­ç»ƒ)
3. [è¯„ä¼°è®­ç»ƒç¨‹åº¦](#è¯„ä¼°è®­ç»ƒç¨‹åº¦)
4. [åŠ å¼ºè®­ç»ƒçš„æ–¹æ³•](#åŠ å¼ºè®­ç»ƒçš„æ–¹æ³•)
5. [è¯„ä¼°æŒ‡æ ‡è¯¦è§£](#è¯„ä¼°æŒ‡æ ‡è¯¦è§£)

---

## ğŸ¯ å‘ç½‘æ£€æµ‹æ¨¡å‹è®­ç»ƒ

### 1. è®­ç»ƒè„šæœ¬ä½¿ç”¨

**è„šæœ¬ä½ç½®**: `scripts/training/train_hairnet_model.py`

**åŸºæœ¬ç”¨æ³•**:
```bash
python scripts/training/train_hairnet_model.py \
    --data datasets/hairnet/data.yaml \
    --epochs 150 \
    --batch-size 16 \
    --img-size 640 \
    --pretrained \
    --device cuda:0
```

**å‚æ•°è¯´æ˜**:
- `--data`: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆYAMLæ ¼å¼ï¼ŒåŒ…å«train/valè·¯å¾„å’Œç±»åˆ«ä¿¡æ¯ï¼‰
- `--epochs`: è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®100-200ï¼‰
- `--batch-size`: æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼Œå»ºè®®8-32ï¼‰
- `--img-size`: å›¾åƒå°ºå¯¸ï¼ˆå»ºè®®640ï¼Œä¸æ£€æµ‹æ—¶ä¿æŒä¸€è‡´ï¼‰
- `--weights`: é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨yolov8n.ptï¼‰
- `--pretrained`: ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼ˆæ¨èï¼‰
- `--resume`: æ¢å¤è®­ç»ƒï¼ˆä»checkpointç»§ç»­ï¼‰
- `--device`: è®­ç»ƒè®¾å¤‡ï¼ˆcuda:0, cuda:1, cpuï¼‰

### 2. æ•°æ®é›†å‡†å¤‡

**æ•°æ®é›†ç»“æ„**:
```
datasets/hairnet/
â”œâ”€â”€ data.yaml          # æ•°æ®é›†é…ç½®æ–‡ä»¶
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/        # è®­ç»ƒå›¾åƒ
â”‚   â””â”€â”€ labels/        # YOLOæ ¼å¼æ ‡æ³¨æ–‡ä»¶
â””â”€â”€ val/
    â”œâ”€â”€ images/        # éªŒè¯å›¾åƒ
    â””â”€â”€ labels/        # YOLOæ ¼å¼æ ‡æ³¨æ–‡ä»¶
```

**data.yaml æ ¼å¼**:
```yaml
path: datasets/hairnet
train: train/images
val: val/images
nc: 3  # ç±»åˆ«æ•°
names:
  0: hairnet    # å‘ç½‘
  1: head       # å¤´éƒ¨
  2: person     # äººä½“
```

### 3. è®­ç»ƒå»ºè®®

#### 3.1 æ•°æ®å¢å¼ºï¼ˆåœ¨data.yamlä¸­é…ç½®ï¼‰
```yaml
# åœ¨è®­ç»ƒè„šæœ¬ä¸­ï¼ŒYOLOv8ä¼šè‡ªåŠ¨åº”ç”¨æ•°æ®å¢å¼º
# å¯ä»¥é€šè¿‡ä¿®æ”¹è®­ç»ƒå‚æ•°è°ƒæ•´ï¼š
hsv_h: 0.015    # è‰²è°ƒå¢å¼º
hsv_s: 0.7      # é¥±å’Œåº¦å¢å¼º
hsv_v: 0.4      # äº®åº¦å¢å¼º
degrees: 10.0   # æ—‹è½¬è§’åº¦
translate: 0.1  # å¹³ç§»
scale: 0.5      # ç¼©æ”¾
fliplr: 0.5     # æ°´å¹³ç¿»è½¬
mosaic: 1.0     # Mosaicå¢å¼º
mixup: 0.1      # Mixupå¢å¼º
```

#### 3.2 è¶…å‚æ•°ä¼˜åŒ–

**å­¦ä¹ ç‡è°ƒæ•´**:
- åˆå§‹å­¦ä¹ ç‡: `lr0: 0.01`
- æœ€ç»ˆå­¦ä¹ ç‡: `lrf: 0.01`
- é¢„çƒ­è½®æ•°: `warmup_epochs: 3.0`

**æŸå¤±å‡½æ•°æƒé‡**:
- è¾¹ç•Œæ¡†æŸå¤±: `box: 7.5`
- åˆ†ç±»æŸå¤±: `cls: 0.5`
- DFLæŸå¤±: `dfl: 1.5`

**æ—©åœæœºåˆ¶**:
- `patience: 50`  # éªŒè¯æŸå¤±50è½®ä¸ä¸‹é™åˆ™åœæ­¢

---

## ğŸ¤² æ‰‹éƒ¨æ£€æµ‹æ¨¡å‹è®­ç»ƒ

### 1. è®­ç»ƒæœåŠ¡ä½¿ç”¨

**æœåŠ¡ä½ç½®**: `src/application/handwash_training_service.py`

**APIè°ƒç”¨**:
```python
from src.application.handwash_training_service import HandwashTrainingService
from pathlib import Path

service = HandwashTrainingService()

result = await service.train(
    dataset_dir=Path("datasets/handwash"),
    annotations_file=Path("datasets/handwash/annotations.json"),
    training_params={
        "epochs": 100,
        "batch_size": 32,
        "learning_rate": 0.001,
        "validation_split": 0.2,
        "device": "cuda:0"
    }
)
```

### 2. æ•°æ®é›†å‡†å¤‡

**æ•°æ®é›†ç»“æ„**:
```
datasets/handwash/
â”œâ”€â”€ annotations.json   # æ ‡æ³¨æ–‡ä»¶ï¼ˆå§¿æ€åºåˆ—ï¼‰
â”œâ”€â”€ sequences/         # å§¿æ€åºåˆ—æ•°æ®
â”‚   â”œâ”€â”€ session_001.npy
â”‚   â”œâ”€â”€ session_002.npy
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata.json      # å…ƒæ•°æ®
```

**annotations.json æ ¼å¼**:
```json
{
  "sessions": [
    {
      "session_id": "session_001",
      "sequence_file": "sequences/session_001.npy",
      "label": 1,  # 1=æ´—æ‰‹, 0=æœªæ´—æ‰‹
      "steps": ["wet", "soap", "scrub", "rinse", "dry"],
      "compliance": true
    }
  ]
}
```

### 3. è®­ç»ƒå»ºè®®

#### 3.1 æ¨¡å‹æ¶æ„
- **è¾“å…¥**: å§¿æ€åºåˆ—ï¼ˆ30å¸§ Ã— 21å…³é”®ç‚¹ Ã— 2åæ ‡ = 1260ç»´ï¼‰
- **æ¨¡å‹**: Temporal CNN (TCN)
- **è¾“å‡º**: äºŒåˆ†ç±»ï¼ˆæ´—æ‰‹/æœªæ´—æ‰‹ï¼‰

#### 3.2 è¶…å‚æ•°ä¼˜åŒ–
```python
training_params = {
    "epochs": 100,              # è®­ç»ƒè½®æ•°
    "batch_size": 32,           # æ‰¹æ¬¡å¤§å°
    "learning_rate": 0.001,     # å­¦ä¹ ç‡
    "validation_split": 0.2,    # éªŒè¯é›†æ¯”ä¾‹
    "sequence_length": 30,      # åºåˆ—é•¿åº¦ï¼ˆå¸§æ•°ï¼‰
    "seed": 42                  # éšæœºç§å­
}
```

---

## ğŸ“Š è¯„ä¼°è®­ç»ƒç¨‹åº¦

### 1. å‘ç½‘æ£€æµ‹è¯„ä¼°æŒ‡æ ‡

#### 1.1 YOLOv8è‡ªåŠ¨è¯„ä¼°

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒYOLOv8ä¼šè‡ªåŠ¨è®¡ç®—ä»¥ä¸‹æŒ‡æ ‡ï¼š

**ä¸»è¦æŒ‡æ ‡**:
- **mAP@0.5**: å¹³å‡ç²¾åº¦ï¼ˆIoU=0.5ï¼‰
- **mAP@0.5:0.95**: å¹³å‡ç²¾åº¦ï¼ˆIoU=0.5-0.95ï¼‰
- **Precision**: ç²¾ç¡®ç‡ï¼ˆæ£€æµ‹ä¸ºå‘ç½‘çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£æ˜¯å‘ç½‘çš„æ¯”ä¾‹ï¼‰
- **Recall**: å¬å›ç‡ï¼ˆçœŸæ­£çš„å‘ç½‘æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®æ£€æµ‹çš„æ¯”ä¾‹ï¼‰
- **F1-Score**: F1åˆ†æ•°ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼‰

**è®­ç»ƒæ—¥å¿—ä½ç½®**:
```
models/hairnet_model/
â”œâ”€â”€ results.csv        # è®­ç»ƒæŒ‡æ ‡CSV
â”œâ”€â”€ results.png        # è®­ç»ƒæ›²çº¿å›¾
â”œâ”€â”€ confusion_matrix.png  # æ··æ·†çŸ©é˜µ
â””â”€â”€ weights/
    â”œâ”€â”€ best.pt        # æœ€ä½³æ¨¡å‹
    â””â”€â”€ last.pt        # æœ€åä¸€è½®æ¨¡å‹
```

#### 1.2 æ‰‹åŠ¨è¯„ä¼°è„šæœ¬

**åˆ›å»ºè¯„ä¼°è„šæœ¬**: `scripts/evaluation/evaluate_hairnet_model.py`

```python
from ultralytics import YOLO
import json
from pathlib import Path

def evaluate_model(model_path, test_data_yaml):
    """è¯„ä¼°å‘ç½‘æ£€æµ‹æ¨¡å‹"""
    model = YOLO(model_path)

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    results = model.val(
        data=test_data_yaml,
        imgsz=640,
        conf=0.25,
        iou=0.45,
        save_json=True,
        plots=True
    )

    # æå–æŒ‡æ ‡
    metrics = {
        "mAP50": results.box.map50,
        "mAP50-95": results.box.map,
        "precision": results.box.mp,
        "recall": results.box.mr,
        "f1": 2 * (results.box.mp * results.box.mr) / (results.box.mp + results.box.mr)
    }

    return metrics

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    metrics = evaluate_model(
        "models/hairnet_detection/hairnet_detection.pt",
        "datasets/hairnet/data.yaml"
    )
    print(json.dumps(metrics, indent=2))
```

#### 1.3 è¯„ä¼°æ ‡å‡†

**ä¼˜ç§€æ¨¡å‹æ ‡å‡†**:
- mAP@0.5 â‰¥ 0.90
- Precision â‰¥ 0.85
- Recall â‰¥ 0.85
- F1-Score â‰¥ 0.85

**è‰¯å¥½æ¨¡å‹æ ‡å‡†**:
- mAP@0.5 â‰¥ 0.80
- Precision â‰¥ 0.75
- Recall â‰¥ 0.75
- F1-Score â‰¥ 0.75

**éœ€è¦æ”¹è¿›**:
- mAP@0.5 < 0.75
- Precision < 0.70
- Recall < 0.70

### 2. æ‰‹éƒ¨æ£€æµ‹è¯„ä¼°æŒ‡æ ‡

#### 2.1 è®­ç»ƒè¿‡ç¨‹è¯„ä¼°

**è¯„ä¼°æŒ‡æ ‡**:
- **Loss**: æŸå¤±å€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
- **Accuracy**: å‡†ç¡®ç‡ï¼ˆæ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼‰
- **Validation Loss**: éªŒè¯æŸå¤±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
- **Validation Accuracy**: éªŒè¯å‡†ç¡®ç‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰

**è®­ç»ƒæ—¥å¿—**:
```python
# è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¾“å‡ºï¼š
# Epoch 1/100: Loss=0.523, Accuracy=0.75, Val_Loss=0.456, Val_Accuracy=0.82
# Epoch 2/100: Loss=0.412, Accuracy=0.83, Val_Loss=0.389, Val_Accuracy=0.85
# ...
```

#### 2.2 è¯¦ç»†è¯„ä¼°è„šæœ¬

**åˆ›å»ºè¯„ä¼°è„šæœ¬**: `scripts/evaluation/evaluate_handwash_model.py`

```python
import torch
from torch.utils.data import DataLoader
from src.application.handwash_training_service import HandwashTrainingService
import json

def evaluate_handwash_model(model_path, test_loader, device="cuda:0"):
    """è¯„ä¼°æ‰‹éƒ¨æ£€æµ‹æ¨¡å‹"""
    model = torch.load(model_path)
    model.eval()

    total_correct = 0
    total_samples = 0
    true_positives = 0
    false_positives = 0
    false_negatives = 0

    with torch.no_grad():
        for sequences, labels in test_loader:
            sequences = sequences.to(device)
            labels = labels.to(device)

            logits = model(sequences)
            predictions = torch.sigmoid(logits) > 0.5

            # è®¡ç®—å‡†ç¡®ç‡
            correct = (predictions.float() == labels).sum().item()
            total_correct += correct
            total_samples += labels.numel()

            # è®¡ç®—æ··æ·†çŸ©é˜µ
            true_positives += ((predictions == 1) & (labels == 1)).sum().item()
            false_positives += ((predictions == 1) & (labels == 0)).sum().item()
            false_negatives += ((predictions == 0) & (labels == 1)).sum().item()

    # è®¡ç®—æŒ‡æ ‡
    accuracy = total_correct / total_samples if total_samples > 0 else 0.0
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    metrics = {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1,
        "true_positives": true_positives,
        "false_positives": false_positives,
        "false_negatives": false_negatives
    }

    return metrics
```

#### 2.3 è¯„ä¼°æ ‡å‡†

**ä¼˜ç§€æ¨¡å‹æ ‡å‡†**:
- Accuracy â‰¥ 0.90
- Precision â‰¥ 0.85
- Recall â‰¥ 0.85
- F1-Score â‰¥ 0.85

**è‰¯å¥½æ¨¡å‹æ ‡å‡†**:
- Accuracy â‰¥ 0.80
- Precision â‰¥ 0.75
- Recall â‰¥ 0.75
- F1-Score â‰¥ 0.75

**éœ€è¦æ”¹è¿›**:
- Accuracy < 0.75
- Precision < 0.70
- Recall < 0.70

---

## ğŸš€ åŠ å¼ºè®­ç»ƒçš„æ–¹æ³•

### 1. æ•°æ®å¢å¼º

#### 1.1 å‘ç½‘æ£€æµ‹æ•°æ®å¢å¼º

**å›¾åƒå¢å¼º**:
- äº®åº¦è°ƒæ•´ï¼ˆÂ±20%ï¼‰
- å¯¹æ¯”åº¦è°ƒæ•´ï¼ˆÂ±15%ï¼‰
- è‰²è°ƒè°ƒæ•´ï¼ˆÂ±10%ï¼‰
- æ—‹è½¬ï¼ˆÂ±15åº¦ï¼‰
- ç¼©æ”¾ï¼ˆ0.8-1.2å€ï¼‰
- æ°´å¹³ç¿»è½¬ï¼ˆ50%æ¦‚ç‡ï¼‰
- Mosaicå¢å¼ºï¼ˆ4å¼ å›¾åƒæ‹¼æ¥ï¼‰
- Mixupå¢å¼ºï¼ˆå›¾åƒæ··åˆï¼‰

**å®ç°æ–¹å¼**:
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­ï¼ŒYOLOv8ä¼šè‡ªåŠ¨åº”ç”¨è¿™äº›å¢å¼º
# å¯ä»¥é€šè¿‡ä¿®æ”¹è®­ç»ƒå‚æ•°è°ƒæ•´å¼ºåº¦
train_args = {
    "hsv_h": 0.02,      # å¢åŠ è‰²è°ƒå˜åŒ–
    "hsv_s": 0.8,       # å¢åŠ é¥±å’Œåº¦å˜åŒ–
    "hsv_v": 0.5,       # å¢åŠ äº®åº¦å˜åŒ–
    "degrees": 15.0,    # å¢åŠ æ—‹è½¬è§’åº¦
    "translate": 0.15,  # å¢åŠ å¹³ç§»
    "scale": 0.6,       # å¢åŠ ç¼©æ”¾èŒƒå›´
    "mosaic": 1.0,      # å¯ç”¨Mosaic
    "mixup": 0.15       # å¯ç”¨Mixup
}
```

#### 1.2 æ‰‹éƒ¨æ£€æµ‹æ•°æ®å¢å¼º

**åºåˆ—å¢å¼º**:
- æ—¶é—´æ‰­æ›²ï¼ˆåŠ é€Ÿ/å‡é€Ÿï¼‰
- å™ªå£°æ·»åŠ ï¼ˆé«˜æ–¯å™ªå£°ï¼‰
- å…³é”®ç‚¹æŠ–åŠ¨ï¼ˆÂ±2åƒç´ ï¼‰
- åºåˆ—è£å‰ªï¼ˆéšæœºèµ·å§‹ç‚¹ï¼‰
- åºåˆ—ç¿»è½¬ï¼ˆå·¦å³æ‰‹äº’æ¢ï¼‰

**å®ç°æ–¹å¼**:
```python
def augment_sequence(sequence, label):
    """å¢å¼ºå§¿æ€åºåˆ—"""
    # æ—¶é—´æ‰­æ›²
    if random.random() < 0.3:
        sequence = time_warp(sequence, sigma=0.2)

    # å™ªå£°æ·»åŠ 
    if random.random() < 0.3:
        noise = np.random.normal(0, 0.01, sequence.shape)
        sequence = sequence + noise

    # å…³é”®ç‚¹æŠ–åŠ¨
    if random.random() < 0.3:
        jitter = np.random.normal(0, 2, sequence.shape)
        sequence = sequence + jitter

    return sequence, label
```

### 2. å¢åŠ è®­ç»ƒæ•°æ®

#### 2.1 æ•°æ®æ”¶é›†ç­–ç•¥

**å‘ç½‘æ£€æµ‹**:
- æ”¶é›†ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„å›¾åƒ
- æ”¶é›†ä¸åŒè§’åº¦çš„å›¾åƒï¼ˆæ­£é¢ã€ä¾§é¢ã€èƒŒé¢ï¼‰
- æ”¶é›†ä¸åŒå‘ç½‘ç±»å‹çš„å›¾åƒï¼ˆé¢œè‰²ã€æè´¨ï¼‰
- æ”¶é›†é®æŒ¡æƒ…å†µä¸‹çš„å›¾åƒï¼ˆéƒ¨åˆ†é®æŒ¡ã€å®Œå…¨é®æŒ¡ï¼‰
- æ”¶é›†ä¸åŒèƒŒæ™¯çš„å›¾åƒ

**æ‰‹éƒ¨æ£€æµ‹**:
- æ”¶é›†ä¸åŒæ´—æ‰‹åŠ¨ä½œçš„åºåˆ—
- æ”¶é›†ä¸åŒé€Ÿåº¦çš„æ´—æ‰‹åºåˆ—ï¼ˆå¿«é€Ÿã€æ…¢é€Ÿï¼‰
- æ”¶é›†ä¸åŒç¯å¢ƒä¸‹çš„åºåˆ—ï¼ˆä¸åŒæ°´æ± ã€ä¸åŒä½ç½®ï¼‰
- æ”¶é›†å¼‚å¸¸æƒ…å†µçš„åºåˆ—ï¼ˆéæ´—æ‰‹åŠ¨ä½œï¼‰

#### 2.2 æ•°æ®æ ‡æ³¨

**å‘ç½‘æ£€æµ‹æ ‡æ³¨**:
- ä½¿ç”¨æ ‡æ³¨å·¥å…·ï¼ˆLabelImgã€CVATç­‰ï¼‰
- æ ‡æ³¨æ ¼å¼ï¼šYOLOæ ¼å¼ï¼ˆç±»åˆ«ID + å½’ä¸€åŒ–åæ ‡ï¼‰
- æ ‡æ³¨ç±»åˆ«ï¼šhairnet, head, person

**æ‰‹éƒ¨æ£€æµ‹æ ‡æ³¨**:
- æ ‡æ³¨æ´—æ‰‹æ­¥éª¤ï¼ˆwet, soap, scrub, rinse, dryï¼‰
- æ ‡æ³¨åˆè§„çŠ¶æ€ï¼ˆcompliant, non-compliantï¼‰
- æ ‡æ³¨æ—¶é—´æˆ³å’ŒæŒç»­æ—¶é—´

### 3. è¶…å‚æ•°ä¼˜åŒ–

#### 3.1 å‘ç½‘æ£€æµ‹è¶…å‚æ•°

**å­¦ä¹ ç‡è°ƒæ•´**:
```python
# ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
from torch.optim.lr_scheduler import CosineAnnealingLR

scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
```

**æ‰¹æ¬¡å¤§å°è°ƒæ•´**:
- GPUå†…å­˜å……è¶³ï¼šbatch_size=32-64
- GPUå†…å­˜ä¸€èˆ¬ï¼šbatch_size=16-32
- GPUå†…å­˜ä¸è¶³ï¼šbatch_size=8-16

**å›¾åƒå°ºå¯¸è°ƒæ•´**:
- é«˜ç²¾åº¦ï¼šimgsz=640ï¼ˆæ¨èï¼‰
- å¹³è¡¡ï¼šimgsz=512
- å¿«é€Ÿï¼šimgsz=416

#### 3.2 æ‰‹éƒ¨æ£€æµ‹è¶…å‚æ•°

**å­¦ä¹ ç‡è°ƒæ•´**:
```python
# ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
from torch.optim.lr_scheduler import ReduceLROnPlateau

scheduler = ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=10,
    verbose=True
)
```

**åºåˆ—é•¿åº¦è°ƒæ•´**:
- é•¿åºåˆ—ï¼šsequence_length=60ï¼ˆæ•è·å®Œæ•´åŠ¨ä½œï¼‰
- æ ‡å‡†ï¼šsequence_length=30ï¼ˆæ¨èï¼‰
- çŸ­åºåˆ—ï¼šsequence_length=15ï¼ˆå¿«é€Ÿæ£€æµ‹ï¼‰

### 4. æ¨¡å‹æ¶æ„ä¼˜åŒ–

#### 4.1 å‘ç½‘æ£€æµ‹æ¨¡å‹

**æ¨¡å‹é€‰æ‹©**:
- YOLOv8n: å¿«é€Ÿï¼Œé€‚åˆå®æ—¶æ£€æµ‹
- YOLOv8s: å¹³è¡¡ï¼Œæ¨èä½¿ç”¨
- YOLOv8m: é«˜ç²¾åº¦ï¼Œé€‚åˆç¦»çº¿æ£€æµ‹

**è¿ç§»å­¦ä¹ **:
```python
# ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
model = YOLO("yolov8s.pt")  # ä½¿ç”¨COCOé¢„è®­ç»ƒæƒé‡
model.train(data="datasets/hairnet/data.yaml", epochs=150)
```

#### 4.2 æ‰‹éƒ¨æ£€æµ‹æ¨¡å‹

**æ¨¡å‹æ¶æ„**:
- Temporal CNN (TCN): å½“å‰ä½¿ç”¨
- Transformer: æ›´å¼ºå¤§çš„æ—¶åºå»ºæ¨¡ï¼ˆå¯å‡çº§ï¼‰
- LSTM/GRU: ä¼ ç»Ÿæ—¶åºæ¨¡å‹ï¼ˆå¤‡é€‰ï¼‰

**å‡çº§åˆ°Transformer**:
```python
from transformers import TransformerEncoder

class HandwashTransformer(nn.Module):
    def __init__(self, input_dim, d_model=256, nhead=8, num_layers=4):
        super().__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        self.transformer = TransformerEncoder(
            TransformerEncoderLayer(d_model, nhead),
            num_layers
        )
        self.classifier = nn.Linear(d_model, 1)
```

### 5. è®­ç»ƒç­–ç•¥ä¼˜åŒ–

#### 5.1 æ¸è¿›å¼è®­ç»ƒ

**é˜¶æ®µ1: åŸºç¡€è®­ç»ƒ**
- ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
- å†»ç»“backboneï¼Œåªè®­ç»ƒæ£€æµ‹å¤´
- è®­ç»ƒ10-20è½®

**é˜¶æ®µ2: å¾®è°ƒè®­ç»ƒ**
- è§£å†»backbone
- é™ä½å­¦ä¹ ç‡ï¼ˆÃ—0.1ï¼‰
- è®­ç»ƒ50-100è½®

**é˜¶æ®µ3: ç²¾ç»†è®­ç»ƒ**
- è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ï¼ˆÃ—0.01ï¼‰
- è®­ç»ƒ30-50è½®

#### 5.2 é›†æˆå­¦ä¹ 

**æ¨¡å‹é›†æˆ**:
```python
# è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œç„¶åé›†æˆ
models = [
    load_model("model_1.pt"),
    load_model("model_2.pt"),
    load_model("model_3.pt")
]

def ensemble_predict(models, input_data):
    predictions = [model(input_data) for model in models]
    return torch.mean(torch.stack(predictions), dim=0)
```

---

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡è¯¦è§£

### 1. å‘ç½‘æ£€æµ‹æŒ‡æ ‡

#### 1.1 mAP (Mean Average Precision)

**å®šä¹‰**: æ‰€æœ‰ç±»åˆ«çš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰çš„å¹³å‡å€¼

**è®¡ç®—æ–¹å¼**:
```
AP = âˆ«â‚€Â¹ P(R) dR
mAP = (1/N) Ã— Î£ AP_i
```

**è§£è¯»**:
- mAP@0.5: IoUé˜ˆå€¼ä¸º0.5æ—¶çš„å¹³å‡ç²¾åº¦
- mAP@0.5:0.95: IoUé˜ˆå€¼ä»0.5åˆ°0.95ï¼ˆæ­¥é•¿0.05ï¼‰çš„å¹³å‡ç²¾åº¦
- å€¼è¶Šé«˜è¶Šå¥½ï¼ŒèŒƒå›´0-1

#### 1.2 Precision (ç²¾ç¡®ç‡)

**å®šä¹‰**: æ£€æµ‹ä¸ºå‘ç½‘çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£æ˜¯å‘ç½‘çš„æ¯”ä¾‹

**å…¬å¼**:
```
Precision = TP / (TP + FP)
```

**è§£è¯»**:
- å€¼è¶Šé«˜ï¼Œè¯¯æŠ¥è¶Šå°‘
- èŒƒå›´0-1ï¼Œç›®æ ‡â‰¥0.85

#### 1.3 Recall (å¬å›ç‡)

**å®šä¹‰**: çœŸæ­£çš„å‘ç½‘æ ·æœ¬ä¸­ï¼Œè¢«æ­£ç¡®æ£€æµ‹çš„æ¯”ä¾‹

**å…¬å¼**:
```
Recall = TP / (TP + FN)
```

**è§£è¯»**:
- å€¼è¶Šé«˜ï¼Œæ¼æŠ¥è¶Šå°‘
- èŒƒå›´0-1ï¼Œç›®æ ‡â‰¥0.85

#### 1.4 F1-Score

**å®šä¹‰**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

**å…¬å¼**:
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**è§£è¯»**:
- å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡
- èŒƒå›´0-1ï¼Œç›®æ ‡â‰¥0.85

### 2. æ‰‹éƒ¨æ£€æµ‹æŒ‡æ ‡

#### 2.1 Accuracy (å‡†ç¡®ç‡)

**å®šä¹‰**: æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬å æ€»æ ·æœ¬çš„æ¯”ä¾‹

**å…¬å¼**:
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

**è§£è¯»**:
- æ•´ä½“æ€§èƒ½æŒ‡æ ‡
- èŒƒå›´0-1ï¼Œç›®æ ‡â‰¥0.90

#### 2.2 Precision (ç²¾ç¡®ç‡)

**å®šä¹‰**: é¢„æµ‹ä¸ºæ´—æ‰‹çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£æ˜¯æ´—æ‰‹çš„æ¯”ä¾‹

**å…¬å¼**:
```
Precision = TP / (TP + FP)
```

**è§£è¯»**:
- å‡å°‘è¯¯æŠ¥ï¼ˆå°†éæ´—æ‰‹åŠ¨ä½œè¯†åˆ«ä¸ºæ´—æ‰‹ï¼‰
- èŒƒå›´0-1ï¼Œç›®æ ‡â‰¥0.85

#### 2.3 Recall (å¬å›ç‡)

**å®šä¹‰**: çœŸæ­£çš„æ´—æ‰‹è¡Œä¸ºä¸­ï¼Œè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹

**å…¬å¼**:
```
Recall = TP / (TP + FN)
```

**è§£è¯»**:
- å‡å°‘æ¼æŠ¥ï¼ˆé—æ¼çœŸæ­£çš„æ´—æ‰‹è¡Œä¸ºï¼‰
- èŒƒå›´0-1ï¼Œç›®æ ‡â‰¥0.85

#### 2.4 F1-Score

**å®šä¹‰**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

**å…¬å¼**:
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**è§£è¯»**:
- å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡
- èŒƒå›´0-1ï¼Œç›®æ ‡â‰¥0.85

### 3. æ··æ·†çŸ©é˜µ

**2Ã—2æ··æ·†çŸ©é˜µ**:
```
               é¢„æµ‹
           æ­£ç±»    è´Ÿç±»
å®é™… æ­£ç±»  TP    FN
     è´Ÿç±»  FP    TN
```

**è§£è¯»**:
- TP (True Positive): çœŸæ­£ä¾‹ï¼ˆæ­£ç¡®è¯†åˆ«ä¸ºæ´—æ‰‹ï¼‰
- FP (False Positive): å‡æ­£ä¾‹ï¼ˆè¯¯æŠ¥ï¼Œéæ´—æ‰‹è¯†åˆ«ä¸ºæ´—æ‰‹ï¼‰
- FN (False Negative): å‡è´Ÿä¾‹ï¼ˆæ¼æŠ¥ï¼Œæ´—æ‰‹æœªè¯†åˆ«ï¼‰
- TN (True Negative): çœŸè´Ÿä¾‹ï¼ˆæ­£ç¡®è¯†åˆ«ä¸ºéæ´—æ‰‹ï¼‰

---

## ğŸ¯ è®­ç»ƒæ£€æŸ¥æ¸…å•

### å‘ç½‘æ£€æµ‹è®­ç»ƒæ£€æŸ¥æ¸…å•

- [ ] æ•°æ®é›†å‡†å¤‡å®Œæˆï¼ˆtrain/val/testï¼‰
- [ ] æ•°æ®æ ‡æ³¨è´¨é‡æ£€æŸ¥ï¼ˆæ ‡æ³¨å‡†ç¡®ã€å®Œæ•´ï¼‰
- [ ] æ•°æ®é›†å¹³è¡¡æ€§æ£€æŸ¥ï¼ˆå„ç±»åˆ«æ ·æœ¬æ•°é‡ï¼‰
- [ ] è®­ç»ƒå‚æ•°è®¾ç½®ï¼ˆepochs, batch_size, learning_rateï¼‰
- [ ] æ•°æ®å¢å¼ºé…ç½®ï¼ˆå¼ºåº¦é€‚ä¸­ï¼‰
- [ ] é¢„è®­ç»ƒæƒé‡åŠ è½½ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼‰
- [ ] è®­ç»ƒè¿‡ç¨‹ç›‘æ§ï¼ˆlossä¸‹é™ã€æŒ‡æ ‡æå‡ï¼‰
- [ ] éªŒè¯é›†è¯„ä¼°ï¼ˆmAP, Precision, Recallï¼‰
- [ ] æµ‹è¯•é›†è¯„ä¼°ï¼ˆæœ€ç»ˆæ€§èƒ½ï¼‰
- [ ] æ¨¡å‹ä¿å­˜ï¼ˆbest.pt, last.ptï¼‰
- [ ] è®­ç»ƒæŠ¥å‘Šç”Ÿæˆï¼ˆmetrics, curvesï¼‰

### æ‰‹éƒ¨æ£€æµ‹è®­ç»ƒæ£€æŸ¥æ¸…å•

- [ ] æ•°æ®é›†å‡†å¤‡å®Œæˆï¼ˆåºåˆ—æ•°æ®ã€æ ‡æ³¨ï¼‰
- [ ] æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆåºåˆ—å®Œæ•´ã€æ ‡æ³¨å‡†ç¡®ï¼‰
- [ ] æ•°æ®é›†å¹³è¡¡æ€§æ£€æŸ¥ï¼ˆæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼‰
- [ ] è®­ç»ƒå‚æ•°è®¾ç½®ï¼ˆepochs, batch_size, learning_rateï¼‰
- [ ] åºåˆ—å¢å¼ºé…ç½®ï¼ˆæ—¶é—´æ‰­æ›²ã€å™ªå£°ï¼‰
- [ ] æ¨¡å‹æ¶æ„é€‰æ‹©ï¼ˆTCN/Transformerï¼‰
- [ ] è®­ç»ƒè¿‡ç¨‹ç›‘æ§ï¼ˆlossä¸‹é™ã€accuracyæå‡ï¼‰
- [ ] éªŒè¯é›†è¯„ä¼°ï¼ˆAccuracy, Precision, Recallï¼‰
- [ ] æµ‹è¯•é›†è¯„ä¼°ï¼ˆæœ€ç»ˆæ€§èƒ½ï¼‰
- [ ] æ¨¡å‹ä¿å­˜ï¼ˆcheckpointï¼‰
- [ ] è®­ç»ƒæŠ¥å‘Šç”Ÿæˆï¼ˆmetrics, confusion matrixï¼‰

---

## ğŸ“ æ€»ç»“

### å‘ç½‘æ£€æµ‹è®­ç»ƒè¦ç‚¹

1. **æ•°æ®è´¨é‡**: ç¡®ä¿æ ‡æ³¨å‡†ç¡®ã€æ•°æ®å¤šæ ·
2. **æ•°æ®å¢å¼º**: é€‚åº¦å¢å¼ºï¼Œé¿å…è¿‡åº¦
3. **è¶…å‚æ•°**: æ ¹æ®GPUå’Œæ•°æ®é›†è°ƒæ•´
4. **è¯„ä¼°æŒ‡æ ‡**: å…³æ³¨mAPã€Precisionã€Recall
5. **æŒç»­æ”¹è¿›**: æ ¹æ®è¯„ä¼°ç»“æœè¿­ä»£ä¼˜åŒ–

### æ‰‹éƒ¨æ£€æµ‹è®­ç»ƒè¦ç‚¹

1. **åºåˆ—è´¨é‡**: ç¡®ä¿åºåˆ—å®Œæ•´ã€æ ‡æ³¨å‡†ç¡®
2. **æ—¶åºå»ºæ¨¡**: é€‰æ‹©åˆé€‚çš„æ—¶åºæ¨¡å‹
3. **ç‰¹å¾å·¥ç¨‹**: æå–æœ‰æ•ˆçš„æ—¶åºç‰¹å¾
4. **è¯„ä¼°æŒ‡æ ‡**: å…³æ³¨Accuracyã€Precisionã€Recall
5. **æŒç»­æ”¹è¿›**: æ ¹æ®è¯„ä¼°ç»“æœè¿­ä»£ä¼˜åŒ–

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [æ¨¡å‹è®­ç»ƒæœåŠ¡æ–‡æ¡£](../src/application/model_training_service.py)
- [æ´—æ‰‹è®­ç»ƒæœåŠ¡æ–‡æ¡£](../src/application/handwash_training_service.py)
- [è®­ç»ƒè„šæœ¬æ–‡æ¡£](../scripts/training/train_hairnet_model.py)
- [MLèåˆå‡†ç¡®ç‡åˆ†æ](./ML_FUSION_ACCURACY_ANALYSIS.md)
- [å‘ç½‘æ£€æµ‹ä¼˜åŒ–æ€»ç»“](./HAIRNET_DETECTION_OPTIMIZATION_SUMMARY.md)
