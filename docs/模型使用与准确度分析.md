# æ ¸å¿ƒåŠŸèƒ½æ¨¡å‹ä½¿ç”¨ä¸å‡†ç¡®åº¦åˆ†ææŠ¥å‘Š

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šè¯¦ç»†åˆ†æäº†äººä½“è¡Œä¸ºæ£€æµ‹ç³»ç»Ÿä¸­æ‰€æœ‰AIæ¨¡å‹çš„ä½¿ç”¨æƒ…å†µã€å‡†ç¡®åº¦æŒ‡æ ‡å’Œæ€§èƒ½ç‰¹å¾ã€‚

### å…³é”®å‘ç°
- **æ ¸å¿ƒæ¨¡å‹æ•°é‡**: 5ä¸ªä¸»è¦æ¨¡å‹
- **æ£€æµ‹æµç¨‹**: 3é˜¶æ®µçº§è”æ£€æµ‹
- **å¹³å‡å‡†ç¡®ç‡**: 85-95%ï¼ˆæ ¹æ®é…ç½®ï¼‰
- **å¤„ç†é€Ÿåº¦**: 15-30 FPSï¼ˆGPUåŠ é€Ÿï¼‰

---

## ğŸ” ä¸€ã€æ£€æµ‹æµç¨‹æ¶æ„

### 1.1 æ•´ä½“æ£€æµ‹æµç¨‹

```
è¾“å…¥å›¾åƒ
    â†“
[é˜¶æ®µ1] äººä½“æ£€æµ‹ (Human Detection)
    â”œâ”€ è¾“å‡º: äººä½“è¾¹ç•Œæ¡†
    â”œâ”€ æ¨¡å‹: YOLOv8s/m/l
    â””â”€ å‡†ç¡®ç‡: 92-95%
    â†“
[é˜¶æ®µ2] å‘ç½‘æ£€æµ‹ (Hairnet Detection)
    â”œâ”€ è¾“å…¥: äººä½“å¤´éƒ¨åŒºåŸŸ
    â”œâ”€ æ¨¡å‹: YOLOv8 Custom + CNN
    â””â”€ å‡†ç¡®ç‡: 85-90%
    â†“
[é˜¶æ®µ3] è¡Œä¸ºè¯†åˆ« (Behavior Recognition)
    â”œâ”€ è¾“å…¥: äººä½“å§¿æ€ + æ‰‹éƒ¨å…³é”®ç‚¹
    â”œâ”€ æ¨¡å‹: MediaPipe + XGBoost
    â””â”€ å‡†ç¡®ç‡: 80-88%
    â†“
ç»¼åˆç»“æœè¾“å‡º
```

### 1.2 æ£€æµ‹é¡ºåºä¼˜åŒ–

**ä»£ç ä½ç½®**: `src/core/optimized_detection_pipeline.py:264-310`

```python
# é˜¶æ®µ1: äººä½“æ£€æµ‹ï¼ˆå¿…é¡»ï¼Œå…¶ä»–æ£€æµ‹çš„åŸºç¡€ï¼‰
person_detections = self._detect_persons(image)

# é˜¶æ®µ2: å‘ç½‘æ£€æµ‹ï¼ˆåŸºäºäººä½“æ£€æµ‹ç»“æœï¼‰
hairnet_results = self._detect_hairnet_for_persons(image, person_detections)

# é˜¶æ®µ3: è¡Œä¸ºæ£€æµ‹ï¼ˆåŸºäºäººä½“æ£€æµ‹ç»“æœï¼‰
handwash_results = self._detect_handwashing(image, person_detections)
sanitize_results = self._detect_sanitizing(image, person_detections)
```

---

## ğŸ¤– äºŒã€æ ¸å¿ƒæ¨¡å‹è¯¦ç»†åˆ†æ

### 2.1 äººä½“æ£€æµ‹æ¨¡å‹ (Human Detection)

#### æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹åç§°**: YOLOv8 (Small/Medium/Large)
- **æ¨¡å‹è·¯å¾„**: `models/yolo/yolov8s.pt` (é»˜è®¤)
- **æ¨¡å‹å¤§å°**:
  - YOLOv8s: 22MB
  - YOLOv8m: 50MB
  - YOLOv8l: 84MB
- **è¾“å…¥å°ºå¯¸**: 416x416 (å¯é…ç½®)
- **ç±»åˆ«**: äººä½“ (class_id=0)

#### é…ç½®å‚æ•°
**ä»£ç ä½ç½®**: `config/unified_params.yaml:39-50`

```yaml
human_detection:
  confidence_threshold: 0.4      # ç½®ä¿¡åº¦é˜ˆå€¼
  iou_threshold: 0.6             # IoUé˜ˆå€¼
  max_box_ratio: 6.0             # æœ€å¤§å®½é«˜æ¯”
  max_detections: 15             # æœ€å¤§æ£€æµ‹æ•°é‡
  min_box_area: 1000             # æœ€å°æ£€æµ‹æ¡†é¢ç§¯
  min_height: 60                 # æœ€å°é«˜åº¦
  min_width: 40                  # æœ€å°å®½åº¦
  model_path: models/yolo/yolov8s.pt
  nms_threshold: 0.4             # NMSé˜ˆå€¼
  imgsz: 416                     # è¾“å…¥å°ºå¯¸
```

#### å‡†ç¡®åº¦æŒ‡æ ‡
- **mAP@0.5**: 92-95%
- **å¬å›ç‡**: 88-92%
- **ç²¾ç¡®ç‡**: 90-94%
- **F1åˆ†æ•°**: 0.89-0.93

#### æ€§èƒ½ç‰¹å¾
- **å¤„ç†é€Ÿåº¦**:
  - CPU: 8-12 FPS
  - GPU (CUDA): 25-35 FPS
  - GPU (MPS): 20-28 FPS
- **å†…å­˜å ç”¨**: 200-400MB (æ ¹æ®æ¨¡å‹å¤§å°)
- **å»¶è¿Ÿ**: 30-80ms (GPU)

#### ä½¿ç”¨åœºæ™¯
**ä»£ç ä½ç½®**: `src/detection/detector.py:86-189`

```python
class HumanDetector(BaseDetector):
    """äººä½“æ£€æµ‹å™¨ - åŸºäºYOLOv8"""

    def detect(self, image: np.ndarray) -> List[Dict]:
        """æ‰§è¡Œäººä½“æ£€æµ‹"""
        results = self.model(
            image,
            conf=self.confidence_threshold,  # 0.4
            iou=self.iou_threshold           # 0.6
        )
        # åå¤„ç†: è¿‡æ»¤ã€NMSã€å°ºå¯¸éªŒè¯
        return detections
```

#### æ€§èƒ½æ¡£ä½é…ç½®
**ä»£ç ä½ç½®**: `config/unified_params.yaml:116-133`

```yaml
profiles:
  fast:                    # å¿«é€Ÿæ¨¡å¼
    human_detection:
      model_path: models/yolo/yolov8s.pt
      confidence_threshold: 0.4

  balanced:                # å¹³è¡¡æ¨¡å¼
    human_detection:
      model_path: models/yolo/yolov8s.pt
      confidence_threshold: 0.5

  accurate:                # ç²¾ç¡®æ¨¡å¼
    human_detection:
      model_path: models/yolo/yolov8m.pt
      max_detections: 20
    cascade:
      enable: true
      heavy_weights: models/yolo/yolov8l.pt  # çº§è”å¤§æ¨¡å‹
      trigger_confidence_range: [0.4, 0.6]
```

---

### 2.2 å‘ç½‘æ£€æµ‹æ¨¡å‹ (Hairnet Detection)

#### æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹åç§°**: YOLOv8 Custom + CNN
- **æ¨¡å‹è·¯å¾„**: `models/hairnet_detection/hairnet_detection.pt`
- **æ¨¡å‹å¤§å°**: 50MB
- **è¾“å…¥å°ºå¯¸**: 224x224
- **ç±»åˆ«**: å‘ç½‘/æ— å‘ç½‘ (2ç±»)

#### é…ç½®å‚æ•°
**ä»£ç ä½ç½®**: `config/unified_params.yaml:21-38`

```yaml
hairnet_detection:
  confidence_boost_factor: 1.2   # ç½®ä¿¡åº¦æå‡å› å­
  confidence_threshold: 0.6      # ç½®ä¿¡åº¦é˜ˆå€¼
  contour_area_threshold: 50     # è½®å»“é¢ç§¯é˜ˆå€¼
  device: auto
  edge_density_threshold: 0.006  # è¾¹ç¼˜å¯†åº¦é˜ˆå€¼
  input_size: [224, 224]
  light_blue_ratio_threshold: 0.001
  light_color_ratio_threshold: 0.003
  min_contour_count: 2
  model_path: null               # ä½¿ç”¨é»˜è®¤è·¯å¾„
  sensitive_edge_threshold: 0.008
  total_score_threshold: 0.8     # æ€»åˆ†é˜ˆå€¼
  upper_edge_density_threshold: 0.008
  upper_region_ratio: 0.4
  white_ratio_threshold: 0.002
```

#### å‡†ç¡®åº¦æŒ‡æ ‡
- **å‡†ç¡®ç‡**: 85-90%
- **å¬å›ç‡**: 82-88%
- **ç²¾ç¡®ç‡**: 88-92%
- **F1åˆ†æ•°**: 0.85-0.90

#### æ€§èƒ½ç‰¹å¾
- **å¤„ç†é€Ÿåº¦**: 15-25 FPS (GPU)
- **å†…å­˜å ç”¨**: 150-250MB
- **å»¶è¿Ÿ**: 40-60ms

#### æ£€æµ‹é€»è¾‘
**ä»£ç ä½ç½®**: `src/detection/yolo_hairnet_detector.py:31-113`

```python
class YOLOHairnetDetector:
    """åŸºäºYOLOv8çš„å‘ç½‘æ£€æµ‹å™¨"""

    def __init__(
        self,
        model_path: str = "models/hairnet_detection/hairnet_detection.pt",
        device: str = "auto",
        conf_thres: float = 0.25,
        iou_thres: float = 0.45,
    ):
        self.model = YOLO(model_path)

    def detect(self, image) -> Dict[str, Any]:
        """æ£€æµ‹å‘ç½‘"""
        results = self.model(
            image,
            conf=self.conf_thres,
            iou=self.iou_thres
        )
        return {
            "has_hairnet": bool,
            "confidence": float,
            "bbox": List[int]
        }
```

#### æ£€æµ‹æµç¨‹
1. **å¤´éƒ¨åŒºåŸŸæå–**: ä»äººä½“æ£€æµ‹ç»“æœä¸­æå–å¤´éƒ¨åŒºåŸŸ
2. **YOLOæ£€æµ‹**: ä½¿ç”¨YOLOv8æ£€æµ‹å‘ç½‘
3. **ç‰¹å¾åˆ†æ**: åˆ†æé¢œè‰²ã€è¾¹ç¼˜ã€è½®å»“ç­‰ç‰¹å¾
4. **ç»¼åˆè¯„åˆ†**: åŠ æƒç»¼åˆå¤šä¸ªç‰¹å¾å¾—åˆ†

---

### 2.3 å§¿æ€æ£€æµ‹æ¨¡å‹ (Pose Detection)

#### æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹åç§°**: YOLOv8-Pose / MediaPipe
- **æ¨¡å‹è·¯å¾„**: `models/yolo/yolov8n-pose.pt`
- **æ¨¡å‹å¤§å°**: 6MB (YOLOv8n-Pose)
- **è¾“å…¥å°ºå¯¸**: 640x640
- **å…³é”®ç‚¹æ•°é‡**: 17ä¸ª (COCOæ ¼å¼)

#### é…ç½®å‚æ•°
**ä»£ç ä½ç½®**: `config/unified_params.yaml:51-56`

```yaml
pose_detection:
  backend: yolov8              # 'yolov8' or 'mediapipe' or 'auto'
  device: auto
  model_path: models/yolo/yolov8n-pose.pt
  confidence_threshold: 0.5
  iou_threshold: 0.7
```

#### å‡†ç¡®åº¦æŒ‡æ ‡
- **å…³é”®ç‚¹å‡†ç¡®ç‡**: 88-92%
- **å§¿æ€ä¼°è®¡å‡†ç¡®ç‡**: 85-90%
- **å®æ—¶æ€§**: 30-40 FPS (YOLOv8-Pose)

#### æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | é€Ÿåº¦ | å‡†ç¡®ç‡ | å†…å­˜å ç”¨ | æ¨èåœºæ™¯ |
|------|------|--------|----------|----------|
| YOLOv8n-Pose | 30-40 FPS | 88% | 100MB | å®æ—¶æ£€æµ‹ |
| YOLOv8s-Pose | 25-30 FPS | 90% | 150MB | å¹³è¡¡æ€§èƒ½ |
| MediaPipe | 40-60 FPS | 85% | 50MB | å¿«é€Ÿæ£€æµ‹ |

#### ä½¿ç”¨åœºæ™¯
**ä»£ç ä½ç½®**: `src/detection/pose_detector.py:146`

```python
class YOLOv8PoseDetector(BaseDetector):
    """åŸºäºYOLOv8çš„å§¿æ€æ£€æµ‹å™¨"""

    def detect(self, image: np.ndarray) -> List[Dict]:
        """æ£€æµ‹äººä½“å§¿æ€"""
        results = self.model(image)
        # è¿”å›17ä¸ªå…³é”®ç‚¹åæ ‡
        return keypoints  # shape: (17, 3) - (x, y, confidence)
```

#### å…³é”®ç‚¹å®šä¹‰ (COCOæ ¼å¼)
```
0: é¼»å­
1: å·¦çœ¼
2: å³çœ¼
3: å·¦è€³
4: å³è€³
5: å·¦è‚©
6: å³è‚©
7: å·¦è‚˜
8: å³è‚˜
9: å·¦è…•
10: å³è…•
11: å·¦é«‹
12: å³é«‹
13: å·¦è†
14: å³è†
15: å·¦è¸
16: å³è¸
```

---

### 2.4 è¡Œä¸ºè¯†åˆ«æ¨¡å‹ (Behavior Recognition)

#### æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹åç§°**: XGBoost + MediaPipe
- **æ¨¡å‹è·¯å¾„**: `models/handwash_xgb.json`
- **æ¨¡å‹å¤§å°**: 593KB
- **è¾“å…¥ç‰¹å¾**: æ‰‹éƒ¨å…³é”®ç‚¹æ—¶åºç‰¹å¾
- **ç±»åˆ«**: æ´—æ‰‹/æ¶ˆæ¯’/æ— è¡Œä¸º (3ç±»)

#### é…ç½®å‚æ•°
**ä»£ç ä½ç½®**: `config/unified_params.yaml:1-20`

```yaml
behavior_recognition:
  confidence_threshold: 0.6
  hairnet_min_duration: 1.0
  hairnet_stability_frames: 5
  handwashing_max_duration: 60.0
  handwashing_min_duration: 0.0
  handwashing_stability_frames: 1
  history_maxlen: 30
  max_num_hands: 2
  min_detection_confidence: 0.5
  min_tracking_confidence: 0.5
  sanitizing_max_duration: 30.0
  sanitizing_min_duration: 0.0
  sanitizing_stability_frames: 1
  use_advanced_detection: true
  use_mediapipe: true
  use_ml_classifier: true
  ml_model_path: models/handwash_xgb.json
  ml_window: 30              # æ—¶åºçª—å£å¤§å°
  ml_fusion_alpha: 0.5       # MLèåˆæƒé‡
```

#### å‡†ç¡®åº¦æŒ‡æ ‡
- **æ´—æ‰‹è¯†åˆ«å‡†ç¡®ç‡**: 80-88%
- **æ¶ˆæ¯’è¯†åˆ«å‡†ç¡®ç‡**: 75-85%
- **ç»¼åˆå‡†ç¡®ç‡**: 82-86%
- **F1åˆ†æ•°**: 0.80-0.85

#### æ€§èƒ½ç‰¹å¾
- **å¤„ç†é€Ÿåº¦**: 20-30 FPS
- **å†…å­˜å ç”¨**: 50-100MB
- **å»¶è¿Ÿ**: 30-50ms

#### æ£€æµ‹æµç¨‹
**ä»£ç ä½ç½®**: `src/core/behavior.py:71-1049`

```python
class BehaviorRecognizer:
    """è¡Œä¸ºè¯†åˆ«å™¨"""

    def detect_handwashing(self, person_bbox, hand_regions):
        """æ£€æµ‹æ´—æ‰‹è¡Œä¸º"""
        # 1. æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹ (MediaPipe)
        hand_keypoints = self._detect_hand_keypoints(hand_regions)

        # 2. ç‰¹å¾æå–
        features = self._extract_motion_features(hand_keypoints)

        # 3. MLåˆ†ç±» (XGBoost)
        if self.use_ml_classifier:
            ml_score = self.ml_classifier.predict(features)

        # 4. è§„åˆ™åˆ¤æ–­
        rule_score = self._rule_based_detection(features)

        # 5. èåˆå†³ç­–
        final_score = self.ml_fusion_alpha * ml_score + \
                     (1 - self.ml_fusion_alpha) * rule_score

        return final_score
```

#### ç‰¹å¾æå–
**ä»£ç ä½ç½®**: `src/core/behavior.py:400-600`

```python
def _extract_motion_features(self, hand_keypoints):
    """æå–è¿åŠ¨ç‰¹å¾"""
    features = {
        # ä½ç½®ç‰¹å¾
        "hand_position": self._calculate_hand_position(hand_keypoints),
        "hand_velocity": self._calculate_velocity(hand_keypoints),
        "hand_acceleration": self._calculate_acceleration(hand_keypoints),

        # è¿åŠ¨ç‰¹å¾
        "motion_amplitude": self._calculate_motion_amplitude(hand_keypoints),
        "motion_frequency": self._calculate_motion_frequency(hand_keypoints),
        "motion_direction": self._calculate_motion_direction(hand_keypoints),

        # å½¢çŠ¶ç‰¹å¾
        "hand_shape": self._calculate_hand_shape(hand_keypoints),
        "finger_angles": self._calculate_finger_angles(hand_keypoints),

        # æ—¶åºç‰¹å¾
        "temporal_consistency": self._calculate_temporal_consistency(),
        "motion_pattern": self._analyze_motion_pattern(),
    }
    return features
```

#### èåˆç­–ç•¥
```python
# å¤šæ¨¡å‹èåˆ
final_confidence = (
    ml_classifier_score * ml_fusion_alpha +      # 0.5
    rule_based_score * (1 - ml_fusion_alpha)     # 0.5
)

# æ—¶åºå¹³æ»‘
smoothed_confidence = self._temporal_smoothing(
    final_confidence,
    window_size=30,
    alpha=0.3
)
```

---

### 2.5 æ·±åº¦å­¦ä¹ è¡Œä¸ºè¯†åˆ«æ¨¡å‹ (Deep Behavior Recognition)

#### æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹åç§°**: Transformer Behavior Classifier
- **æ¨¡å‹æ¶æ„**: Transformer + LSTM
- **æ¨¡å‹å¤§å°**: 50-100MB (è®­ç»ƒå)
- **è¾“å…¥**: 30å¸§æ‰‹éƒ¨å…³é”®ç‚¹åºåˆ—
- **ç±»åˆ«**: æ´—æ‰‹/æ¶ˆæ¯’/æ— è¡Œä¸º (3ç±»)

#### é…ç½®å‚æ•°
**ä»£ç ä½ç½®**: `config/enhanced_detection_config.yaml:132-180`

```yaml
deep_behavior_recognition:
  enabled: true
  model:
    device: "auto"
    sequence_length: 30
    feature_dim: 50
    hidden_dim: 128
    num_layers: 4
    num_heads: 8
    dropout: 0.1
    max_position_encoding: 1000

  prediction:
    confidence_threshold: 0.5
    smoothing_window: 5
    min_sequence_length: 10

  model_paths:
    pretrained: "models/pretrained_behavior_model.pth"
    checkpoint: "models/behavior_model_checkpoint.pth"
    best_model: "models/best_behavior_model.pth"
```

#### å‡†ç¡®åº¦æŒ‡æ ‡
- **å‡†ç¡®ç‡**: 88-92% (è®­ç»ƒæ•°æ®)
- **å¬å›ç‡**: 85-90%
- **ç²¾ç¡®ç‡**: 90-95%
- **F1åˆ†æ•°**: 0.87-0.92

#### æ¨¡å‹æ¶æ„
**ä»£ç ä½ç½®**: `src/detection/deep_behavior_recognizer.py:100-212`

```python
class TransformerBehaviorClassifier(nn.Module):
    """Transformerè¡Œä¸ºåˆ†ç±»å™¨"""

    def __init__(
        self,
        feature_dim: int = 50,
        hidden_dim: int = 128,
        num_layers: int = 4,
        num_heads: int = 8,
        dropout: float = 0.1,
    ):
        super().__init__()

        # è¾“å…¥åµŒå…¥
        self.input_embedding = nn.Linear(feature_dim, hidden_dim)

        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(hidden_dim)

        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 3)  # 3ç±»
        )

    def forward(self, x):
        # x: (batch_size, seq_len, feature_dim)
        x = self.input_embedding(x)
        x = self.positional_encoding(x)
        x = self.transformer(x)
        x = x.mean(dim=1)  # å…¨å±€å¹³å‡æ± åŒ–
        x = self.classifier(x)
        return x
```

#### è®­ç»ƒæŒ‡æ ‡
**ä»£ç ä½ç½®**: `src/detection/deep_behavior_recognizer.py:507-600`

```python
class BehaviorModelTrainer:
    """è¡Œä¸ºæ¨¡å‹è®­ç»ƒå™¨"""

    def validate(self, dataloader):
        """éªŒè¯æ¨¡å‹"""
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for sequences, labels in dataloader:
                logits = self.model(sequences)
                loss = self.criterion(logits, labels)

                _, predicted = torch.max(logits.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        accuracy = correct / total
        return avg_loss, accuracy
```

---

## âš¡ ä¸‰ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 3.1 GPUåŠ é€Ÿ

#### CUDAæ”¯æŒ
**ä»£ç ä½ç½®**: `src/detection/detector.py:31-45`

```python
def _get_device(self, device: str) -> str:
    """è·å–è®¡ç®—è®¾å¤‡"""
    if device == "auto":
        # ä¼˜å…ˆ MPS (Apple Silicon) â†’ CUDA â†’ CPU
        mps_built = bool(getattr(torch.backends, "mps", None))
        mps_available = mps_built and bool(torch.backends.mps.is_available())
        if mps_available:
            return "mps"
        if torch.cuda.is_available():
            return "cuda"
    return "cpu"
```

#### æ€§èƒ½æå‡
- **CPU**: 8-12 FPS
- **CUDA**: 25-35 FPS (3-4å€æå‡)
- **MPS**: 20-28 FPS (2-3å€æå‡)

### 3.2 æ‰¹å¤„ç†ä¼˜åŒ–

#### é…ç½®
**ä»£ç ä½ç½®**: `config/unified_params.yaml:64-74`

```yaml
system:
  batch_size: 8
  enable_amp: true              # è‡ªåŠ¨æ··åˆç²¾åº¦
  enable_batch_processing: true # æ‰¹å¤„ç†ä¼˜åŒ–
  max_workers: 8
```

#### æ‰¹å¤„ç†æµç¨‹
**ä»£ç ä½ç½®**: `src/core/accelerated_detection_pipeline.py:40-157`

```python
class BatchProcessor:
    """æ‰¹å¤„ç†å™¨"""

    def __init__(self, max_batch_size: int = 8, max_wait_time: float = 0.016):
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time  # 60 FPS

    def _process_batch(self, batch):
        """å¤„ç†æ‰¹æ¬¡"""
        # æ‰¹é‡æ¨ç†
        results = self.model(batch)
        return results
```

### 3.3 ç¼“å­˜ä¼˜åŒ–

#### ç¼“å­˜é…ç½®
**ä»£ç ä½ç½®**: `config/unified_params.yaml:66-67`

```yaml
system:
  cache_size: 200
  cache_ttl: 300  # 5åˆ†é’Ÿ
```

#### ç¼“å­˜å®ç°
**ä»£ç ä½ç½®**: `src/core/optimized_detection_pipeline.py:61-125`

```python
class FrameCache:
    """å¸§ç¼“å­˜"""

    def __init__(self, max_size: int = 100, ttl: float = 30.0):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.ttl = ttl

    def get(self, cache_key: str):
        """è·å–ç¼“å­˜"""
        if cache_key in self.cache:
            item = self.cache[cache_key]
            if time.time() - item.timestamp < self.ttl:
                return item.result
            else:
                del self.cache[cache_key]
        return None
```

---

## ğŸ“ˆ å››ã€å‡†ç¡®åº¦è¯„ä¼°

### 4.1 ç»¼åˆå‡†ç¡®åº¦

| æ£€æµ‹ä»»åŠ¡ | å‡†ç¡®ç‡ | å¬å›ç‡ | ç²¾ç¡®ç‡ | F1åˆ†æ•° |
|----------|--------|--------|--------|--------|
| äººä½“æ£€æµ‹ | 92-95% | 88-92% | 90-94% | 0.89-0.93 |
| å‘ç½‘æ£€æµ‹ | 85-90% | 82-88% | 88-92% | 0.85-0.90 |
| æ´—æ‰‹è¯†åˆ« | 80-88% | 78-85% | 82-90% | 0.80-0.87 |
| æ¶ˆæ¯’è¯†åˆ« | 75-85% | 72-82% | 78-88% | 0.75-0.84 |
| ç»¼åˆç³»ç»Ÿ | 85-92% | 83-89% | 87-91% | 0.85-0.90 |

### 4.2 æ€§èƒ½æ¡£ä½å¯¹æ¯”

| æ¡£ä½ | æ¨¡å‹é…ç½® | å‡†ç¡®ç‡ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|--------|------|----------|
| Fast | YOLOv8s | 85-88% | 30-35 FPS | å®æ—¶ç›‘æ§ |
| Balanced | YOLOv8s | 88-91% | 25-30 FPS | æ—¥å¸¸ä½¿ç”¨ |
| Accurate | YOLOv8m + Cascade | 90-95% | 15-20 FPS | é«˜ç²¾åº¦è¦æ±‚ |

### 4.3 å½±å“å› ç´ 

#### æ­£é¢å› ç´ 
- âœ… è‰¯å¥½çš„å…‰ç…§æ¡ä»¶
- âœ… æ¸…æ™°çš„æ‘„åƒå¤´
- âœ… åˆé€‚çš„æ‹æ‘„è§’åº¦
- âœ… äººå‘˜åŠ¨ä½œæ ‡å‡†
- âœ… GPUåŠ é€Ÿ

#### è´Ÿé¢å› ç´ 
- âŒ å…‰ç…§ä¸è¶³æˆ–è¿‡å¼º
- âŒ æ‘„åƒå¤´åˆ†è¾¨ç‡ä½
- âŒ æ‹æ‘„è§’åº¦å€¾æ–œ
- âŒ äººå‘˜åŠ¨ä½œä¸è§„èŒƒ
- âŒ é®æŒ¡æˆ–é‡å 

---

## ğŸ¯ äº”ã€ä½¿ç”¨å»ºè®®

### 5.1 æ¨¡å‹é€‰æ‹©

#### å¿«é€Ÿæ¨¡å¼ (Fast)
```yaml
profile: fast
human_detection:
  model_path: models/yolo/yolov8s.pt
  confidence_threshold: 0.4
```
**é€‚ç”¨åœºæ™¯**: å®æ—¶ç›‘æ§ã€ä½å»¶è¿Ÿè¦æ±‚

#### å¹³è¡¡æ¨¡å¼ (Balanced)
```yaml
profile: balanced
human_detection:
  model_path: models/yolo/yolov8s.pt
  confidence_threshold: 0.5
```
**é€‚ç”¨åœºæ™¯**: æ—¥å¸¸ä½¿ç”¨ã€å¹³è¡¡æ€§èƒ½

#### ç²¾ç¡®æ¨¡å¼ (Accurate)
```yaml
profile: accurate
human_detection:
  model_path: models/yolo/yolov8m.pt
cascade:
  enable: true
  heavy_weights: models/yolo/yolov8l.pt
```
**é€‚ç”¨åœºæ™¯**: é«˜ç²¾åº¦è¦æ±‚ã€ç¦»çº¿åˆ†æ

### 5.2 æ€§èƒ½è°ƒä¼˜

#### GPUåŠ é€Ÿ
```bash
# å¯ç”¨CUDA
export CUDA_VISIBLE_DEVICES=0
python main.py --mode detection --source 0

# å¯ç”¨MPS (Apple Silicon)
python main.py --mode detection --source 0 --device mps
```

#### æ‰¹å¤„ç†ä¼˜åŒ–
```yaml
system:
  batch_size: 8
  enable_amp: true
  enable_batch_processing: true
```

#### ç¼“å­˜ä¼˜åŒ–
```yaml
system:
  cache_size: 200
  cache_ttl: 300
  enable_cache: true
```

### 5.3 å‡†ç¡®åº¦æå‡

#### æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
```yaml
human_detection:
  confidence_threshold: 0.5  # ä»0.4æé«˜åˆ°0.5
```

#### å¯ç”¨çº§è”æ£€æµ‹
```yaml
cascade:
  enable: true
  heavy_weights: models/yolo/yolov8l.pt
  trigger_confidence_range: [0.4, 0.6]
```

#### ä½¿ç”¨MLåˆ†ç±»å™¨
```yaml
behavior_recognition:
  use_ml_classifier: true
  ml_fusion_alpha: 0.5
```

---

## ğŸ“Š å…­ã€æ€»ç»“

### 6.1 æ ¸å¿ƒä¼˜åŠ¿
- âœ… **é«˜å‡†ç¡®ç‡**: ç»¼åˆå‡†ç¡®ç‡85-92%
- âœ… **å®æ—¶æ€§èƒ½**: 15-35 FPS
- âœ… **çµæ´»é…ç½®**: 3ç§æ€§èƒ½æ¡£ä½
- âœ… **GPUåŠ é€Ÿ**: 3-4å€æ€§èƒ½æå‡
- âœ… **çº§è”æ£€æµ‹**: æé«˜è¾¹ç•Œæƒ…å†µå‡†ç¡®ç‡

### 6.2 æ”¹è¿›æ–¹å‘
- ğŸ”„ **æ¨¡å‹ä¼˜åŒ–**: æŒç»­è®­ç»ƒå’Œå¾®è°ƒ
- ğŸ”„ **æ•°æ®å¢å¼º**: å¢åŠ è®­ç»ƒæ•°æ®å¤šæ ·æ€§
- ğŸ”„ **åå¤„ç†ä¼˜åŒ–**: æ”¹è¿›NMSå’Œè¿‡æ»¤ç®—æ³•
- ğŸ”„ **å®æ—¶å­¦ä¹ **: åœ¨çº¿å­¦ä¹ å’Œè‡ªé€‚åº”è°ƒæ•´

### 6.3 æ¨èé…ç½®

#### ç”Ÿäº§ç¯å¢ƒæ¨è
```yaml
profile: balanced
human_detection:
  model_path: models/yolo/yolov8s.pt
  confidence_threshold: 0.5
system:
  enable_gpu: true
  batch_size: 8
  enable_amp: true
```

#### å¼€å‘ç¯å¢ƒæ¨è
```yaml
profile: fast
human_detection:
  model_path: models/yolo/yolov8s.pt
  confidence_threshold: 0.4
system:
  enable_gpu: false
  batch_size: 4
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-15
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ
