# 面向本地化部署的流程合规性检测系统方案（MVP → 数据驱动自学习）

本文档从工程与系统架构视角，结合当前项目实现与运行环境（Apple Silicon，MPS 可用），给出“初步部署（MVP）→灰度增强→自学习闭环”的完整方案，目标是稳定上线、可靠抓拍并高质量采集数据，为后续算法迭代提供坚实基础。

---

## 1. 背景与目标

- 终极目标：稳定识别“发网佩戴 → 洗手 → 烘干 → 工作区”流程，发现违规并抓拍，具备自学习优化能力。
- 现状：
  - YOLOv8 人检与发网检测可用；行为识别（洗手/消毒）有启发式与姿态融合，但准确率仍受场景影响。
  - 项目已支持区域管理、Web API/WS、前端可视化、MPS 加速。
- MVP 阶段目标：以强规则（区域门控 + 停留时长）替代不稳定 AI 行为判断，先行部署，可靠抓拍并沉淀数据。

约束：部署侧以本地化为主，资源有限、需高可靠性、安全可运维；方案需可逐步升级而不破坏现有接口。

---

## 2. 当前能力与差距（与仓库映射）

| 能力 | 现有组件/文件 | 说明 |
| --- | --- | --- |
| 人体检测 | `src/core/detector.py::HumanDetector` | YOLOv8，人检过滤阈值可配，已支持 mps→cuda→cpu 自动选择 |
| 发网检测 | `src/core/yolo_hairnet_detector.py::YOLOHairnetDetector` | YOLO 模型，整帧+人检结果，能输出 has_hairnet |
| 手部/姿态 | `src/core/pose_detector.py::PoseDetectorFactory` | Mediapipe / YOLOv8-Pose，ROI 多尺度增强与关键点映射 |
| 行为识别 | `src/core/behavior.py::BehaviorRecognizer` | 启发式 + 姿态融合 + 可选 ML（XGBoost），可保留为增益信号 |
| 优化管线 | `src/core/optimized_detection_pipeline.py` | 人检→（可选级联）→发网→行为，含帧缓存、统计、可视化 |
| 区域管理 | `src/services/region_service.py`, `src/core/region.py` | 读取 `config/regions.json`，提供多区域管理 |
| 跟踪 | `src/core/tracker.py::MultiObjectTracker` | IoU/中心距离 + Hungarian/greedy，满足 MVP 需求 |
| API/WS | `src/api/app.py` 与路由 | 已可用；WS 端返回标注/统计，兼容旧前端 |
| Demo | `demo_camera_direct.py` | 已集成跟踪、区域融合、违规抓拍、视频优化 |

差距：缺少“流程状态层（基于状态机）”与“事件判断层（统一规则/时序）”的明确抽象，以及标准化的违规抓拍与数据埋点协议。

---

## 3. 总体架构（分层与数据流）

```
              [视频流/图片]
                   |
                   v
        ┌─────────────────────────┐
        │ 感知层 Perception       │ YOLO人检 / 发网YOLO / 手部/姿态
        └─────────┬───────────────┘
                  (人/手/发网对象、bbox、score、landmarks)
                   v
        ┌─────────────────────────┐
        │ 统一目标描述 UOD        │ 绑定 track_id、状态字段（has_hairnet 等）
        └─────────┬───────────────┘
                   v
        ┌─────────────────────────┐
        │ 流程状态层 ProcessState │ 每人状态机（区域进入/离开、停留时长、流程进度）
        └─────────┬───────────────┘
                   v
        ┌─────────────────────────┐
        │ 事件判断层 Events       │ 合规/违规（发网未戴、流程跳跃、停留不足）
        └─────────┬───────────────┘
                   v
        ┌─────────────────────────┐
        │ 动作层 Actions          │ 抓拍（图片+片段）、日志、告警、数据落库
        └─────────────────────────┘
```

### 3.1 层职责与文件映射（建议新增/调整）

- 感知层：沿用现有实现。
- 统一目标描述（UOD）：
  - 新增：`src/core/uod.py`
  - 定义 `UODPerson`（bbox、confidence、track_id、has_hairnet、hand_regions、timestamps）。
- 流程状态层：
  - 新增：`src/core/process_state.py`
  - `PersonState(track_id, has_hairnet, current_region, visited_regions, enter_ts, status)`
  - `ProcessEngine`：接收 UOD + 区域变更，维护状态机与停留计时。
- 事件判断层：
  - 新增：`src/core/event_rules.py`
  - 规则：发网准入、流程顺序、停留时长（阈值可配），输出事件 `Event{type, track_id, ts, evidence}`。
- 动作层：
  - 新增：`src/services/capture_service.py`
  - 统一抓拍/录像/元数据保存（见 §5.4），提供冷却与文件命名规范，匿名化处理。

---

## 4. Phase 1（MVP）——强规则流程合规（可上线）

### 4.1 区域精确标定

- 区域建议：`入口线`、`洗手池区域`、`烘干区域`、`工作区区域`（命名一致以便规则匹配）。
- 标定策略：区域应略大于物理设备，考虑镜头畸变；配置保存在 `config/regions.json`，Demo 页面可视化校验。

### 4.2 状态机与规则

```python
class PersonState:
    def __init__(self, track_id: int):
        self.track_id = track_id
        self.has_hairnet: bool = False
        self.current_region: str | None = None
        self.visited_regions: set[str] = set()
        self.enter_ts: dict[str, float] = {}   # region -> enter timestamp
        self.status: str = "IN_PROGRESS"      # COMPLIANT / VIOLATION
```

规则（核心且可配置）：

1) 发网准入：进入“洗手池区域”若 `has_hairnet=False` → 立即违规事件 `NO_HAIRNET_AT_SINK`。

2) 流程顺序：若从“洗手池区域”直接到“工作区区域”（未到“烘干区域”）→ 违规 `SKIP_DRYING`。

3) 停留时长：
   - 进入“洗手池区域/烘干区域”记录 enter_ts，离开时计算停留 `dt`；若 `dt < MIN_DWELL_SECONDS`（例如 3s）→ 违规 `INSUFFICIENT_DWELL_TIME`。
   - 替代不稳定的 AI 行为识别，作为上线阶段最关键规则。

### 4.3 与现有管线衔接

- 在主入口 `main.py` 的 detection 模式：
  - 启用 `YOLOHairnetDetector`，将发网结果写入 UOD。
  - 引入 `MultiObjectTracker` 以稳定 track_id。
  - 调用 `ProcessEngine.step(uod_batch, region_changes)`，产出事件送 `capture_service`。

### 4.4 抓拍与数据采集规范

目录：

```
output/
  violations/
    no_hairnet/
    skip_drying/
    insufficient_dwell/
  clips/
    <event_id>.mp4
  meta/
    <event_id>.json
```

`meta/<event_id>.json` 示例：

```json
{
  "event_id": "20250905T114437Z_000123",
  "type": "NO_HAIRNET_AT_SINK",
  "ts": 1757044477.336,
  "track_id": 7,
  "regions": ["入口线", "洗手池区域"],
  "person_bbox": [x1, y1, x2, y2],
  "has_hairnet": false,
  "dwell_seconds": 1.2,
  "uod_snapshot": {"persons": [...], "hands": [...]} ,
  "source": {"video": "tests/fixtures/videos/20250724072822_175680.mp4", "frame": 348}
}
```

要求：
- 抓拍图片默认匿名化（人脸或上部模糊，项目已有实现）。
- 片段时长：前 2s + 后 3s，保证可回放与再标注。
- 冷却策略：同一 `track_id` 同类事件 10s 内不重复抓拍。

### 4.5 性能预算（Apple Silicon, MPS）

- 人检 YOLOv8n 384×640：~5–10ms/帧（已验证）。
- 发网 YOLO：接近人检，ROI 策略可降低开销。
- 手检/姿态：Mediapipe 在 CPU 模式可控；MPS 上 YOLOv8-Pose 可选。
- 跟踪与状态机：轻量，基本不构成瓶颈。

---

## 5. Phase 2（灰度增强）——引入 AI 行为识别

### 5.1 数据到模型的闭环

- 用 Phase 1 采集的违规与合规片段构建高质量数据集（洗手、烘干）。
- 训练模型（可先用手部关键点序列 + XGBoost/轻量时序网络）。

### 5.2 双轨灰度与 A/B 测试

- 线上同时运行“强规则”与“AI 推断”，最终判定仍以规则为准。
- 记录模型输出置信度，与规则结论做一致性评估；达到阈值且稳定后逐步放权（仍保留规则兜底）。

---

## 6. Phase 3（自学习）——自动收集、重训练、灰度上线

- 定期抽样正常流程片段作为“合规”负样本，结合违规抓拍构成训练集。
- 设定样本阈值自动触发重训练；评测通过后，进入 Phase 2 的灰度流程。

---

## 7. 配置与参数（建议）

`config/unified_params.yaml` 新增/强化：

```yaml
runtime:
  frame_skip: 1
  similarity_threshold: 0.98   # 可在主入口引入相似度跳过

cascade:
  enable: true
  heavy_weights: models/yolo/yolov8l.pt
  trigger_confidence_range: [0.4, 0.6]
  trigger_roi: null            # 可按入口通道配置

process:
  min_dwell_seconds:
    sink: 3
    dryer: 3
  cooldown_seconds: 10
  region_names:
    entrance: "入口线"
    sink: "洗手池区域"
    dryer: "烘干区域"
    work: "工作区区域"
```

---

## 8. API/WS 影响与兼容

- REST：可新增 `/api/v1/events/recent` 返回近期事件、图片与元数据。
- WS：在实时消息中加入 `events: [...]` 字段（事件类型、track_id、ts、hint）。
- 不破坏现有 `/detect/comprehensive` 返回结构；仅增加字段。

---

## 9. 部署与监控

- 设备选择：默认 mps→cuda→cpu；打印设备日志（已在 `main.py` 添加）。
- 运行监控：
  - 性能指标：人检推理耗时、后处理耗时、端到端 FPS。
  - 业务指标：事件数量/类型、抓拍产出量、重复率、无效率。
  - 健康检查：`/health`，日志轮转，崩溃自恢复脚本。

---

## 10. 验证与 KPI（MVP）

- 漏报率（关键违规场景）：< 10%
- 误报率：< 5%
- 抓拍可解释性（含片段与元数据）：> 95%
- 端到端延迟（单路 1080p@20fps 输入）：满足现场需求（可通过跳帧/相似度降低计算）。

---

## 11. 任务清单（文件级）

1) 新增模块：
   - `src/core/uod.py`：统一目标描述数据结构与转换（感知结果→UOD）。
   - `src/core/process_state.py`：`PersonState` 与 `ProcessEngine`（状态机，区域进入/离开、停留计时）。
   - `src/core/event_rules.py`：事件规则（发网准入、顺序校验、停留阈值）与可配置阈值。
   - `src/services/capture_service.py`：抓拍（匿名化）、片段导出、元数据写入、冷却机制。

2) 主入口改造：
   - `main.py::run_detection`：
     - 启用 `YOLOHairnetDetector` 并将结果写入 UOD。
     - 引入 `MultiObjectTracker` 产出稳定 track_id。
     - 接入 `ProcessEngine` 与 `event_rules`，将事件推给 `capture_service`。
     - 可选引入 `VideoStreamOptimizer`（跳帧/相似度）。

3) 配置：
   - `unified_params.yaml` 增加 `process.*`、完善 `cascade.*` 与 `runtime.*`。
   - `regions.json` 标定并回传 Demo 页面验证。

4) API/WS：
   - 可选新增 `/api/v1/events/recent`，WS 返回中新增 `events` 字段。

---

## 12. 风险与回退

- 行为 AI 不稳定 → 用区域 + 停留规则兜底（MVP 核心）。
- 场景变化（灯光/遮挡）→ 强化区域标定与人检过滤阈值；必要时切换人检权重（`yolov8s/m`）。
- 计算压力 → 跳帧、相似度跳过、降低 imgsz；后续可切换 TensorRT/ONNX Runtime（CUDA）。
- 设备差异 → 统一设备选择（mps→cuda→cpu），打印设备日志，允许 CLI 覆盖。

---

### 附录 A：UOD 结构（建议）

```python
class UODPerson(TypedDict):
    track_id: int
    bbox: list[int]           # [x1,y1,x2,y2]
    confidence: float
    has_hairnet: bool
    hairnet_confidence: float
    hand_regions: list[dict]  # 可能含 landmarks
    region: str | None
    ts: float
```

### 附录 B：目录与命名

```
output/
  violations/
    no_hairnet/
    skip_drying/
    insufficient_dwell/
  clips/
    <event_id>.mp4
  meta/
    <event_id>.json
```

> 本文与仓库当前代码完全兼容：MVP 仅新增独立模块与主入口组装层改动，不影响现有 API/WS 与 Demo；可分支灰度合入。



## 13. 进一步优化建议（可选，默认关闭）

> 本节汇总在不显著增加复杂度的前提下的增强点，均可渐进接入、随时回退，优先保证兼容与可运维性。

### 13.1 数据结构健壮性（TypedDict → dataclass / Pydantic）
- 价值/现状：当前 `UODPerson` 等用 TypedDict，仅静态约束，运行时缺少校验与统一序列化。
- 建议：
  - 优先使用 `dataclasses` 定义内部数据模型；在 API/持久化边界按需使用 Pydantic 模型（可选依赖）。
  - 统一 `asdict()/model_dump()` 出口，保持对外字段不变。
- 改动范围：新增 `src/core/schemas.py`（或并入 `uod.py`）；替换 UOD 生成/消费处（3–5 个文件，<150 行）。
- 影响：更早暴露数据问题，错误信息清晰；默认不改变 API/WS 字段。

### 13.2 架构可扩展性（预留事件总线与分布式演进）
- 价值/现状：当前单进程通信高效；为将来“感知层 ↔ 事件层”解耦预留接口。
- 建议：
  - 定义 `EventBus` 接口，默认实现 `InprocEventBus`（进程内队列）；预留 Kafka/RabbitMQ/gRPC 适配器空壳。
  - `ProcessEngine` 通过 `EventBus.publish()` 输出事件（本地实现等价于直接调用）。
- 改动范围：新增 `src/infrastructure/event_bus.py`；接入点 1–2 处（<120 行）。
- 影响：默认行为不变；未来切分布式仅替换实现，不动业务层。

### 13.3 测试策略（单元/集成/端到端）
- 价值/现状：规则/状态机对误报漏报敏感，需专门测试覆盖；当前缺少面向流程规则的用例。
- 建议：
  - 单元测试：`tests/test_event_rules.py` 覆盖 NO_HAIRNET / SKIP_DRYING / DWELL_TIME 边界与阈值。
  - 集成测试：`tests/test_process_engine.py` 用区域进入/离开时序驱动状态机并断言事件。
  - 端到端：基于真实录制视频片段的小评测集，离线断言事件与人工标注一致性，作为发布准入。
- 改动范围：新增 3–5 个测试文件（~200–400 行）。
- 影响：无线上影响；显著降低规则调整回归风险。

### 13.4 视觉跟踪鲁棒性（ID Switch 缓解与可升级）
- 价值/现状：IoU/中心距离在高密度/遮挡场景可能 ID 切换，影响流程顺序与停留计算。
- 建议（渐进）：
  - 短期增强：在现有 `MultiObjectTracker` 增加速度平滑与 HSV 直方图相似度作为二级匹配，降低短时切换。
  - 预留接口：抽象 `Tracker` 接口，默认实现为现有跟踪器；为 DeepSORT / BoT-SORT 适配器留扩展点（Phase 2 再引入依赖）。
- 改动范围：轻改 `src/core/tracker.py`（~120–250 行）；可选新增 `src/core/tracking/adapters.py`。
- 影响：性能开销小、可回退；默认不开启高级跟踪依赖。
