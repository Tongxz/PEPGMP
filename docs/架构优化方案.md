# 系统架构优化方案：基于Redis通信与可扩展调度层

## 1. 背景与目标

### 1.1 背景
当前“人体行为检测系统”已进入稳定迭代阶段。在现有单服务器、内网部署的模式下，为了提升系统的健壮性、实时性与长期可维护性，有必要对当前架构中的部分环节进行优化。

本项目方案充分考虑以下核心前提：
- **内网环境:** 系统部署在与公网隔离的内部网络中。
- **简化部署:** 运维要求简单，不应引入复杂的外部依赖或重量级的编排工具。
- **利用现有技术栈:** 项目已引入并使用Redis，应最大化利用该组件。

### 1.2 优化目标
1.  **提升通信健壮性:** 废除当前通过解析日志文件获取统计信息的方式，替换为更可靠、高效的进程间通信（IPC）机制。
2.  **增强架构扩展性:** 在当前单机架构内部进行重构，为未来平滑演进到多服务器部署模式预留清晰的扩展点。
3.  **保障实时体验:** 将监控数据的延迟从秒级降低到毫秒级，提升前端用户体验。
4.  **维持运维简易性:** 所有改造应在不显著增加部署和运维复杂度的情况下完成。

---

## 2. 核心问题分析

### 2.1. 通信机制的脆弱性
当前系统通过API后端读取并解析检测进程输出的日志文件来获取运行状态。此方式存在三大风险：
- **格式强耦合:** API的统计功能与日志文本格式强耦合，任何对日志格式的微小修改都可能导致统计功能静默失效，排查困难。
- **并发风险:** 高并发读取与持续写入同一个文件可能引发文件锁、数据不完整等问题。
- **性能损耗:** 频繁的磁盘I/O操作在高负载下会成为性能瓶颈。

### 2.2. 架构的扩展性限制
当前API层的`ProcessManager`直接调用`subprocess`在本地启动检测进程。这种设计将“启动一个检测”的**意图**与“在本地用子进程方式执行”的**实现**硬编码在一起，导致：
- **缺乏抽象:** 当未来希望将检测任务分发到另一台服务器或以不同方式（如Docker容器）运行时，必须修改API层的核心逻辑。
- **扩展困难:** 无法在不进行大规模重构的前提下，将系统从单机部署平滑过渡到多机部署。

---

## 3. 整体优化方案

针对上述问题，我们提出两大核心改造措施：

1.  **引入Redis Pub/Sub作为实时数据总线:** 利用项目中已有的Redis，实现检测进程与API后端之间结构化、实时的信息传递。
2.  **构建抽象调度层以实现未来扩展:** 在API层与具体的进程执行逻辑之间增加一个“调度器”抽象层，实现意图与执行的解耦。

---

## 4. 方案一：实时数据总线实施细则

### 4.1. Redis频道与消息格式定义

- **频道 (Channel):**
  - `hbd:stats`: 用于发布实时的性能与检测统计数据。
  - `hbd:events`: 用于发布关键的违规事件（如发现违规行为）。

- **消息格式 (JSON):**
  ```json
  // 发布到 hbd:stats 的消息示例
  {
    "type": "stats",
    "camera_id": "vid1",
    "timestamp": 1678886400.123,
    "data": {
      "fps": 15.2,
      "persons": 2,
      "hairnets": 1,
      "handwash": 0
    }
  }
  ```

### 4.2. 检测进程改造 (`main.py`)
在检测循环的末尾，增加向Redis发布消息的逻辑。

```python
# 示例代码
import json
import time
# from utils.redis_client import get_redis_connection # 假设项目中有此工具函数

def publish_to_redis(redis_conn, channel, data):
    try:
        redis_conn.publish(channel, json.dumps(data))
    except Exception as e:
        logger.warning(f"Failed to publish to Redis: {e}")

# 在 _run_detection_loop 的循环中...
# ... 计算完统计数据后 ...
# redis_conn = get_redis_connection()
stats_message = {
    "type": "stats", "camera_id": args.camera_id,
    "timestamp": time.time(), "data": {"fps": avg_fps, "persons": person_count, ...}
}
publish_to_redis(redis_conn, "hbd:stats", stats_message)
```

### 4.3. API后端改造 (FastAPI)
后端应用启动时，创建一个后台任务监听Redis频道，并将数据实时更新到内存缓存中。

```python
# 示例代码: app.py 或一个专门的缓存模块
import asyncio
import json

# 全局内存缓存，存储所有摄像头的最新统计信息
CAMERA_STATS_CACHE = {}

async def redis_stats_listener():
    # ... 获取Redis连接并订阅 hbd:stats 频道 ...
    while True:
        message = await pubsub.get_message(ignore_subscribe_messages=True)
        if message:
            try:
                data = json.loads(message['data'])
                if data.get('type') == 'stats':
                    CAMERA_STATS_CACHE[data['camera_id']] = data
            except (json.JSONDecodeError, TypeError):
                pass # 忽略格式错误的消息

@app.on_event("startup")
async def startup_event():
    # FastAPI启动时，在后台启动监听器
    asyncio.create_task(redis_stats_listener())

# 改造 /stats 接口
@router.get("/cameras/{camera_id}/stats")
def get_camera_stats(camera_id: str):
    stats = CAMERA_STATS_CACHE.get(camera_id)
    if not stats:
        raise HTTPException(status_code=404, detail="No real-time stats available.")
    return stats
```

### 4.4. 收益总结
- **实时性:** 数据延迟从秒级降低到毫秒级。
- **高可靠:** 彻底消除因日志格式变更或文件读写冲突导致的问题。
- **高性能:** API响应时间稳定在毫秒级（纯内存读取）。
- **零新增依赖:** 充分利用了项目现有的Redis。

---

## 5. 方案二：抽象调度层实施细则

### 5.1. 职责分离：重构`ProcessManager`
将当前的`ProcessManager`更名为`LocalProcessExecutor`，其职责被严格限定为：**只在本地操作系统上通过`subprocess`启动和管理进程**。其内部方法不变。

### 5.2. 引入`DetectionScheduler`
创建一个新的调度器模块，作为API层和执行层之间的桥梁。

```python
# 新文件: src/services/scheduler.py
from .local_executor import LocalProcessExecutor # 导入重构后的执行器

class DetectionScheduler:
    def __init__(self, config):
        # 当前单机模式下，执行器固定为本地执行器
        self.executor = LocalProcessExecutor()
        # 未来可根据配置选择不同的执行器，实现扩展
        # if config.mode == 'swarm': self.executor = SwarmExecutor()

    def start_detection(self, camera_id: str):
        """调度一个检测任务的启动"""
        # 目前直接将任务传递给本地执行器
        return self.executor.start(camera_id)

    def stop_detection(self, camera_id: str):
        return self.executor.stop(camera_id)

# 提供单例
_scheduler = None
def get_scheduler():
    global _scheduler
    if _scheduler is None:
        _scheduler = DetectionScheduler(load_app_config())
    return _scheduler
```

### 5.3. API层解耦
修改API路由，使其只依赖于`DetectionScheduler`的抽象接口。

```python
# 文件: src/api/routers/cameras.py
from src.services.scheduler import get_scheduler # 导入新的调度器

@router.post("/cameras/{camera_id}/start")
def start_camera(camera_id: str):
    scheduler = get_scheduler()
    # API层只负责下达“启动”的意图，不关心如何执行
    res = scheduler.start_detection(camera_id)
    # ...
```

### 5.4. 未来扩展路径
此架构为未来向多服务器部署的演进提供了极大的便利。当需要支持时，只需：
1.  **新增执行器:** 创建一个新的`SshExecutor`或`SwarmExecutor`类，实现与`LocalProcessExecutor`相同的接口方法。
2.  **修改调度器:** 在`DetectionScheduler`的初始化逻辑中，根据配置文件选择加载对应的执行器实例。

整个过程中，**API层的代码无需任何改动**，完美实现了对上层业务的透明。

---

## 6. 补充建议

以下建议与本次核心改造不冲突，可作为并行的优化任务。

- **自适应检测:** 在检测循环中引入基于帧差异的运动分析，动态调整跳帧率，以在保证不丢失关键事件的前提下，进一步优化资源利用率。
- **本地化MLOps:** 采用MLflow的本地文件模式和DVC，对模型训练和数据集进行版本化管理，提升AI开发的规范性和可复现性。

---

## 7. 行动计划与优先级

1.  **第一阶段 (立即实施):**
    - **任务1:** 按照方案一，完成Redis Pub/Sub的对接，废除所有日志解析代码。
    - **任务2:** 按照方案二，完成`DetectionScheduler`抽象层的引入和`ProcessManager`的重构。

2.  **第二阶段 (持续优化):**
    - **任务3:** 在检测算法中，实施自适应检测逻辑。
    - **任务4:** 在模型训练脚本中，集成MLflow进行实验追踪。

---

## 8. 总结

本方案在充分尊重“内网简化部署”核心前提的基础上，通过引入Redis Pub/Sub和抽象调度层，精准地解决了当前架构在**健壮性**和**扩展性**方面的核心痛点。方案改动清晰、风险可控，不仅能显著提升当前系统的质量，也为未来的发展构建了坚实而灵活的架构基础。
