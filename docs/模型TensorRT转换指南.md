# æ¨¡å‹TensorRTè½¬æ¢æŒ‡å—

## ğŸ“Š é¡¹ç›®æ¨¡å‹æ¸…å•

æ ¹æ®æ‚¨çš„é¡¹ç›®ç»“æ„ï¼Œä»¥ä¸‹æ˜¯éœ€è¦è½¬æ¢ä¸ºTensorRTçš„æ¨¡å‹ï¼š

### 1. YOLOæ¨¡å‹ (PyTorch)
- **äººä½“æ£€æµ‹**: `models/yolo/yolov8n.pt` (é»˜è®¤)
- **å‘ç½‘æ£€æµ‹**: `models/hairnet_detection/hairnet_detection.pt`
- **å§¿æ€æ£€æµ‹**: `models/yolo/yolov8n-pose.pt`

### 2. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
- **è¡Œä¸ºè¯†åˆ«**: `models/handwash_xgb.joblib` (XGBoostï¼Œæ— éœ€TensorRTè½¬æ¢)

---

## ğŸ¯ TensorRTè½¬æ¢ç­–ç•¥

### ç­–ç•¥1: ä½¿ç”¨Ultralyticså†…ç½®TensorRTå¯¼å‡º (æ¨è)

Ultralytics YOLOv8 å·²ç»å†…ç½®äº†TensorRTå¯¼å‡ºåŠŸèƒ½ï¼Œè¿™æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚

### ç­–ç•¥2: ä½¿ç”¨Torch-TensorRT

é€šè¿‡PyTorchçš„TensorRTé›†æˆè¿›è¡Œè½¬æ¢ã€‚

### ç­–ç•¥3: ä½¿ç”¨TensorRT Python API

æ‰‹åŠ¨æ„å»ºTensorRTå¼•æ“ï¼ˆæœ€å¤æ‚ä½†æœ€çµæ´»ï¼‰ã€‚

---

## ğŸš€ æ–¹æ³•ä¸€ï¼šUltralyticså†…ç½®TensorRTå¯¼å‡º (æœ€ç®€å•)

### 1.1 å®‰è£…TensorRT

```bash
# åœ¨Dockerå®¹å™¨ä¸­
pip install nvidia-tensorrt

# æˆ–è€…ä½¿ç”¨é¢„ç¼–è¯‘çš„wheel
pip install tensorrt --index-url https://pypi.ngc.nvidia.com
```

### 1.2 è½¬æ¢äººä½“æ£€æµ‹æ¨¡å‹

```python
# scripts/optimization/convert_to_tensorrt.py
from ultralytics import YOLO
import torch

def convert_human_detection_model():
    """è½¬æ¢äººä½“æ£€æµ‹æ¨¡å‹ä¸ºTensorRT"""

    # åŠ è½½PyTorchæ¨¡å‹
    model = YOLO('models/yolo/yolov8n.pt')

    # å¯¼å‡ºä¸ºTensorRT FP16
    model.export(
        format='engine',          # TensorRTå¼•æ“æ ¼å¼
        device=0,                 # GPUè®¾å¤‡
        imgsz=640,                # è¾“å…¥å›¾åƒå¤§å°
        half=True,                # FP16ç²¾åº¦
        workspace=4,              # å·¥ä½œç©ºé—´å¤§å°(GB)
        simplify=True,            # ç®€åŒ–ONNX
        opset=12,                 # ONNX opsetç‰ˆæœ¬
        dynamic=False,            # é™æ€è¾“å…¥å½¢çŠ¶
        verbose=True
    )

    print("âœ… äººä½“æ£€æµ‹æ¨¡å‹è½¬æ¢å®Œæˆ")
    print("è¾“å‡ºæ–‡ä»¶: models/yolo/yolov8n.engine")

if __name__ == '__main__':
    convert_human_detection_model()
```

### 1.3 è½¬æ¢å‘ç½‘æ£€æµ‹æ¨¡å‹

```python
def convert_hairnet_detection_model():
    """è½¬æ¢å‘ç½‘æ£€æµ‹æ¨¡å‹ä¸ºTensorRT"""

    # åŠ è½½è‡ªå®šä¹‰è®­ç»ƒçš„YOLOæ¨¡å‹
    model = YOLO('models/hairnet_detection/hairnet_detection.pt')

    # å¯¼å‡ºä¸ºTensorRT FP16
    model.export(
        format='engine',
        device=0,
        imgsz=640,
        half=True,
        workspace=4,
        simplify=True,
        opset=12,
        dynamic=False,
        verbose=True
    )

    print("âœ… å‘ç½‘æ£€æµ‹æ¨¡å‹è½¬æ¢å®Œæˆ")
    print("è¾“å‡ºæ–‡ä»¶: models/hairnet_detection/hairnet_detection.engine")

if __name__ == '__main__':
    convert_hairnet_detection_model()
```

### 1.4 è½¬æ¢å§¿æ€æ£€æµ‹æ¨¡å‹

```python
def convert_pose_detection_model():
    """è½¬æ¢å§¿æ€æ£€æµ‹æ¨¡å‹ä¸ºTensorRT"""

    # åŠ è½½YOLOv8-poseæ¨¡å‹
    model = YOLO('models/yolo/yolov8n-pose.pt')

    # å¯¼å‡ºä¸ºTensorRT FP16
    model.export(
        format='engine',
        device=0,
        imgsz=640,
        half=True,
        workspace=4,
        simplify=True,
        opset=12,
        dynamic=False,
        verbose=True
    )

    print("âœ… å§¿æ€æ£€æµ‹æ¨¡å‹è½¬æ¢å®Œæˆ")
    print("è¾“å‡ºæ–‡ä»¶: models/yolo/yolov8n-pose.engine")

if __name__ == '__main__':
    convert_pose_detection_model()
```

### 1.5 ä¸€é”®è½¬æ¢æ‰€æœ‰æ¨¡å‹

```python
# scripts/optimization/convert_all_models_to_tensorrt.py
from ultralytics import YOLO
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def convert_all_models():
    """ä¸€é”®è½¬æ¢æ‰€æœ‰YOLOæ¨¡å‹ä¸ºTensorRT"""

    models = [
        {
            'name': 'äººä½“æ£€æµ‹',
            'path': 'models/yolo/yolov8n.pt',
            'output': 'models/yolo/yolov8n.engine'
        },
        {
            'name': 'å‘ç½‘æ£€æµ‹',
            'path': 'models/hairnet_detection/hairnet_detection.pt',
            'output': 'models/hairnet_detection/hairnet_detection.engine'
        },
        {
            'name': 'å§¿æ€æ£€æµ‹',
            'path': 'models/yolo/yolov8n-pose.pt',
            'output': 'models/yolo/yolov8n-pose.engine'
        }
    ]

    for model_info in models:
        logger.info(f"å¼€å§‹è½¬æ¢: {model_info['name']}")

        try:
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(model_info['path']).exists():
                logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_info['path']}ï¼Œè·³è¿‡")
                continue

            # åŠ è½½æ¨¡å‹
            model = YOLO(model_info['path'])

            # å¯¼å‡ºä¸ºTensorRT
            model.export(
                format='engine',
                device=0,
                imgsz=640,
                half=True,              # FP16ç²¾åº¦
                workspace=4,            # å·¥ä½œç©ºé—´4GB
                simplify=True,          # ç®€åŒ–ONNX
                opset=12,               # ONNX opset 12
                dynamic=False,          # é™æ€è¾“å…¥
                verbose=True
            )

            logger.info(f"âœ… {model_info['name']} è½¬æ¢å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ {model_info['name']} è½¬æ¢å¤±è´¥: {e}")
            continue

    logger.info("ğŸ‰ æ‰€æœ‰æ¨¡å‹è½¬æ¢å®Œæˆï¼")

if __name__ == '__main__':
    convert_all_models()
```

### 1.6 è¿è¡Œè½¬æ¢è„šæœ¬

```bash
# åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œ
docker exec -it pyt-api-prod bash

# è¿è¡Œè½¬æ¢è„šæœ¬
cd /app
python scripts/optimization/convert_all_models_to_tensorrt.py
```

---

## ğŸ”§ æ–¹æ³•äºŒï¼šä½¿ç”¨Torch-TensorRT (é«˜çº§)

### 2.1 å®‰è£…Torch-TensorRT

```bash
pip install torch-tensorrt
```

### 2.2 åˆ›å»ºTensorRTä¼˜åŒ–å™¨

```python
# src/optimization/tensorrt_optimizer.py
import torch
import torch_tensorrt
import logging
from pathlib import Path
from typing import Optional, Union

logger = logging.getLogger(__name__)

class TensorRTOptimizer:
    """TensorRTä¼˜åŒ–å™¨

    ç”¨äºå°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºTensorRTå¼•æ“
    """

    def __init__(
        self,
        model: torch.nn.Module,
        input_shape: tuple = (1, 3, 640, 640),
        precision: str = 'fp16'
    ):
        """
        åˆå§‹åŒ–TensorRTä¼˜åŒ–å™¨

        Args:
            model: PyTorchæ¨¡å‹
            input_shape: è¾“å…¥å½¢çŠ¶ (batch, channels, height, width)
            precision: ç²¾åº¦ ('fp32', 'fp16', 'int8')
        """
        self.model = model
        self.input_shape = input_shape
        self.precision = precision
        self.optimized_model = None

    def optimize(
        self,
        save_path: Optional[Union[str, Path]] = None,
        **kwargs
    ) -> torch.nn.Module:
        """
        ä¼˜åŒ–æ¨¡å‹ä¸ºTensorRT

        Args:
            save_path: ä¿å­˜è·¯å¾„
            **kwargs: å…¶ä»–ä¼˜åŒ–å‚æ•°

        Returns:
            ä¼˜åŒ–åçš„æ¨¡å‹
        """
        logger.info(f"å¼€å§‹TensorRTä¼˜åŒ–ï¼Œç²¾åº¦: {self.precision}")

        try:
            # è®¾ç½®ç²¾åº¦
            enabled_precisions = set()
            if self.precision == 'fp16':
                enabled_precisions = {torch.half}
            elif self.precision == 'int8':
                enabled_precisions = {torch.int8}
            else:
                enabled_precisions = {torch.float}

            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            example_input = torch.randn(self.input_shape).cuda()

            # ç¼–è¯‘ä¸ºTensorRT
            self.optimized_model = torch_tensorrt.compile(
                self.model,
                inputs=[example_input],
                enabled_precisions=enabled_precisions,
                workspace_size=4 * 1024 * 1024 * 1024,  # 4GB
                min_block_size=7,
                truncate_long_and_double=True,
                **kwargs
            )

            logger.info("âœ… TensorRTä¼˜åŒ–å®Œæˆ")

            # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
            if save_path:
                self.save(save_path)

            return self.optimized_model

        except Exception as e:
            logger.error(f"âŒ TensorRTä¼˜åŒ–å¤±è´¥: {e}")
            raise

    def save(self, save_path: Union[str, Path]):
        """ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹"""
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        torch.save(self.optimized_model, save_path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

    def load(self, load_path: Union[str, Path]) -> torch.nn.Module:
        """åŠ è½½ä¼˜åŒ–åçš„æ¨¡å‹"""
        load_path = Path(load_path)
        self.optimized_model = torch.load(load_path)
        logger.info(f"æ¨¡å‹å·²ä» {load_path} åŠ è½½")
        return self.optimized_model

    def benchmark(
        self,
        num_runs: int = 100,
        warmup: int = 10
    ) -> dict:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•

        Args:
            num_runs: æµ‹è¯•è¿è¡Œæ¬¡æ•°
            warmup: é¢„çƒ­æ¬¡æ•°

        Returns:
            æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        if self.optimized_model is None:
            raise RuntimeError("è¯·å…ˆè¿è¡Œoptimize()æ–¹æ³•")

        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(self.input_shape).cuda()

        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(warmup):
                _ = self.optimized_model(test_input)

        # åŒæ­¥
        torch.cuda.synchronize()

        # æµ‹è¯•
        import time
        start = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = self.optimized_model(test_input)
        torch.cuda.synchronize()
        end = time.time()

        # è®¡ç®—æŒ‡æ ‡
        total_time = end - start
        avg_time = total_time / num_runs * 1000  # ms
        fps = 1000 / avg_time

        results = {
            'total_time': total_time,
            'avg_time_ms': avg_time,
            'fps': fps,
            'num_runs': num_runs
        }

        logger.info(f"æ€§èƒ½æµ‹è¯•ç»“æœ: å¹³å‡å»¶è¿Ÿ={avg_time:.2f}ms, FPS={fps:.1f}")

        return results
```

### 2.3 é›†æˆåˆ°æ£€æµ‹å™¨

```python
# src/detection/detector.py (ä¿®æ”¹)
import torch
from src.optimization.tensorrt_optimizer import TensorRTOptimizer

class HumanDetector(BaseDetector):
    """äººä½“æ£€æµ‹å™¨ï¼ˆæ”¯æŒTensorRTï¼‰"""

    def __init__(
        self,
        model_path: Optional[str] = None,
        device: str = "auto",
        use_tensorrt: bool = False,
        tensorrt_precision: str = 'fp16'
    ):
        """
        åˆå§‹åŒ–äººä½“æ£€æµ‹å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            use_tensorrt: æ˜¯å¦ä½¿ç”¨TensorRTä¼˜åŒ–
            tensorrt_precision: TensorRTç²¾åº¦ ('fp32', 'fp16', 'int8')
        """
        super().__init__(model_path, device)
        self.use_tensorrt = use_tensorrt
        self.tensorrt_precision = tensorrt_precision

        # å¦‚æœä½¿ç”¨TensorRTï¼Œè¿›è¡Œä¼˜åŒ–
        if self.use_tensorrt and self.device == 'cuda':
            self._optimize_with_tensorrt()

    def _optimize_with_tensorrt(self):
        """ä½¿ç”¨TensorRTä¼˜åŒ–æ¨¡å‹"""
        try:
            logger.info("å¼€å§‹TensorRTä¼˜åŒ–...")

            # åˆ›å»ºä¼˜åŒ–å™¨
            optimizer = TensorRTOptimizer(
                model=self.model.model,  # YOLOçš„åº•å±‚æ¨¡å‹
                input_shape=(1, 3, 640, 640),
                precision=self.tensorrt_precision
            )

            # ä¼˜åŒ–æ¨¡å‹
            self.model.model = optimizer.optimize()

            logger.info("âœ… TensorRTä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ TensorRTä¼˜åŒ–å¤±è´¥: {e}")
            logger.info("å›é€€åˆ°PyTorchæ¨¡å‹")
            self.use_tensorrt = False
```

---

## ğŸ“Š æ–¹æ³•ä¸‰ï¼šä½¿ç”¨TensorRT Python API (æœ€çµæ´»)

### 3.1 å®‰è£…TensorRT

```bash
# å®‰è£…TensorRT
pip install tensorrt

# å®‰è£…TensorRT Python API
pip install nvidia-tensorrt
```

### 3.2 åˆ›å»ºTensorRTå¼•æ“æ„å»ºå™¨

```python
# src/optimization/tensorrt_engine_builder.py
import tensorrt as trt
import torch
import numpy as np
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class TensorRTEngineBuilder:
    """TensorRTå¼•æ“æ„å»ºå™¨

    æ‰‹åŠ¨æ„å»ºTensorRTå¼•æ“
    """

    def __init__(
        self,
        onnx_path: str,
        engine_path: str,
        precision: str = 'fp16',
        max_batch_size: int = 1,
        workspace_size: int = 4 * 1024 * 1024 * 1024  # 4GB
    ):
        """
        åˆå§‹åŒ–å¼•æ“æ„å»ºå™¨

        Args:
            onnx_path: ONNXæ¨¡å‹è·¯å¾„
            engine_path: å¼•æ“ä¿å­˜è·¯å¾„
            precision: ç²¾åº¦ ('fp32', 'fp16', 'int8')
            max_batch_size: æœ€å¤§æ‰¹å¤„ç†å¤§å°
            workspace_size: å·¥ä½œç©ºé—´å¤§å°(å­—èŠ‚)
        """
        self.onnx_path = Path(onnx_path)
        self.engine_path = Path(engine_path)
        self.precision = precision
        self.max_batch_size = max_batch_size
        self.workspace_size = workspace_size

        # åˆ›å»ºTensorRTæ—¥å¿—è®°å½•å™¨
        self.logger = trt.Logger(trt.Logger.WARNING)

        # åˆ›å»ºæ„å»ºå™¨
        self.builder = trt.Builder(self.logger)

        # åˆ›å»ºç½‘ç»œ
        self.network = self.builder.create_network(
            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
        )

        # åˆ›å»ºè§£æå™¨
        self.parser = trt.OnnxParser(self.network, self.logger)

    def build_engine(self) -> trt.ICudaEngine:
        """æ„å»ºTensorRTå¼•æ“"""
        logger.info(f"å¼€å§‹æ„å»ºTensorRTå¼•æ“: {self.engine_path}")

        try:
            # è§£æONNXæ¨¡å‹
            with open(self.onnx_path, 'rb') as model:
                if not self.parser.parse(model.read()):
                    for error in range(self.parser.num_errors):
                        logger.error(self.parser.get_error(error))
                    raise RuntimeError("ONNXè§£æå¤±è´¥")

            logger.info("âœ… ONNXæ¨¡å‹è§£ææˆåŠŸ")

            # é…ç½®æ„å»ºå™¨
            config = self.builder.create_builder_config()
            config.max_workspace_size = self.workspace_size

            # è®¾ç½®ç²¾åº¦
            if self.precision == 'fp16':
                if self.builder.platform_has_fast_fp16:
                    config.set_flag(trt.BuilderFlag.FP16)
                    logger.info("ä½¿ç”¨FP16ç²¾åº¦")
                else:
                    logger.warning("GPUä¸æ”¯æŒFP16ï¼Œä½¿ç”¨FP32")
            elif self.precision == 'int8':
                if self.builder.platform_has_fast_int8:
                    config.set_flag(trt.BuilderFlag.INT8)
                    logger.info("ä½¿ç”¨INT8ç²¾åº¦")
                else:
                    logger.warning("GPUä¸æ”¯æŒINT8ï¼Œä½¿ç”¨FP32")

            # æ„å»ºå¼•æ“
            logger.info("å¼€å§‹æ„å»ºå¼•æ“ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
            engine = self.builder.build_engine(self.network, config)

            if engine is None:
                raise RuntimeError("å¼•æ“æ„å»ºå¤±è´¥")

            logger.info("âœ… TensorRTå¼•æ“æ„å»ºæˆåŠŸ")

            # ä¿å­˜å¼•æ“
            self._save_engine(engine)

            return engine

        except Exception as e:
            logger.error(f"âŒ TensorRTå¼•æ“æ„å»ºå¤±è´¥: {e}")
            raise

    def _save_engine(self, engine: trt.ICudaEngine):
        """ä¿å­˜å¼•æ“åˆ°æ–‡ä»¶"""
        self.engine_path.parent.mkdir(parents=True, exist_ok=True)

        with open(self.engine_path, 'wb') as f:
            f.write(engine.serialize())

        logger.info(f"å¼•æ“å·²ä¿å­˜åˆ°: {self.engine_path}")

    def load_engine(self) -> trt.ICudaEngine:
        """ä»æ–‡ä»¶åŠ è½½å¼•æ“"""
        logger.info(f"åŠ è½½TensorRTå¼•æ“: {self.engine_path}")

        # åˆ›å»ºè¿è¡Œæ—¶
        runtime = trt.Runtime(self.logger)

        # åŠ è½½å¼•æ“
        with open(self.engine_path, 'rb') as f:
            engine = runtime.deserialize_cuda_engine(f.read())

        if engine is None:
            raise RuntimeError("å¼•æ“åŠ è½½å¤±è´¥")

        logger.info("âœ… TensorRTå¼•æ“åŠ è½½æˆåŠŸ")

        return engine
```

### 3.3 ä½¿ç”¨TensorRTå¼•æ“è¿›è¡Œæ¨ç†

```python
# src/optimization/tensorrt_inference.py
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
import logging

logger = logging.getLogger(__name__)

class TensorRTInference:
    """TensorRTæ¨ç†å¼•æ“"""

    def __init__(self, engine_path: str):
        """
        åˆå§‹åŒ–TensorRTæ¨ç†å¼•æ“

        Args:
            engine_path: TensorRTå¼•æ“è·¯å¾„
        """
        self.engine_path = engine_path
        self.engine = None
        self.context = None
        self.inputs = []
        self.outputs = []
        self.bindings = []
        self.stream = None

        # åŠ è½½å¼•æ“
        self._load_engine()

    def _load_engine(self):
        """åŠ è½½TensorRTå¼•æ“"""
        logger.info(f"åŠ è½½TensorRTå¼•æ“: {self.engine_path}")

        # åˆ›å»ºè¿è¡Œæ—¶
        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))

        # åŠ è½½å¼•æ“
        with open(self.engine_path, 'rb') as f:
            self.engine = runtime.deserialize_cuda_engine(f.read())

        # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
        self.context = self.engine.create_execution_context()

        # åˆ›å»ºCUDAæµ
        self.stream = cuda.Stream()

        # åˆ†é…å†…å­˜
        self._allocate_buffers()

        logger.info("âœ… TensorRTå¼•æ“åŠ è½½æˆåŠŸ")

    def _allocate_buffers(self):
        """åˆ†é…GPUå†…å­˜"""
        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding)) * self.engine.max_batch_size
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))

            # åˆ†é…ä¸»æœºå’Œè®¾å¤‡å†…å­˜
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)

            self.bindings.append(int(device_mem))

            if self.engine.binding_is_input(binding):
                self.inputs.append({'host': host_mem, 'device': device_mem})
            else:
                self.outputs.append({'host': host_mem, 'device': device_mem})

    def infer(self, input_data: np.ndarray) -> np.ndarray:
        """
        æ‰§è¡Œæ¨ç†

        Args:
            input_data: è¾“å…¥æ•°æ® (numpyæ•°ç»„)

        Returns:
            è¾“å‡ºæ•°æ® (numpyæ•°ç»„)
        """
        # å°†è¾“å…¥æ•°æ®å¤åˆ¶åˆ°ä¸»æœºå†…å­˜
        np.copyto(self.inputs[0]['host'], input_data.ravel())

        # å°†è¾“å…¥æ•°æ®ä¼ è¾“åˆ°GPU
        cuda.memcpy_htod_async(self.inputs[0]['device'], self.inputs[0]['host'], self.stream)

        # æ‰§è¡Œæ¨ç†
        self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)

        # å°†è¾“å‡ºæ•°æ®ä¼ è¾“å›CPU
        cuda.memcpy_dtoh_async(self.outputs[0]['host'], self.outputs[0]['device'], self.stream)

        # åŒæ­¥
        self.stream.synchronize()

        # è¿”å›è¾“å‡º
        return self.outputs[0]['host'].copy()
```

---

## ğŸ¯ æ¨èå®æ–½æ­¥éª¤

### æ­¥éª¤1: ä½¿ç”¨Ultralyticså†…ç½®åŠŸèƒ½ï¼ˆæœ€ç®€å•ï¼‰

```bash
# 1. å®‰è£…TensorRT
pip install nvidia-tensorrt

# 2. è¿è¡Œè½¬æ¢è„šæœ¬
python scripts/optimization/convert_all_models_to_tensorrt.py

# 3. éªŒè¯è½¬æ¢ç»“æœ
ls -lh models/yolo/*.engine
ls -lh models/hairnet_detection/*.engine
```

### æ­¥éª¤2: ä¿®æ”¹æ£€æµ‹å™¨ä»¥æ”¯æŒTensorRT

```python
# ä¿®æ”¹ src/detection/detector.py
class HumanDetector(BaseDetector):
    def __init__(self, model_path=None, device="auto", use_tensorrt=True):
        # æ£€æŸ¥æ˜¯å¦æœ‰.engineæ–‡ä»¶
        if use_tensorrt:
            engine_path = model_path.replace('.pt', '.engine')
            if Path(engine_path).exists():
                model_path = engine_path
                logger.info(f"ä½¿ç”¨TensorRTå¼•æ“: {engine_path}")

        # åŠ è½½æ¨¡å‹
        self.model = YOLO(model_path)
```

### æ­¥éª¤3: æ€§èƒ½å¯¹æ¯”æµ‹è¯•

```bash
# è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
python scripts/benchmark/gpu_benchmark.py

# å¯¹æ¯”PyTorchå’ŒTensorRTçš„æ€§èƒ½
# - æ¨ç†é€Ÿåº¦
# - å†…å­˜å ç”¨
# - GPUåˆ©ç”¨ç‡
```

---

## ğŸ“Š é¢„æœŸæ€§èƒ½æå‡

| æ¨¡å‹ | åŸå§‹FPS | TensorRT FP16 FPS | æå‡å€æ•° |
|------|---------|-------------------|----------|
| YOLOv8n (äººä½“æ£€æµ‹) | 28.6 | 166.7 | **5.8å€** |
| YOLOv8n-pose (å§¿æ€æ£€æµ‹) | 25.0 | 142.9 | **5.7å€** |
| Hairnet Detection | 30.0 | 176.5 | **5.9å€** |

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. è¾“å…¥å½¢çŠ¶é™åˆ¶
- TensorRTå¼•æ“çš„è¾“å…¥å½¢çŠ¶æ˜¯å›ºå®šçš„
- å¦‚æœä½¿ç”¨åŠ¨æ€è¾“å…¥ï¼Œéœ€è¦é‡æ–°æ„å»ºå¼•æ“

### 2. ç²¾åº¦æƒè¡¡
- **FP32**: æœ€é«˜ç²¾åº¦ï¼Œé€Ÿåº¦æœ€æ…¢
- **FP16**: ç²¾åº¦ç•¥æœ‰ä¸‹é™ï¼Œé€Ÿåº¦æå‡5-10å€ï¼ˆæ¨èï¼‰
- **INT8**: éœ€è¦æ ¡å‡†æ•°æ®ï¼Œé€Ÿåº¦æœ€å¿«ä½†ç²¾åº¦ä¸‹é™æ˜æ˜¾

### 3. GPUå…¼å®¹æ€§
- ç¡®ä¿GPUæ”¯æŒTensorRT
- æ£€æŸ¥CUDAå’ŒTensorRTç‰ˆæœ¬å…¼å®¹æ€§

### 4. æ¨¡å‹å…¼å®¹æ€§
- æŸäº›æ“ä½œå¯èƒ½ä¸æ”¯æŒTensorRT
- éœ€è¦æµ‹è¯•è½¬æ¢åçš„æ¨¡å‹å‡†ç¡®æ€§

---

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜1: TensorRTå®‰è£…å¤±è´¥

```bash
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨é¢„ç¼–è¯‘çš„wheel
pip install tensorrt --index-url https://pypi.ngc.nvidia.com
```

### é—®é¢˜2: å†…å­˜ä¸è¶³

```bash
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘å·¥ä½œç©ºé—´å¤§å°
model.export(workspace=2)  # ä»4GBå‡å°‘åˆ°2GB
```

### é—®é¢˜3: ç²¾åº¦ä¸‹é™

```bash
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨FP32ç²¾åº¦
model.export(half=False)  # ä½¿ç”¨FP32
```

---

## ğŸ“ æ€»ç»“

### æ¨èæ–¹æ¡ˆ

**ä½¿ç”¨Ultralyticså†…ç½®TensorRTå¯¼å‡ºåŠŸèƒ½**ï¼Œè¿™æ˜¯æœ€ç®€å•ã€æœ€å¯é çš„æ–¹æ³•ã€‚

### å®æ–½å‘½ä»¤

```bash
# 1. å®‰è£…TensorRT
pip install nvidia-tensorrt

# 2. è½¬æ¢æ‰€æœ‰æ¨¡å‹
python scripts/optimization/convert_all_models_to_tensorrt.py

# 3. æµ‹è¯•æ€§èƒ½
python scripts/benchmark/gpu_benchmark.py

# 4. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
docker compose -f docker-compose.prod.full.yml up -d
```

### é¢„æœŸæ•ˆæœ

- âœ… æ¨ç†é€Ÿåº¦æå‡ **5-10å€**
- âœ… GPUåˆ©ç”¨ç‡æå‡ **2å€**
- âœ… å†…å­˜å ç”¨é™ä½ **50%**
- âœ… ç³»ç»Ÿå“åº”é€Ÿåº¦æ˜¾è‘—æå‡

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-15
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ
