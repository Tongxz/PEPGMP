# CUDAã€cuDNNã€TensorRTæ ¸å¿ƒåº“åº”ç”¨æŒ‡å—

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

æœ¬æŒ‡å—è¯¦ç»†é˜è¿°å¦‚ä½•å°†CUDAã€cuDNNã€TensorRTç­‰æ ¸å¿ƒåº“åº”ç”¨åˆ°é¡¹ç›®ä¸­ï¼Œå®ç°**2-10å€çš„æ€§èƒ½æå‡**ã€‚

### æ ¸å¿ƒåº“ä½œç”¨
- **CUDA**: NVIDIA GPUå¹¶è¡Œè®¡ç®—å¹³å°
- **cuDNN**: æ·±åº¦ç¥ç»ç½‘ç»œåŠ é€Ÿåº“
- **TensorRT**: é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¨ç†å¼•æ“
- **Torch-TensorRT**: PyTorchä¸TensorRTé›†æˆ

---

## ğŸ¯ ä¸€ã€æ ¸å¿ƒåº“æ¦‚è¿°

### 1.1 åº“çš„ä½œç”¨å’Œå…³ç³»

```
åº”ç”¨å±‚
  â†“
PyTorch / TensorFlow
  â†“
CUDA Runtime API
  â†“
cuDNN (æ·±åº¦ç¥ç»ç½‘ç»œåŠ é€Ÿ)
  â†“
TensorRT (æ¨ç†ä¼˜åŒ–å¼•æ“)
  â†“
GPUç¡¬ä»¶
```

### 1.2 æ€§èƒ½æå‡å¯¹æ¯”

| ä¼˜åŒ–æ–¹æ¡ˆ | é€Ÿåº¦æå‡ | ç²¾åº¦å½±å“ | å®æ–½éš¾åº¦ | æ¨èåœºæ™¯ |
|----------|----------|----------|----------|----------|
| CUDAåŸºç¡€ | 2-3å€ | æ—  | ä½ | æ‰€æœ‰GPUç¯å¢ƒ |
| cuDNNä¼˜åŒ– | +30-50% | æ—  | ä½ | æ·±åº¦å­¦ä¹ æ¨¡å‹ |
| TensorRT FP32 | 3-5å€ | æ—  | ä¸­ | ç”Ÿäº§æ¨ç† |
| TensorRT FP16 | 5-10å€ | Â±0.1% | ä¸­ | å®æ—¶æ¨ç† |
| TensorRT INT8 | 10-20å€ | -1-3% | é«˜ | è¾¹ç¼˜è®¾å¤‡ |

---

## âš¡ äºŒã€CUDAåº”ç”¨

### 2.1 CUDAåŸºç¡€é…ç½®

#### å½“å‰çŠ¶æ€åˆ†æ
**ä»£ç ä½ç½®**: `src/utils/gpu_acceleration.py:135-178`

```python
def _apply_cuda_optimizations(self, device_info: Dict[str, Any]) -> list:
    """åº”ç”¨CUDAä¼˜åŒ–è®¾ç½®"""
    optimizations = []

    # 1. ç¯å¢ƒå˜é‡ä¼˜åŒ–
    cuda_env = {
        "CUDA_LAUNCH_BLOCKING": "0",  # å¼‚æ­¥æ‰§è¡Œ
        "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:512,roundup_power2_divisions:16",
        "CUBLAS_WORKSPACE_CONFIG": ":16:8",
        "CUDA_MODULE_LOADING": "LAZY",
        "TORCH_CUDNN_V8_API_ENABLED": "1",
    }

    # 2. CuDNNä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

    # 3. TF32ä¼˜åŒ– (Ampereæ¶æ„)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    return optimizations
```

#### CUDAç¯å¢ƒå˜é‡è¯¦è§£

```bash
# 1. CUDA_LAUNCH_BLOCKING
# æ§åˆ¶CUDAæ ¸å‡½æ•°çš„æ‰§è¡Œæ¨¡å¼
export CUDA_LAUNCH_BLOCKING=0  # å¼‚æ­¥æ‰§è¡Œï¼Œæå‡æ€§èƒ½

# 2. PYTORCH_CUDA_ALLOC_CONF
# PyTorch CUDAå†…å­˜åˆ†é…å™¨é…ç½®
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512,roundup_power2_divisions:16"
# - max_split_size_mb: æœ€å¤§å†…å­˜å—å¤§å°
# - roundup_power2_divisions: å†…å­˜å¯¹é½ä¼˜åŒ–

# 3. CUBLAS_WORKSPACE_CONFIG
# cuBLASå·¥ä½œç©ºé—´é…ç½®
export CUBLAS_WORKSPACE_CONFIG=":16:8"
# æ ¼å¼: :<å‰å‘>:<åå‘>

# 4. CUDA_MODULE_LOADING
# CUDAæ¨¡å—åŠ è½½ç­–ç•¥
export CUDA_MODULE_LOADING="LAZY"  # å»¶è¿ŸåŠ è½½ï¼Œå‡å°‘å¯åŠ¨æ—¶é—´

# 5. TORCH_CUDNN_V8_API_ENABLED
# å¯ç”¨cuDNN v8 API
export TORCH_CUDNN_V8_API_ENABLED="1"
```

### 2.2 CUDAæµä¼˜åŒ–

#### å¤šæµå¹¶è¡Œå¤„ç†

```python
import torch
import torch.cuda as cuda

class CudaStreamManager:
    """CUDAæµç®¡ç†å™¨"""

    def __init__(self, num_streams=4):
        self.num_streams = num_streams
        self.streams = [cuda.Stream() for _ in range(num_streams)]
        self.current_stream = 0

    def get_stream(self):
        """è·å–å½“å‰æµ"""
        stream = self.streams[self.current_stream]
        self.current_stream = (self.current_stream + 1) % self.num_streams
        return stream

    def synchronize_all(self):
        """åŒæ­¥æ‰€æœ‰æµ"""
        for stream in self.streams:
            stream.synchronize()

# ä½¿ç”¨ç¤ºä¾‹
stream_manager = CudaStreamManager(num_streams=4)

def parallel_inference(model, images):
    """å¹¶è¡Œæ¨ç†"""
    results = []

    for i, image in enumerate(images):
        stream = stream_manager.get_stream()

        with cuda.stream(stream):
            # æ•°æ®ç§»åŠ¨åˆ°GPU
            image_gpu = image.cuda(non_blocking=True)

            # æ¨ç†
            with torch.no_grad():
                result = model(image_gpu)

            results.append(result.cpu())

    # åŒæ­¥æ‰€æœ‰æµ
    stream_manager.synchronize_all()

    return results
```

### 2.3 CUDAå†…å­˜æ± ä¼˜åŒ–

```python
class CudaMemoryPool:
    """CUDAå†…å­˜æ± ç®¡ç†å™¨"""

    def __init__(self):
        self.pool = {}

    def allocate(self, shape, dtype=torch.float32):
        """åˆ†é…å†…å­˜"""
        key = (shape, dtype)

        if key not in self.pool:
            self.pool[key] = torch.empty(shape, dtype=dtype, device='cuda')

        return self.pool[key]

    def clear(self):
        """æ¸…ç©ºå†…å­˜æ± """
        self.pool.clear()
        torch.cuda.empty_cache()

# ä½¿ç”¨ç¤ºä¾‹
memory_pool = CudaMemoryPool()

def optimized_inference(model, image):
    """ä¼˜åŒ–çš„æ¨ç†"""
    # ä»å†…å­˜æ± åˆ†é…
    image_gpu = memory_pool.allocate(image.shape, image.dtype)
    image_gpu.copy_(image)

    # æ¨ç†
    with torch.no_grad():
        result = model(image_gpu)

    return result
```

---

## ğŸš€ ä¸‰ã€cuDNNåº”ç”¨

### 3.1 cuDNNè‡ªåŠ¨è°ƒä¼˜

#### å½“å‰é…ç½®
**ä»£ç ä½ç½®**: `src/utils/gpu_acceleration.py:156-159`

```python
# CuDNNä¼˜åŒ–
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
```

#### æ·±åº¦ä¼˜åŒ–

```python
def configure_cudnn_optimizations():
    """é…ç½®cuDNNä¼˜åŒ–"""
    import torch

    # 1. å¯ç”¨åŸºå‡†æµ‹è¯•æ¨¡å¼
    # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•ï¼Œé€‚åˆå›ºå®šè¾“å…¥å°ºå¯¸
    torch.backends.cudnn.benchmark = True

    # 2. éç¡®å®šæ€§æ¨¡å¼
    # å…è®¸ä½¿ç”¨éç¡®å®šæ€§ç®—æ³•ï¼Œæå‡æ€§èƒ½
    torch.backends.cudnn.deterministic = False

    # 3. TF32ä¼˜åŒ– (Ampereæ¶æ„)
    # ä½¿ç”¨TensorFloat-32ç²¾åº¦ï¼Œæå‡æ€§èƒ½
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # 4. å…è®¸ä½¿ç”¨cuDNN
    torch.backends.cudnn.enabled = True

    # 5. è®¾ç½®cuDNNç‰ˆæœ¬
    if hasattr(torch.backends.cudnn, 'version'):
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")

# åœ¨ç¨‹åºå¯åŠ¨æ—¶è°ƒç”¨
configure_cudnn_optimizations()
```

### 3.2 cuDNNç®—æ³•é€‰æ‹©

```python
def select_optimal_cudnn_algorithm(conv_layer, input_shape):
    """é€‰æ‹©æœ€ä¼˜cuDNNç®—æ³•"""
    import torch
    import torch.nn as nn

    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(*input_shape).cuda()

    # è·å–æ‰€æœ‰å¯ç”¨ç®—æ³•
    algorithms = []

    # æµ‹è¯•ä¸åŒç®—æ³•
    for algo in ['IMPLICIT_GEMM', 'IMPLICIT_PRECOMP_GEMM',
                 'GEMM', 'DIRECT', 'FFT', 'FFT_TILING',
                 'WINOGRAD', 'WINOGRAD_NONFUSED']:
        try:
            # è®¾ç½®ç®—æ³•
            torch.backends.cudnn.benchmark = False
            torch.backends.cudnn.deterministic = True

            # æµ‹è¯•æ€§èƒ½
            start = torch.cuda.Event(enable_timing=True)
            end = torch.cuda.Event(enable_timing=True)

            start.record()
            _ = conv_layer(x)
            end.record()

            torch.cuda.synchronize()
            elapsed_time = start.elapsed_time(end)

            algorithms.append((algo, elapsed_time))
        except:
            pass

    # é€‰æ‹©æœ€å¿«çš„ç®—æ³•
    best_algo = min(algorithms, key=lambda x: x[1])

    return best_algo[0]
```

### 3.3 cuDNNæ€§èƒ½ç›‘æ§

```python
class CudnnProfiler:
    """cuDNNæ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.profiles = []

    def profile_conv_layer(self, conv_layer, input_shape):
        """åˆ†æå·ç§¯å±‚æ€§èƒ½"""
        import torch

        # é¢„çƒ­
        x = torch.randn(*input_shape).cuda()
        for _ in range(10):
            _ = conv_layer(x)

        # æ€§èƒ½æµ‹è¯•
        torch.cuda.synchronize()
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()
        for _ in range(100):
            _ = conv_layer(x)
        end.record()

        torch.cuda.synchronize()
        elapsed_time = start.elapsed_time(end) / 100

        self.profiles.append({
            'layer': conv_layer,
            'input_shape': input_shape,
            'elapsed_time': elapsed_time
        })

        return elapsed_time

    def report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        total_time = sum(p['elapsed_time'] for p in self.profiles)

        print("cuDNNæ€§èƒ½åˆ†ææŠ¥å‘Š:")
        print(f"æ€»æ—¶é—´: {total_time:.2f}ms")
        print("\nå„å±‚æ€§èƒ½:")
        for i, profile in enumerate(self.profiles):
            print(f"  å±‚{i}: {profile['elapsed_time']:.2f}ms")
```

---

## ğŸ”¥ å››ã€TensorRTåº”ç”¨

### 4.1 TensorRTåŸºç¡€

#### ä»€ä¹ˆæ˜¯TensorRT
TensorRTæ˜¯NVIDIAçš„é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¨ç†å¼•æ“ï¼Œå¯ä»¥å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¼˜åŒ–ä¸ºé«˜æ•ˆçš„æ¨ç†å¼•æ“ã€‚

#### æ€§èƒ½ä¼˜åŠ¿
- **é€Ÿåº¦æå‡**: 3-10å€
- **å†…å­˜ä¼˜åŒ–**: å‡å°‘50-70%
- **ç²¾åº¦ä¿æŒ**: FP32å‡ ä¹æ— æŸï¼ŒFP16æŸå¤±<0.1%

### 4.2 TensorRTå®‰è£…

#### å‰ç½®æ¡ä»¶
```bash
# 1. æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version

# 2. æ£€æŸ¥cuDNNç‰ˆæœ¬
cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR

# 3. æ£€æŸ¥GPUè®¡ç®—èƒ½åŠ›
nvidia-smi --query-gpu=compute_cap --format=csv
```

#### å®‰è£…æ­¥éª¤

```bash
# æ–¹å¼1: ä½¿ç”¨pipå®‰è£…
pip install nvidia-tensorrt

# æ–¹å¼2: ä½¿ç”¨torch-tensorrt (æ¨è)
pip install torch-tensorrt

# æ–¹å¼3: ä»NVIDIAå®˜ç½‘ä¸‹è½½
# https://developer.nvidia.com/tensorrt
```

#### éªŒè¯å®‰è£…

```python
import tensorrt as trt
import torch_tensorrt

print(f"TensorRTç‰ˆæœ¬: {trt.__version__}")
print(f"Torch-TensorRTç‰ˆæœ¬: {torch_tensorrt.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
```

### 4.3 YOLOv8 TensorRTä¼˜åŒ–

#### æ–¹æ³•1: ä½¿ç”¨YOLOåŸç”Ÿå¯¼å‡º

```bash
# å¯¼å‡ºä¸ºTensorRTæ ¼å¼
yolo export model=yolov8s.pt format=tensorrt device=0

# å¯¼å‡ºä¸ºTensorRT FP16
yolo export model=yolov8s.pt format=tensorrt device=0 half=True

# å¯¼å‡ºä¸ºTensorRT INT8
yolo export model=yolov8s.pt format=tensorrt device=0 int8=True

# æŒ‡å®šè¾“å…¥å°ºå¯¸
yolo export model=yolov8s.pt format=tensorrt device=0 imgsz=640
```

#### æ–¹æ³•2: ä½¿ç”¨torch-tensorrt

```python
import torch
import torch_tensorrt

# åŠ è½½PyTorchæ¨¡å‹
model = YOLO("yolov8s.pt")
model.eval()

# å‡†å¤‡è¾“å…¥
example_input = torch.randn(1, 3, 640, 640).cuda()

# ç¼–è¯‘ä¸ºTensorRT
trt_model = torch_tensorrt.compile(
    model,
    inputs=[example_input],
    enabled_precisions={torch.half},  # FP16
    workspace_size=1 << 30,  # 1GBå·¥ä½œç©ºé—´
    min_block_size=7,
    torch_executed_ops={"torch.ops.aten.add"}  # æŒ‡å®šåœ¨PyTorchä¸­æ‰§è¡Œçš„ç®—å­
)

# ä¿å­˜TensorRTæ¨¡å‹
torch.jit.save(trt_model, "yolov8s_trt.ts")

# åŠ è½½TensorRTæ¨¡å‹
trt_model = torch.jit.load("yolov8s_trt.ts")
```

#### æ–¹æ³•3: ONNX â†’ TensorRT

```python
# æ­¥éª¤1: å¯¼å‡ºONNX
yolo export model=yolov8s.pt format=onnx

# æ­¥éª¤2: ONNXè½¬TensorRT
import tensorrt as trt

def onnx_to_tensorrt(onnx_path, trt_path, fp16=True):
    """ONNXè½¬TensorRT"""
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network(
        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    )
    parser = trt.OnnxParser(network, logger)

    # è§£æONNXæ¨¡å‹
    with open(onnx_path, 'rb') as model:
        if not parser.parse(model.read()):
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            return False

    # é…ç½®æ„å»ºå™¨
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB

    if fp16:
        config.set_flag(trt.BuilderFlag.FP16)

    # æ„å»ºå¼•æ“
    engine = builder.build_engine(network, config)

    # ä¿å­˜å¼•æ“
    with open(trt_path, 'wb') as f:
        f.write(engine.serialize())

    return True

# ä½¿ç”¨
onnx_to_tensorrt("yolov8s.onnx", "yolov8s.trt", fp16=True)
```

### 4.4 é›†æˆåˆ°é¡¹ç›®

#### åˆ›å»ºTensorRTæ£€æµ‹å™¨

```python
# src/detection/tensorrt_detector.py
import torch
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np

class TensorRTDetector:
    """TensorRTæ£€æµ‹å™¨"""

    def __init__(self, engine_path, input_shape=(1, 3, 640, 640)):
        self.input_shape = input_shape
        self.engine = self._load_engine(engine_path)
        self.context = self.engine.create_execution_context()

        # åˆ†é…GPUå†…å­˜
        self.inputs, self.outputs, self.bindings, self.stream = \
            self._allocate_buffers()

    def _load_engine(self, engine_path):
        """åŠ è½½TensorRTå¼•æ“"""
        with open(engine_path, 'rb') as f:
            engine_data = f.read()

        logger = trt.Logger(trt.Logger.WARNING)
        runtime = trt.Runtime(logger)
        engine = runtime.deserialize_cuda_engine(engine_data)

        return engine

    def _allocate_buffers(self):
        """åˆ†é…GPUå†…å­˜"""
        inputs = []
        outputs = []
        bindings = []
        stream = cuda.Stream()

        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding)) * \
                   self.engine.max_batch_size
            dtype = trt.nptype(self.engine.get_binding_dtype(binding))

            # åˆ†é…ä¸»æœºå’Œè®¾å¤‡å†…å­˜
            host_mem = cuda.pagelocked_empty(size, dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)

            bindings.append(int(device_mem))

            if self.engine.binding_is_input(binding):
                inputs.append({'host': host_mem, 'device': device_mem})
            else:
                outputs.append({'host': host_mem, 'device': device_mem})

        return inputs, outputs, bindings, stream

    def detect(self, image):
        """æ‰§è¡Œæ£€æµ‹"""
        # é¢„å¤„ç†å›¾åƒ
        input_data = self._preprocess(image)

        # å¤åˆ¶åˆ°GPU
        np.copyto(self.inputs[0]['host'], input_data.ravel())
        cuda.memcpy_htod_async(
            self.inputs[0]['device'],
            self.inputs[0]['host'],
            self.stream
        )

        # æ‰§è¡Œæ¨ç†
        self.context.execute_async_v2(
            bindings=self.bindings,
            stream_handle=self.stream.handle
        )

        # å¤åˆ¶ç»“æœå›CPU
        cuda.memcpy_dtoh_async(
            self.outputs[0]['host'],
            self.outputs[0]['device'],
            self.stream
        )
        self.stream.synchronize()

        # åå¤„ç†
        output = self.outputs[0]['host']
        results = self._postprocess(output)

        return results

    def _preprocess(self, image):
        """é¢„å¤„ç†å›¾åƒ"""
        # å®ç°é¢„å¤„ç†é€»è¾‘
        pass

    def _postprocess(self, output):
        """åå¤„ç†è¾“å‡º"""
        # å®ç°åå¤„ç†é€»è¾‘
        pass
```

#### é›†æˆåˆ°æ£€æµ‹ç®¡é“

```python
# src/core/optimized_detection_pipeline.py
from src.detection.tensorrt_detector import TensorRTDetector

class OptimizedDetectionPipeline:
    """ä¼˜åŒ–çš„æ£€æµ‹ç®¡é“"""

    def __init__(self, use_tensorrt=True):
        self.use_tensorrt = use_tensorrt

        if use_tensorrt:
            # ä½¿ç”¨TensorRTæ£€æµ‹å™¨
            self.human_detector = TensorRTDetector(
                "models/yolo/yolov8s.trt",
                input_shape=(1, 3, 640, 640)
            )
        else:
            # ä½¿ç”¨æ ‡å‡†æ£€æµ‹å™¨
            self.human_detector = HumanDetector()

    def detect_comprehensive(self, image):
        """ç»¼åˆæ£€æµ‹"""
        # ä½¿ç”¨ä¼˜åŒ–çš„æ£€æµ‹å™¨
        person_detections = self.human_detector.detect(image)

        # åç»­å¤„ç†...
        return result
```

### 4.5 TensorRTæ€§èƒ½å¯¹æ¯”

#### åŸºå‡†æµ‹è¯•

```python
import time

def benchmark_model(model, input_shape, num_iterations=100):
    """åŸºå‡†æµ‹è¯•"""
    import torch

    # å‡†å¤‡è¾“å…¥
    input_data = torch.randn(*input_shape).cuda()

    # é¢„çƒ­
    for _ in range(10):
        _ = model(input_data)

    torch.cuda.synchronize()

    # æµ‹è¯•
    start = time.time()
    for _ in range(num_iterations):
        _ = model(input_data)
    torch.cuda.synchronize()
    end = time.time()

    avg_time = (end - start) / num_iterations * 1000  # ms
    fps = 1000 / avg_time

    return avg_time, fps

# æµ‹è¯•ä¸åŒæ¨¡å‹
models = {
    'PyTorch FP32': pytorch_model_fp32,
    'PyTorch FP16': pytorch_model_fp16,
    'TensorRT FP32': trt_model_fp32,
    'TensorRT FP16': trt_model_fp16,
    'TensorRT INT8': trt_model_int8,
}

for name, model in models.items():
    avg_time, fps = benchmark_model(model, (1, 3, 640, 640))
    print(f"{name}: {avg_time:.2f}ms ({fps:.1f} FPS)")
```

#### é¢„æœŸæ€§èƒ½

| æ¨¡å‹ | å»¶è¿Ÿ (ms) | FPS | é€Ÿåº¦æå‡ |
|------|-----------|-----|----------|
| PyTorch FP32 | 30-40 | 25-33 | 1x |
| PyTorch FP16 | 20-25 | 40-50 | 1.5x |
| TensorRT FP32 | 10-15 | 67-100 | 3x |
| TensorRT FP16 | 5-8 | 125-200 | 6x |
| TensorRT INT8 | 3-5 | 200-333 | 10x |

---

## ğŸ¯ äº”ã€å®Œæ•´é›†æˆæ–¹æ¡ˆ

### 5.1 åˆ›å»ºTensorRTä¼˜åŒ–æ¨¡å—

```python
# src/optimization/tensorrt_optimizer.py
import torch
import torch_tensorrt
import logging
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)

class TensorRTOptimizer:
    """TensorRTä¼˜åŒ–å™¨"""

    def __init__(self, model, input_shape=(1, 3, 640, 640)):
        self.model = model
        self.input_shape = input_shape
        self.optimized_model = None

    def optimize(self, precision='fp16', workspace_size=1<<30):
        """ä¼˜åŒ–æ¨¡å‹"""
        logger.info(f"å¼€å§‹TensorRTä¼˜åŒ–ï¼Œç²¾åº¦: {precision}")

        try:
            # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
            example_input = torch.randn(*self.input_shape).cuda()

            # ç¼–è¯‘é…ç½®
            enabled_precisions = set()
            if precision == 'fp16':
                enabled_precisions = {torch.half}
            elif precision == 'int8':
                enabled_precisions = {torch.int8}

            # ç¼–è¯‘æ¨¡å‹
            self.optimized_model = torch_tensorrt.compile(
                self.model,
                inputs=[example_input],
                enabled_precisions=enabled_precisions,
                workspace_size=workspace_size,
                min_block_size=7,
                truncate_long_and_double=True,
            )

            logger.info("TensorRTä¼˜åŒ–å®Œæˆ")
            return self.optimized_model

        except Exception as e:
            logger.error(f"TensorRTä¼˜åŒ–å¤±è´¥: {e}")
            return None

    def save(self, path):
        """ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹"""
        if self.optimized_model is not None:
            torch.jit.save(self.optimized_model, path)
            logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
        else:
            logger.error("æ¨¡å‹æœªä¼˜åŒ–ï¼Œæ— æ³•ä¿å­˜")

    def load(self, path):
        """åŠ è½½ä¼˜åŒ–åçš„æ¨¡å‹"""
        self.optimized_model = torch.jit.load(path)
        logger.info(f"æ¨¡å‹å·²ä»{path}åŠ è½½")
        return self.optimized_model
```

### 5.2 é›†æˆåˆ°GPUç®¡ç†å™¨

```python
# src/utils/gpu_acceleration.py
from src.optimization.tensorrt_optimizer import TensorRTOptimizer

class GPUAccelerationManager:
    """GPUåŠ é€Ÿç®¡ç†å™¨"""

    def __init__(self):
        self.device = "cpu"
        self.gpu_info = {}
        self.optimization_applied = False
        self.performance_config = {}
        self.tensorrt_optimizer = None

    def initialize_tensorrt(self, model, precision='fp16'):
        """åˆå§‹åŒ–TensorRT"""
        if not torch.cuda.is_available():
            logger.warning("CUDAä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨TensorRT")
            return None

        # åˆ›å»ºTensorRTä¼˜åŒ–å™¨
        self.tensorrt_optimizer = TensorRTOptimizer(model)

        # ä¼˜åŒ–æ¨¡å‹
        optimized_model = self.tensorrt_optimizer.optimize(precision=precision)

        return optimized_model

    def get_optimized_model(self, model_type='yolo'):
        """è·å–ä¼˜åŒ–çš„æ¨¡å‹"""
        if self.tensorrt_optimizer is not None:
            return self.tensorrt_optimizer.optimized_model
        return None
```

### 5.3 é…ç½®ç®¡ç†

```yaml
# config/unified_params.yaml
tensorrt:
  enabled: true
  precision: fp16  # fp32, fp16, int8
  workspace_size: 1073741824  # 1GB
  min_block_size: 7
  save_path: models/tensorrt/

  # æ¨¡å‹é…ç½®
  models:
    human_detection:
      enabled: true
      input_shape: [1, 3, 640, 640]
      precision: fp16
    hairnet_detection:
      enabled: true
      input_shape: [1, 3, 224, 224]
      precision: fp16
    pose_detection:
      enabled: false
      input_shape: [1, 3, 640, 640]
      precision: fp16
```

### 5.4 è‡ªåŠ¨ä¼˜åŒ–è„šæœ¬

```python
# scripts/optimization/auto_tensorrt_optimization.py
import torch
from src.optimization.tensorrt_optimizer import TensorRTOptimizer
from src.detection.detector import HumanDetector
from src.detection.yolo_hairnet_detector import YOLOHairnetDetector
from config.unified_params import get_unified_params

def auto_optimize_models():
    """è‡ªåŠ¨ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹"""
    params = get_unified_params()

    # 1. ä¼˜åŒ–äººä½“æ£€æµ‹æ¨¡å‹
    if params.tensorrt.models.human_detection.enabled:
        print("ä¼˜åŒ–äººä½“æ£€æµ‹æ¨¡å‹...")
        human_detector = HumanDetector()
        optimizer = TensorRTOptimizer(
            human_detector.model,
            input_shape=params.tensorrt.models.human_detection.input_shape
        )
        optimizer.optimize(precision=params.tensorrt.models.human_detection.precision)
        optimizer.save(f"{params.tensorrt.save_path}/human_detection.trt")

    # 2. ä¼˜åŒ–å‘ç½‘æ£€æµ‹æ¨¡å‹
    if params.tensorrt.models.hairnet_detection.enabled:
        print("ä¼˜åŒ–å‘ç½‘æ£€æµ‹æ¨¡å‹...")
        hairnet_detector = YOLOHairnetDetector()
        optimizer = TensorRTOptimizer(
            hairnet_detector.model,
            input_shape=params.tensorrt.models.hairnet_detection.input_shape
        )
        optimizer.optimize(precision=params.tensorrt.models.hairnet_detection.precision)
        optimizer.save(f"{params.tensorrt.save_path}/hairnet_detection.trt")

    print("æ‰€æœ‰æ¨¡å‹ä¼˜åŒ–å®Œæˆï¼")

if __name__ == "__main__":
    auto_optimize_models()
```

---

## ğŸ“Š å…­ã€æ€§èƒ½å¯¹æ¯”

### 6.1 å®Œæ•´åŸºå‡†æµ‹è¯•

```python
# scripts/benchmark/gpu_benchmark.py
import time
import torch
from src.utils.gpu_acceleration import initialize_gpu_acceleration
from src.detection.detector import HumanDetector
from src.optimization.tensorrt_optimizer import TensorRTOptimizer

def benchmark_all():
    """å…¨é¢åŸºå‡†æµ‹è¯•"""
    results = {}

    # åˆå§‹åŒ–GPU
    gpu_info = initialize_gpu_acceleration()
    print(f"GPU: {gpu_info['gpu_name']}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print()

    # å‡†å¤‡è¾“å…¥
    input_shape = (1, 3, 640, 640)
    input_data = torch.randn(*input_shape).cuda()

    # æµ‹è¯•1: PyTorch FP32
    print("æµ‹è¯• PyTorch FP32...")
    model_fp32 = HumanDetector().model.cuda()
    model_fp32.eval()

    avg_time, fps = benchmark_model(model_fp32, input_data)
    results['PyTorch FP32'] = {'time': avg_time, 'fps': fps}
    print(f"  å»¶è¿Ÿ: {avg_time:.2f}ms, FPS: {fps:.1f}")

    # æµ‹è¯•2: PyTorch FP16
    print("æµ‹è¯• PyTorch FP16...")
    model_fp16 = model_fp32.half()

    avg_time, fps = benchmark_model(model_fp16, input_data.half())
    results['PyTorch FP16'] = {'time': avg_time, 'fps': fps}
    print(f"  å»¶è¿Ÿ: {avg_time:.2f}ms, FPS: {fps:.1f}")

    # æµ‹è¯•3: TensorRT FP32
    print("æµ‹è¯• TensorRT FP32...")
    optimizer_fp32 = TensorRTOptimizer(model_fp32)
    model_trt_fp32 = optimizer_fp32.optimize(precision='fp32')

    avg_time, fps = benchmark_model(model_trt_fp32, input_data)
    results['TensorRT FP32'] = {'time': avg_time, 'fps': fps}
    print(f"  å»¶è¿Ÿ: {avg_time:.2f}ms, FPS: {fps:.1f}")

    # æµ‹è¯•4: TensorRT FP16
    print("æµ‹è¯• TensorRT FP16...")
    optimizer_fp16 = TensorRTOptimizer(model_fp32)
    model_trt_fp16 = optimizer_fp16.optimize(precision='fp16')

    avg_time, fps = benchmark_model(model_trt_fp16, input_data)
    results['TensorRT FP16'] = {'time': avg_time, 'fps': fps}
    print(f"  å»¶è¿Ÿ: {avg_time:.2f}ms, FPS: {fps:.1f}")

    # ç”ŸæˆæŠ¥å‘Š
    print("\næ€§èƒ½å¯¹æ¯”æŠ¥å‘Š:")
    print("-" * 60)
    print(f"{'æ¨¡å‹':<20} {'å»¶è¿Ÿ(ms)':<15} {'FPS':<15} {'é€Ÿåº¦æå‡':<15}")
    print("-" * 60)

    baseline_fps = results['PyTorch FP32']['fps']
    for name, metrics in results.items():
        speedup = metrics['fps'] / baseline_fps
        print(f"{name:<20} {metrics['time']:<15.2f} {metrics['fps']:<15.1f} {speedup:<15.2f}x")

    return results

def benchmark_model(model, input_data, num_iterations=100):
    """åŸºå‡†æµ‹è¯•æ¨¡å‹"""
    # é¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            _ = model(input_data)

    torch.cuda.synchronize()

    # æµ‹è¯•
    start = time.time()
    for _ in range(num_iterations):
        with torch.no_grad():
            _ = model(input_data)
    torch.cuda.synchronize()
    end = time.time()

    avg_time = (end - start) / num_iterations * 1000  # ms
    fps = 1000 / avg_time

    return avg_time, fps

if __name__ == "__main__":
    results = benchmark_all()
```

### 6.2 é¢„æœŸæ€§èƒ½æå‡

| ä¼˜åŒ–æ–¹æ¡ˆ | å»¶è¿Ÿ (ms) | FPS | é€Ÿåº¦æå‡ | ç²¾åº¦å½±å“ |
|----------|-----------|-----|----------|----------|
| PyTorch FP32 | 35 | 28.6 | 1.0x | åŸºå‡† |
| PyTorch FP16 | 22 | 45.5 | 1.6x | Â±0.1% |
| TensorRT FP32 | 12 | 83.3 | 2.9x | æ—  |
| TensorRT FP16 | 6 | 166.7 | 5.8x | Â±0.1% |
| TensorRT INT8 | 4 | 250.0 | 8.7x | -1-3% |

---

## ğŸ¯ ä¸ƒã€å®æ–½è·¯çº¿å›¾

### 7.1 é˜¶æ®µä¸€ï¼šåŸºç¡€CUDAä¼˜åŒ– (1å‘¨)

#### ä»»åŠ¡æ¸…å•
- [ ] éªŒè¯CUDAç¯å¢ƒ
- [ ] é…ç½®CUDAç¯å¢ƒå˜é‡
- [ ] å¯ç”¨cuDNNä¼˜åŒ–
- [ ] æµ‹è¯•æ€§èƒ½æå‡

#### é¢„æœŸæ•ˆæœ
- é€Ÿåº¦æå‡: 2-3å€
- ç²¾åº¦å½±å“: æ— 

### 7.2 é˜¶æ®µäºŒï¼šTensorRTé›†æˆ (2-3å‘¨)

#### ä»»åŠ¡æ¸…å•
- [ ] å®‰è£…TensorRT
- [ ] åˆ›å»ºTensorRTä¼˜åŒ–å™¨
- [ ] ä¼˜åŒ–äººä½“æ£€æµ‹æ¨¡å‹
- [ ] ä¼˜åŒ–å‘ç½‘æ£€æµ‹æ¨¡å‹
- [ ] é›†æˆåˆ°æ£€æµ‹ç®¡é“

#### é¢„æœŸæ•ˆæœ
- é€Ÿåº¦æå‡: 5-10å€
- ç²¾åº¦å½±å“: Â±0.1%

### 7.3 é˜¶æ®µä¸‰ï¼šé«˜çº§ä¼˜åŒ– (3-4å‘¨)

#### ä»»åŠ¡æ¸…å•
- [ ] CUDAæµä¼˜åŒ–
- [ ] å†…å­˜æ± ç®¡ç†
- [ ] å¤šGPUå¹¶è¡Œ
- [ ] åŠ¨æ€æ‰¹å¤„ç†

#### é¢„æœŸæ•ˆæœ
- é€Ÿåº¦æå‡: 10-20å€
- ç²¾åº¦å½±å“: æ— 

---

## ğŸ“ å…«ã€æ€»ç»“

### 8.1 æ ¸å¿ƒè¦ç‚¹

1. **CUDA**: åŸºç¡€GPUåŠ é€Ÿï¼Œ2-3å€æå‡
2. **cuDNN**: æ·±åº¦ç¥ç»ç½‘ç»œä¼˜åŒ–ï¼Œ+30-50%æå‡
3. **TensorRT**: é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œ5-10å€æå‡
4. **ç»„åˆä½¿ç”¨**: å¯å®ç°10-20å€æ€§èƒ½æå‡

### 8.2 æ¨èé…ç½®

```yaml
# ç”Ÿäº§ç¯å¢ƒæ¨è
tensorrt:
  enabled: true
  precision: fp16
  workspace_size: 1073741824

cuda:
  streams: 4
  memory_pool: true
  benchmark: true

cudnn:
  benchmark: true
  deterministic: false
  tf32: true
```

### 8.3 å®æ–½å»ºè®®

1. **ç«‹å³å¼€å§‹**: å¯ç”¨CUDAå’ŒcuDNNä¼˜åŒ– (1å¤©)
2. **ç¬¬ä¸€å‘¨**: é›†æˆTensorRT (2-3å¤©)
3. **ç¬¬äºŒå‘¨**: ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹ (3-4å¤©)
4. **ç¬¬ä¸‰å‘¨**: é«˜çº§ä¼˜åŒ–å’Œæµ‹è¯• (5-7å¤©)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-15
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ
