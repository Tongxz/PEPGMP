# MLOps è®­ç»ƒä¸è¯„ä¼°æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [ä½¿ç”¨ Roboflow æ•°æ®é›†](#ä½¿ç”¨-roboflow-æ•°æ®é›†)
2. [ä¸Šä¼ æ•°æ®é›†åˆ° MLOps](#ä¸Šä¼ æ•°æ®é›†åˆ°-mlops)
3. [åˆ›å»ºå·¥ä½œæµè¿›è¡Œè®­ç»ƒ](#åˆ›å»ºå·¥ä½œæµè¿›è¡Œè®­ç»ƒ)
4. [è¯„ä¼°è®­ç»ƒç»“æœ](#è¯„ä¼°è®­ç»ƒç»“æœ)
5. [å‘ç½‘æ£€æµ‹è®­ç»ƒç¤ºä¾‹](#å‘ç½‘æ£€æµ‹è®­ç»ƒç¤ºä¾‹)
6. [æ‰‹éƒ¨æ£€æµ‹è®­ç»ƒç¤ºä¾‹](#æ‰‹éƒ¨æ£€æµ‹è®­ç»ƒç¤ºä¾‹)

---

## ğŸŒ ä½¿ç”¨ Roboflow æ•°æ®é›†

### 1. åœ¨ Roboflow ä¸Šé€‰æ‹©æ•°æ®é›†

**è®¿é—® Roboflow**: https://roboflow.com

**æœç´¢æ•°æ®é›†**:
- æœç´¢å…³é”®è¯: `hairnet detection`, `safety helmet`, `PPE detection`, `handwashing detection`
- ç­›é€‰æ¡ä»¶:
  - **æ ¼å¼**: YOLOv8 / YOLOv5 (æ¨è)
  - **ç±»åˆ«**: åŒ…å«å‘ç½‘ã€å¤´éƒ¨ã€äººä½“ç­‰ç±»åˆ«
  - **æ•°æ®é‡**: å»ºè®® â‰¥ 500 å¼ å›¾åƒ
  - **æ ‡æ³¨è´¨é‡**: æŸ¥çœ‹æ•°æ®é›†é¢„è§ˆï¼Œç¡®ä¿æ ‡æ³¨å‡†ç¡®

**æ¨èæ•°æ®é›†**:
- **å‘ç½‘æ£€æµ‹**: æœç´¢ "hairnet" æˆ– "PPE detection"
- **æ‰‹éƒ¨æ£€æµ‹**: æœç´¢ "handwashing" æˆ– "hand detection"

### 2. ä¸‹è½½æ•°æ®é›†

**æ­¥éª¤**:
1. åœ¨ Roboflow ä¸Šé€‰æ‹©æ•°æ®é›†
2. ç‚¹å‡» "Download" æŒ‰é’®
3. é€‰æ‹©æ ¼å¼: **YOLOv8** (æ¨è) æˆ– **YOLOv5**
4. é€‰æ‹©ç‰ˆæœ¬: æœ€æ–°ç‰ˆæœ¬
5. ä¸‹è½½ ZIP æ–‡ä»¶

**æ•°æ®é›†ç»“æ„** (YOLOæ ¼å¼):
```
dataset_name/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/        # è®­ç»ƒå›¾åƒ
â”‚   â””â”€â”€ labels/        # YOLOæ ¼å¼æ ‡æ³¨æ–‡ä»¶ (.txt)
â”œâ”€â”€ valid/             # éªŒè¯é›†
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ labels/
â”œâ”€â”€ test/              # æµ‹è¯•é›†ï¼ˆå¯é€‰ï¼‰
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ labels/
â””â”€â”€ data.yaml          # æ•°æ®é›†é…ç½®æ–‡ä»¶
```

**data.yaml æ ¼å¼**:
```yaml
path: /path/to/dataset
train: train/images
val: valid/images
test: test/images  # å¯é€‰

nc: 3  # ç±»åˆ«æ•°
names:
  0: hairnet    # å‘ç½‘
  1: head       # å¤´éƒ¨
  2: person     # äººä½“
```

### 3. å‡†å¤‡æ•°æ®é›†

**è§£å‹æ•°æ®é›†**:
```bash
unzip dataset_name.zip -d datasets/
```

**éªŒè¯æ•°æ®é›†ç»“æ„**:
```bash
# æ£€æŸ¥æ–‡ä»¶ç»“æ„
ls -R datasets/dataset_name/

# æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶æ•°é‡
find datasets/dataset_name/train/labels -name "*.txt" | wc -l
find datasets/dataset_name/valid/labels -name "*.txt" | wc -l
```

**ä¿®æ”¹ data.yaml** (å¦‚æœéœ€è¦):
```yaml
# ä¿®æ”¹è·¯å¾„ä¸ºç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„
path: datasets/dataset_name  # æˆ–ä½¿ç”¨ç»å¯¹è·¯å¾„
train: train/images
val: valid/images
nc: 3
names:
  0: hairnet
  1: head
  2: person
```

---

## ğŸ“¤ ä¸Šä¼ æ•°æ®é›†åˆ° MLOps

### æ–¹æ³•1: é€šè¿‡ API ä¸Šä¼ 

**API ç«¯ç‚¹**: `POST /api/v1/mlops/datasets/upload`

**è¯·æ±‚ç¤ºä¾‹** (ä½¿ç”¨ curl):
```bash
curl -X POST "http://localhost:8000/api/v1/mlops/datasets/upload" \
  -F "files=@datasets/dataset_name/data.yaml" \
  -F "files=@datasets/dataset_name/train/images/image1.jpg" \
  -F "files=@datasets/dataset_name/train/labels/image1.txt" \
  -F "dataset_name=hairnet_roboflow_v1" \
  -F "dataset_type=detection" \
  -F "description=å‘ç½‘æ£€æµ‹æ•°æ®é›†ï¼Œæ¥è‡ªRoboflow"
```

**Python ç¤ºä¾‹** (æ¨è: ç›´æ¥å¤åˆ¶æ•°æ®é›†ç›®å½•):
```python
import shutil
from pathlib import Path
import requests

# æ–¹æ³•1: ç›´æ¥å¤åˆ¶æ•°æ®é›†åˆ°data/datasetsç›®å½•ï¼ˆæ¨èï¼Œé€‚åˆå¤§æ–‡ä»¶ï¼‰
source_dir = Path("datasets/hairnet_roboflow_v1")
target_dir = Path("data/datasets/hairnet_roboflow_v1")

# å¤åˆ¶æ•´ä¸ªæ•°æ®é›†ç›®å½•
if source_dir.exists():
    shutil.copytree(source_dir, target_dir, dirs_exist_ok=True)
    print(f"æ•°æ®é›†å·²å¤åˆ¶åˆ°: {target_dir}")

# ç„¶åé€šè¿‡APIæ³¨å†Œæ•°æ®é›†ï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦åœ¨MLOpsç³»ç»Ÿä¸­ç®¡ç†ï¼‰
url = "http://localhost:8000/api/v1/mlops/datasets"
data = {
    "id": f"dataset_{int(time.time())}",
    "name": "hairnet_roboflow_v1",
    "version": "1.0.0",
    "status": "active",
    "file_path": str(target_dir),
    "tags": ["detection", "roboflow", "hairnet"]
}

response = requests.post(url, json=data)
print(response.json())
```

**æ³¨æ„**: 
- å¯¹äºå¤§å‹æ•°æ®é›†ï¼ˆ>100MBï¼‰ï¼Œå»ºè®®ç›´æ¥å¤åˆ¶åˆ° `data/datasets/` ç›®å½•ï¼Œè€Œä¸æ˜¯é€šè¿‡APIä¸Šä¼ 
- APIä¸Šä¼ é€‚åˆå°æ–‡ä»¶æˆ–å•ä¸ªæ–‡ä»¶
- ç¡®ä¿ `data.yaml` æ–‡ä»¶ä¸­çš„è·¯å¾„é…ç½®æ­£ç¡®

### æ–¹æ³•2: ç›´æ¥å¤åˆ¶åˆ°æ•°æ®é›†ç›®å½•

**æ•°æ®é›†ç›®å½•**: `data/datasets/`

**æ­¥éª¤**:
1. å°†æ•°æ®é›†å¤åˆ¶åˆ° `data/datasets/` ç›®å½•
2. é€šè¿‡ API æ³¨å†Œæ•°æ®é›†åˆ°æ•°æ®åº“

**Python è„šæœ¬**:
```python
import shutil
from pathlib import Path
import requests

# å¤åˆ¶æ•°æ®é›†
source_dir = Path("datasets/dataset_name")
target_dir = Path("data/datasets/hairnet_roboflow_v1")
shutil.copytree(source_dir, target_dir, dirs_exist_ok=True)

# æ³¨å†Œæ•°æ®é›†åˆ°æ•°æ®åº“
url = "http://localhost:8000/api/v1/mlops/datasets"
data = {
    "id": f"dataset_{int(time.time())}",
    "name": "hairnet_roboflow_v1",
    "version": "1.0.0",
    "status": "active",
    "file_path": str(target_dir),
    "tags": ["detection", "roboflow"]
}

response = requests.post(url, json=data)
```

### æ–¹æ³•3: é€šè¿‡å‰ç«¯ç•Œé¢ä¸Šä¼ 

1. è®¿é—® MLOps å‰ç«¯ç•Œé¢
2. è¿›å…¥ "æ•°æ®é›†ç®¡ç†" é¡µé¢
3. ç‚¹å‡» "ä¸Šä¼ æ•°æ®é›†"
4. é€‰æ‹©æ–‡ä»¶æˆ–æ‹–æ‹½æ–‡ä»¶
5. å¡«å†™æ•°æ®é›†ä¿¡æ¯ï¼ˆåç§°ã€ç±»å‹ã€æè¿°ï¼‰
6. ç‚¹å‡» "ä¸Šä¼ "

---

## ğŸ”„ åˆ›å»ºå·¥ä½œæµè¿›è¡Œè®­ç»ƒ

### 1. å·¥ä½œæµç»“æ„

**å·¥ä½œæµé…ç½®** (å‘ç½‘æ£€æµ‹):
```json
{
  "name": "å‘ç½‘æ£€æµ‹æ¨¡å‹è®­ç»ƒ",
  "type": "multi_behavior_training",
  "trigger": "manual",
  "description": "ä½¿ç”¨Roboflowæ•°æ®é›†è®­ç»ƒå‘ç½‘æ£€æµ‹æ¨¡å‹",
  "steps": [
    {
      "type": "multi_behavior_training",
      "name": "è®­ç»ƒå‘ç½‘æ£€æµ‹æ¨¡å‹",
      "config": {
        "dataset_dir": "data/datasets/hairnet_roboflow_v1",
        "data_config": "data/datasets/hairnet_roboflow_v1/data.yaml",
        "training_params": {
          "model": "yolov8s.pt",
          "epochs": 150,
          "batch_size": 16,
          "image_size": 640,
          "device": "cuda:0",
          "patience": 50
        }
      }
    },
    {
      "type": "model_evaluation",
      "name": "è¯„ä¼°æ¨¡å‹æ€§èƒ½",
      "config": {
        "model_path": "{{steps[0].outputs.model_path}}",
        "test_data": "data/datasets/hairnet_roboflow_v1/valid",
        "metrics": ["mAP50", "precision", "recall", "f1_score"]
      }
    }
  ]
}
```

**æ³¨æ„**: 
- å‘ç½‘æ£€æµ‹ä½¿ç”¨ `multi_behavior_training` æ­¥éª¤ç±»å‹ï¼ˆæ”¯æŒYOLOæ ¼å¼ï¼‰
- æ‰‹éƒ¨æ£€æµ‹ä½¿ç”¨ `handwash_training` æ­¥éª¤ç±»å‹ï¼ˆæ—¶åºæ¨¡å‹ï¼‰

### 2. é€šè¿‡ API åˆ›å»ºå·¥ä½œæµ

**API ç«¯ç‚¹**: `POST /api/v1/mlops/workflows`

**è¯·æ±‚ç¤ºä¾‹** (å‘ç½‘æ£€æµ‹):
```python
import requests
import json

url = "http://localhost:8000/api/v1/mlops/workflows"

workflow = {
    "name": "å‘ç½‘æ£€æµ‹æ¨¡å‹è®­ç»ƒ",
    "type": "multi_behavior_training",
    "trigger": "manual",
    "description": "ä½¿ç”¨Roboflowæ•°æ®é›†è®­ç»ƒå‘ç½‘æ£€æµ‹æ¨¡å‹",
    "steps": [
        {
            "type": "multi_behavior_training",
            "name": "è®­ç»ƒå‘ç½‘æ£€æµ‹æ¨¡å‹",
            "config": {
                "dataset_dir": "data/datasets/hairnet_roboflow_v1",
                "data_config": "data/datasets/hairnet_roboflow_v1/data.yaml",
                "training_params": {
                    "model": "yolov8s.pt",
                    "epochs": 150,
                    "batch_size": 16,
                    "image_size": 640,
                    "device": "cuda:0",
                    "patience": 50
                }
            }
        }
    ]
}

response = requests.post(url, json=workflow)
result = response.json()
print(f"å·¥ä½œæµID: {result['workflow_id']}")
```

### 3. è¿è¡Œå·¥ä½œæµ

**API ç«¯ç‚¹**: `POST /api/v1/mlops/workflows/{workflow_id}/run`

**è¯·æ±‚ç¤ºä¾‹**:
```python
workflow_id = result["workflow_id"]
run_url = f"http://localhost:8000/api/v1/mlops/workflows/{workflow_id}/run"

response = requests.post(run_url)
run_result = response.json()
print(f"è¿è¡ŒID: {run_result['run_id']}")
print(f"çŠ¶æ€: {run_result['status']}")
```

### 4. æŸ¥çœ‹å·¥ä½œæµçŠ¶æ€

**API ç«¯ç‚¹**: `GET /api/v1/mlops/workflows/{workflow_id}`

**è¯·æ±‚ç¤ºä¾‹**:
```python
status_url = f"http://localhost:8000/api/v1/mlops/workflows/{workflow_id}"
response = requests.get(status_url)
workflow_status = response.json()
print(json.dumps(workflow_status, indent=2))
```

---

## ğŸ“Š è¯„ä¼°è®­ç»ƒç»“æœ

### 1. æŸ¥çœ‹è®­ç»ƒæŠ¥å‘Š

**è®­ç»ƒå®Œæˆåï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š**:
- **ä½ç½®**: `models/runs/multi_behavior_YYYYMMDD_HHMMSS/` (å‘ç½‘æ£€æµ‹)
- **æ–‡ä»¶**:
  - `results.csv`: è®­ç»ƒæŒ‡æ ‡CSVï¼ˆåŒ…å«æ¯è½®çš„lossã€mAPã€Precisionã€Recallç­‰ï¼‰
  - `results.png`: è®­ç»ƒæ›²çº¿å›¾ï¼ˆlossæ›²çº¿ã€mAPæ›²çº¿ç­‰ï¼‰
  - `confusion_matrix.png`: æ··æ·†çŸ©é˜µ
  - `weights/best.pt`: æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯é›†ä¸Šæ€§èƒ½æœ€å¥½çš„æ¨¡å‹ï¼‰
  - `weights/last.pt`: æœ€åä¸€è½®æ¨¡å‹

**è®­ç»ƒæŠ¥å‘Šä½ç½®** (ä»å·¥ä½œæµè¾“å‡ºè·å–):
- å·¥ä½œæµè¿è¡Œå®Œæˆåï¼Œå¯ä»¥ä» `outputs` ä¸­è·å– `report_path`
- æŠ¥å‘Šæ–‡ä»¶: `models/reports/multi_behavior_report_YYYYMMDD_HHMMSS.json`

### 2. ä»å·¥ä½œæµè¾“å‡ºè·å–è¯„ä¼°ç»“æœ

**è®­ç»ƒå®Œæˆåï¼Œè¯„ä¼°æŒ‡æ ‡ä¼šè‡ªåŠ¨åŒ…å«åœ¨å·¥ä½œæµè¾“å‡ºä¸­**:

```python
# è·å–å·¥ä½œæµè¿è¡Œç»“æœ
workflow_id = "workflow_xxx"
run_id = "run_xxx"

url = f"http://localhost:8000/api/v1/mlops/workflows/{workflow_id}/runs/{run_id}"
response = requests.get(url)
run_info = response.json()

# ä»è¾“å‡ºä¸­æå–è¯„ä¼°æŒ‡æ ‡
outputs = run_info.get("outputs", [])
for output in outputs:
    if output.get("step_name") == "è®­ç»ƒå‘ç½‘æ£€æµ‹æ¨¡å‹":
        step_output = output.get("output", {})
        metrics = step_output.get("metrics", {})
        
        print(f"mAP@0.5: {metrics.get('mAP50', 'N/A')}")
        print(f"mAP@0.5:0.95: {metrics.get('mAP50_95', 'N/A')}")
        print(f"Precision: {metrics.get('precision', 'N/A')}")
        print(f"Recall: {metrics.get('recall', 'N/A')}")
        
        # æ¨¡å‹è·¯å¾„
        model_path = step_output.get("model_path")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        
        # æŠ¥å‘Šè·¯å¾„
        report_path = step_output.get("report_path")
        print(f"æŠ¥å‘Šè·¯å¾„: {report_path}")
```

### 3. é€šè¿‡æ¨¡å‹æ³¨å†Œè¡¨è·å–è¯„ä¼°ç»“æœ

**å¦‚æœæ¨¡å‹å·²æ³¨å†Œåˆ°æ¨¡å‹æ³¨å†Œè¡¨**:

```python
# è·å–æ¨¡å‹åˆ—è¡¨
url = "http://localhost:8000/api/v1/mlops/models"
response = requests.get(url)
models = response.json()

# æŸ¥æ‰¾æœ€æ–°è®­ç»ƒçš„æ¨¡å‹
latest_model = max(models, key=lambda m: m.get("created_at", ""))
model_id = latest_model["id"]

# è·å–æ¨¡å‹è¯¦æƒ…
url = f"http://localhost:8000/api/v1/mlops/models/{model_id}"
response = requests.get(url)
model_info = response.json()

# æŸ¥çœ‹è¯„ä¼°æŒ‡æ ‡
metrics = model_info.get("metrics", {})
print(f"mAP@0.5: {metrics.get('mAP50', 'N/A')}")
print(f"Precision: {metrics.get('precision', 'N/A')}")
print(f"Recall: {metrics.get('recall', 'N/A')}")
print(f"F1-Score: {metrics.get('f1_score', 'N/A')}")
```

### 3. è¯„ä¼°æ ‡å‡†

**ä¼˜ç§€æ¨¡å‹**:
- mAP@0.5 â‰¥ 0.90
- Precision â‰¥ 0.85
- Recall â‰¥ 0.85
- F1-Score â‰¥ 0.85

**è‰¯å¥½æ¨¡å‹**:
- mAP@0.5 â‰¥ 0.80
- Precision â‰¥ 0.75
- Recall â‰¥ 0.75
- F1-Score â‰¥ 0.75

**éœ€è¦æ”¹è¿›**:
- mAP@0.5 < 0.75
- Precision < 0.70
- Recall < 0.70

---

## ğŸ¯ å‘ç½‘æ£€æµ‹è®­ç»ƒç¤ºä¾‹

### å®Œæ•´å·¥ä½œæµé…ç½®

**é‡è¦**: å‘ç½‘æ£€æµ‹ä½¿ç”¨ `multi_behavior_training` æ­¥éª¤ç±»å‹ï¼Œå› ä¸ºå®ƒæ”¯æŒYOLOæ ¼å¼çš„ `data.yaml` é…ç½®æ–‡ä»¶ã€‚

```json
{
  "name": "å‘ç½‘æ£€æµ‹æ¨¡å‹è®­ç»ƒï¼ˆRoboflowï¼‰",
  "type": "multi_behavior_training",
  "trigger": "manual",
  "description": "ä½¿ç”¨Roboflowå‘ç½‘æ£€æµ‹æ•°æ®é›†è®­ç»ƒYOLOv8æ¨¡å‹",
  "steps": [
    {
      "type": "multi_behavior_training",
      "name": "è®­ç»ƒå‘ç½‘æ£€æµ‹æ¨¡å‹",
      "config": {
        "dataset_dir": "data/datasets/hairnet_roboflow_v1",
        "data_config": "data/datasets/hairnet_roboflow_v1/data.yaml",
        "training_params": {
          "model": "yolov8s.pt",
          "epochs": 150,
          "batch_size": 16,
          "image_size": 640,
          "device": "cuda:0",
          "patience": 50,
          "lr0": 0.01,
          "lrf": 0.01,
          "momentum": 0.937,
          "weight_decay": 0.0005,
          "warmup_epochs": 3.0
        }
      }
    },
    {
      "type": "model_evaluation",
      "name": "è¯„ä¼°æ¨¡å‹æ€§èƒ½",
      "config": {
        "model_path": "{{steps[0].outputs.model_path}}",
        "test_data": "data/datasets/hairnet_roboflow_v1/valid",
        "metrics": ["mAP50", "mAP50_95", "precision", "recall", "f1_score"]
      }
    }
  ]
}
```

### å…³é”®é…ç½®è¯´æ˜

**æ­¥éª¤ç±»å‹**: `multi_behavior_training`
- æ”¯æŒYOLOæ ¼å¼æ•°æ®é›†ï¼ˆtrain/valid/test + data.yamlï¼‰
- è‡ªåŠ¨æå–è®­ç»ƒæŒ‡æ ‡ï¼ˆmAPã€Precisionã€Recallç­‰ï¼‰
- ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨

**æ•°æ®é›†è·¯å¾„**: 
- `dataset_dir`: æ•°æ®é›†æ ¹ç›®å½•ï¼ˆåŒ…å«train/validç›®å½•ï¼‰
- `data_config`: data.yamlæ–‡ä»¶è·¯å¾„ï¼ˆå¿…é¡»ï¼‰

**è®­ç»ƒå‚æ•°**:
- `model`: é¢„è®­ç»ƒæ¨¡å‹ï¼ˆyolov8n.pt, yolov8s.pt, yolov8m.ptç­‰ï¼‰
- `epochs`: è®­ç»ƒè½®æ•°ï¼ˆå»ºè®®100-200ï¼‰
- `batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰
- `image_size`: å›¾åƒå°ºå¯¸ï¼ˆå»ºè®®640ï¼Œä¸æ£€æµ‹æ—¶ä¿æŒä¸€è‡´ï¼‰
- `device`: è®­ç»ƒè®¾å¤‡ï¼ˆcuda:0, cpuç­‰ï¼‰
- `patience`: æ—©åœè€å¿ƒå€¼ï¼ˆéªŒè¯æŸå¤±ä¸ä¸‹é™çš„è½®æ•°ï¼‰

### Python å®Œæ•´ç¤ºä¾‹

```python
import requests
import json
import time
from pathlib import Path

# 1. ä¸Šä¼ æ•°æ®é›†ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ä¸Šä¼ ï¼‰
def upload_dataset(dataset_dir: Path):
    url = "http://localhost:8000/api/v1/mlops/datasets/upload"
    
    files = []
    # æ·»åŠ  data.yaml
    data_yaml = dataset_dir / "data.yaml"
    if data_yaml.exists():
        files.append(("files", ("data.yaml", open(data_yaml, "rb"))))
    
    data = {
        "dataset_name": "hairnet_roboflow_v1",
        "dataset_type": "detection",
        "description": "å‘ç½‘æ£€æµ‹æ•°æ®é›†ï¼Œæ¥è‡ªRoboflow"
    }
    
    response = requests.post(url, files=files, data=data)
    return response.json()

# 2. åˆ›å»ºå·¥ä½œæµ
def create_training_workflow(dataset_path: str):
    url = "http://localhost:8000/api/v1/mlops/workflows"
    
    workflow = {
        "name": "å‘ç½‘æ£€æµ‹æ¨¡å‹è®­ç»ƒï¼ˆRoboflowï¼‰",
        "type": "multi_behavior_training",
        "trigger": "manual",
        "description": "ä½¿ç”¨Roboflowå‘ç½‘æ£€æµ‹æ•°æ®é›†è®­ç»ƒYOLOv8æ¨¡å‹",
        "steps": [
            {
                "type": "multi_behavior_training",
                "name": "è®­ç»ƒå‘ç½‘æ£€æµ‹æ¨¡å‹",
                "config": {
                    "dataset_dir": dataset_path,
                    "data_config": f"{dataset_path}/data.yaml",
                    "training_params": {
                        "model": "yolov8s.pt",
                        "epochs": 150,
                        "batch_size": 16,
                        "image_size": 640,
                        "device": "cuda:0",
                        "patience": 50
                    }
                }
            }
        ]
    }
    
    response = requests.post(url, json=workflow)
    return response.json()

# 3. è¿è¡Œå·¥ä½œæµ
def run_workflow(workflow_id: str):
    url = f"http://localhost:8000/api/v1/mlops/workflows/{workflow_id}/run"
    response = requests.post(url)
    return response.json()

# 4. ç›‘æ§å·¥ä½œæµçŠ¶æ€
def monitor_workflow(workflow_id: str):
    url = f"http://localhost:8000/api/v1/mlops/workflows/{workflow_id}"
    
    while True:
        response = requests.get(url)
        workflow = response.json()
        status = workflow.get("status")
        
        print(f"å·¥ä½œæµçŠ¶æ€: {status}")
        
        if status in ["success", "failed"]:
            break
        
        time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
    
    return workflow

# 5. è·å–è®­ç»ƒç»“æœ
def get_training_results(workflow_id: str):
    url = f"http://localhost:8000/api/v1/mlops/workflows/{workflow_id}"
    response = requests.get(url)
    workflow = response.json()
    
    # ä»å·¥ä½œæµè¾“å‡ºä¸­è·å–æ¨¡å‹ä¿¡æ¯
    last_run = workflow.get("last_run")
    if last_run:
        run_url = f"http://localhost:8000/api/v1/mlops/workflows/{workflow_id}/runs/{last_run}"
        response = requests.get(run_url)
        run_info = response.json()
        
        outputs = run_info.get("outputs", [])
        for output in outputs:
            if output.get("step_name") == "è®­ç»ƒå‘ç½‘æ£€æµ‹æ¨¡å‹":
                model_path = output.get("output", {}).get("model_path")
                metrics = output.get("output", {}).get("metrics", {})
                print(f"æ¨¡å‹è·¯å¾„: {model_path}")
                print(f"è¯„ä¼°æŒ‡æ ‡: {json.dumps(metrics, indent=2)}")
                return model_path, metrics
    
    return None, None

# ä¸»æµç¨‹
if __name__ == "__main__":
    dataset_path = "data/datasets/hairnet_roboflow_v1"
    
    # 1. åˆ›å»ºå·¥ä½œæµ
    workflow_result = create_training_workflow(dataset_path)
    workflow_id = workflow_result["workflow_id"]
    print(f"å·¥ä½œæµåˆ›å»ºæˆåŠŸ: {workflow_id}")
    
    # 2. è¿è¡Œå·¥ä½œæµ
    run_result = run_workflow(workflow_id)
    print(f"å·¥ä½œæµè¿è¡Œä¸­: {run_result['run_id']}")
    
    # 3. ç›‘æ§å·¥ä½œæµ
    final_workflow = monitor_workflow(workflow_id)
    
    # 4. è·å–è®­ç»ƒç»“æœ
    model_path, metrics = get_training_results(workflow_id)
    if model_path:
        print(f"\nè®­ç»ƒå®Œæˆï¼")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"è¯„ä¼°æŒ‡æ ‡: {json.dumps(metrics, indent=2)}")
```

---

## ğŸ¤² æ‰‹éƒ¨æ£€æµ‹è®­ç»ƒç¤ºä¾‹

### å·¥ä½œæµé…ç½®

```json
{
  "name": "æ‰‹éƒ¨æ£€æµ‹æ¨¡å‹è®­ç»ƒ",
  "type": "handwash_training",
  "trigger": "manual",
  "description": "è®­ç»ƒæ‰‹éƒ¨æ£€æµ‹æ—¶åºæ¨¡å‹",
  "steps": [
    {
      "type": "handwash_training",
      "name": "è®­ç»ƒæ‰‹éƒ¨æ£€æµ‹æ¨¡å‹",
      "config": {
        "dataset_dir": "data/datasets/handwash_roboflow_v1",
        "annotations_file": "data/datasets/handwash_roboflow_v1/annotations.json",
        "training_params": {
          "epochs": 100,
          "batch_size": 32,
          "learning_rate": 0.001,
          "validation_split": 0.2,
          "device": "cuda:0"
        }
      }
    },
    {
      "type": "model_evaluation",
      "name": "è¯„ä¼°æ¨¡å‹æ€§èƒ½",
      "config": {
        "model_path": "{{steps[0].outputs.model_path}}",
        "test_data": "data/datasets/handwash_roboflow_v1/test",
        "metrics": ["accuracy", "precision", "recall", "f1_score"]
      }
    }
  ]
}
```

---

## ğŸ” é€‰æ‹© Roboflow æ•°æ®é›†çš„å»ºè®®

### å‘ç½‘æ£€æµ‹æ•°æ®é›†é€‰æ‹©æ ‡å‡†

1. **æ•°æ®é‡**:
   - è®­ç»ƒé›†: â‰¥ 500 å¼ å›¾åƒ
   - éªŒè¯é›†: â‰¥ 100 å¼ å›¾åƒ
   - æµ‹è¯•é›†: â‰¥ 50 å¼ å›¾åƒï¼ˆå¯é€‰ï¼‰

2. **ç±»åˆ«è¦†ç›–**:
   - å¿…é¡»åŒ…å«: `hairnet` (å‘ç½‘)
   - å»ºè®®åŒ…å«: `head` (å¤´éƒ¨), `person` (äººä½“)
   - ç±»åˆ«æ•°é‡: 2-4 ä¸ªç±»åˆ«ï¼ˆé¿å…è¿‡å¤šç±»åˆ«ï¼‰

3. **åœºæ™¯å¤šæ ·æ€§**:
   - ä¸åŒå…‰ç…§æ¡ä»¶
   - ä¸åŒè§’åº¦ï¼ˆæ­£é¢ã€ä¾§é¢ã€èƒŒé¢ï¼‰
   - ä¸åŒå‘ç½‘ç±»å‹ï¼ˆé¢œè‰²ã€æè´¨ï¼‰
   - ä¸åŒèƒŒæ™¯ç¯å¢ƒ

4. **æ ‡æ³¨è´¨é‡**:
   - è¾¹ç•Œæ¡†å‡†ç¡®
   - ç±»åˆ«æ ‡ç­¾æ­£ç¡®
   - æ— é—æ¼æ ‡æ³¨

### æ‰‹éƒ¨æ£€æµ‹æ•°æ®é›†é€‰æ‹©æ ‡å‡†

1. **æ•°æ®æ ¼å¼**:
   - å¦‚æœæ˜¯å›¾åƒåºåˆ—: éœ€è¦æ—¶åºæ ‡æ³¨
   - å¦‚æœæ˜¯å•å¼ å›¾åƒ: éœ€è¦æ‰‹éƒ¨å…³é”®ç‚¹æ ‡æ³¨

2. **åŠ¨ä½œè¦†ç›–**:
   - æ ‡å‡†æ´—æ‰‹åŠ¨ä½œ
   - å¿«é€Ÿæ´—æ‰‹åŠ¨ä½œ
   - éæ´—æ‰‹åŠ¨ä½œï¼ˆè´Ÿæ ·æœ¬ï¼‰

3. **ç¯å¢ƒå¤šæ ·æ€§**:
   - ä¸åŒæ°´æ± ç±»å‹
   - ä¸åŒå…‰ç…§æ¡ä»¶
   - ä¸åŒè§†è§’

---

## ğŸ“ æ€»ç»“

### ä½¿ç”¨ Roboflow æ•°æ®é›†çš„å®Œæ•´æµç¨‹

1. **é€‰æ‹©æ•°æ®é›†**: åœ¨ Roboflow ä¸Šæœç´¢å¹¶é€‰æ‹©åˆé€‚çš„æ•°æ®é›†
2. **ä¸‹è½½æ•°æ®é›†**: ä¸‹è½½ YOLOv8 æ ¼å¼çš„æ•°æ®é›†
3. **å‡†å¤‡æ•°æ®é›†**: è§£å‹å¹¶éªŒè¯æ•°æ®é›†ç»“æ„
4. **ä¸Šä¼ æ•°æ®é›†**: é€šè¿‡ API æˆ–å‰ç«¯ç•Œé¢ä¸Šä¼ åˆ° MLOps ç³»ç»Ÿ
5. **åˆ›å»ºå·¥ä½œæµ**: é…ç½®è®­ç»ƒå·¥ä½œæµï¼ˆåŒ…å«è®­ç»ƒå’Œè¯„ä¼°æ­¥éª¤ï¼‰
6. **è¿è¡Œå·¥ä½œæµ**: æ‰§è¡Œè®­ç»ƒå·¥ä½œæµ
7. **ç›‘æ§è®­ç»ƒ**: æŸ¥çœ‹è®­ç»ƒè¿›åº¦å’Œæ—¥å¿—
8. **è¯„ä¼°ç»“æœ**: æŸ¥çœ‹è®­ç»ƒæŠ¥å‘Šå’Œè¯„ä¼°æŒ‡æ ‡
9. **éƒ¨ç½²æ¨¡å‹**: å¦‚æœæ€§èƒ½æ»¡è¶³è¦æ±‚ï¼Œéƒ¨ç½²æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒ

### å…³é”® API ç«¯ç‚¹

**æ•°æ®é›†ç®¡ç†**:
- `POST /api/v1/mlops/datasets/upload` - ä¸Šä¼ æ•°æ®é›†ï¼ˆå°æ–‡ä»¶ï¼‰
- `GET /api/v1/mlops/datasets` - è·å–æ•°æ®é›†åˆ—è¡¨
- `GET /api/v1/mlops/datasets/{dataset_id}` - è·å–æ•°æ®é›†è¯¦æƒ…
- `DELETE /api/v1/mlops/datasets/{dataset_id}` - åˆ é™¤æ•°æ®é›†

**å·¥ä½œæµç®¡ç†**:
- `POST /api/v1/mlops/workflows` - åˆ›å»ºå·¥ä½œæµ
- `GET /api/v1/mlops/workflows` - è·å–å·¥ä½œæµåˆ—è¡¨
- `GET /api/v1/mlops/workflows/{workflow_id}` - æŸ¥çœ‹å·¥ä½œæµè¯¦æƒ…
- `POST /api/v1/mlops/workflows/{workflow_id}/run` - è¿è¡Œå·¥ä½œæµ
- `GET /api/v1/mlops/workflows/{workflow_id}/runs/{run_id}` - è·å–å·¥ä½œæµè¿è¡Œç»“æœ
- `PUT /api/v1/mlops/workflows/{workflow_id}` - æ›´æ–°å·¥ä½œæµ
- `DELETE /api/v1/mlops/workflows/{workflow_id}` - åˆ é™¤å·¥ä½œæµ

**æ¨¡å‹ç®¡ç†**:
- `GET /api/v1/mlops/models` - è·å–æ¨¡å‹åˆ—è¡¨
- `GET /api/v1/mlops/models/{model_id}` - è·å–æ¨¡å‹è¯¦æƒ…
- `POST /api/v1/mlops/models/{model_id}/deploy` - éƒ¨ç½²æ¨¡å‹

### å·¥ä½œæµæ­¥éª¤ç±»å‹

**è®­ç»ƒç›¸å…³**:
- `multi_behavior_training` - å¤šè¡Œä¸ºæ£€æµ‹è®­ç»ƒï¼ˆYOLOæ ¼å¼ï¼Œç”¨äºå‘ç½‘æ£€æµ‹ï¼‰
- `handwash_training` - æ‰‹éƒ¨æ£€æµ‹è®­ç»ƒï¼ˆæ—¶åºæ¨¡å‹ï¼‰
- `model_training` - é€šç”¨æ¨¡å‹è®­ç»ƒï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰

**æ•°æ®é›†ç›¸å…³**:
- `dataset_generation` - ä»æ£€æµ‹è®°å½•ç”Ÿæˆæ•°æ®é›†
- `multi_behavior_dataset` - ç”Ÿæˆå¤šè¡Œä¸ºæ£€æµ‹æ•°æ®é›†
- `handwash_dataset` - ç”Ÿæˆæ‰‹éƒ¨æ£€æµ‹æ•°æ®é›†

**è¯„ä¼°å’Œéƒ¨ç½²**:
- `model_evaluation` - æ¨¡å‹è¯„ä¼°
- `model_deployment` - æ¨¡å‹éƒ¨ç½²

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©ä½¿ç”¨å“ªä¸ªè®­ç»ƒæ­¥éª¤ç±»å‹ï¼Ÿ

**A**: 
- **å‘ç½‘æ£€æµ‹**: ä½¿ç”¨ `multi_behavior_training`ï¼ˆYOLOæ ¼å¼æ•°æ®é›†ï¼Œæ”¯æŒdata.yamlï¼‰
- **æ‰‹éƒ¨æ£€æµ‹**: ä½¿ç”¨ `handwash_training`ï¼ˆæ—¶åºæ¨¡å‹ï¼Œéœ€è¦annotations.jsonï¼‰
- **åˆ†ç±»ä»»åŠ¡**: ä½¿ç”¨ `model_training`ï¼ˆäºŒåˆ†ç±»ï¼šè¿è§„/æ­£å¸¸ï¼‰

### Q2: Roboflowæ•°æ®é›†ä¸‹è½½åå¦‚ä½•ä½¿ç”¨ï¼Ÿ

**A**: 
1. è§£å‹æ•°æ®é›†åˆ°æœ¬åœ°
2. å¤åˆ¶åˆ° `data/datasets/` ç›®å½•
3. æ£€æŸ¥å¹¶ä¿®æ”¹ `data.yaml` ä¸­çš„è·¯å¾„é…ç½®ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ï¼‰
4. åœ¨å·¥ä½œæµä¸­æŒ‡å®š `dataset_dir` å’Œ `data_config` è·¯å¾„

### Q3: data.yaml è·¯å¾„é…ç½®é”™è¯¯æ€ä¹ˆåŠï¼Ÿ

**A**: 
ä¿®æ”¹ `data.yaml` ä¸­çš„è·¯å¾„ä¸ºç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºæ•°æ®é›†ç›®å½•çš„è·¯å¾„:
```yaml
# æ–¹å¼1: ç»å¯¹è·¯å¾„ï¼ˆæ¨èï¼‰
path: /Users/zhou/Code/Pyt/data/datasets/hairnet_roboflow_v1

# æ–¹å¼2: ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºdata.yamlæ‰€åœ¨ç›®å½•ï¼‰
path: .
train: train/images
val: valid/images
```

### Q4: è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•æŸ¥çœ‹è¿›åº¦ï¼Ÿ

**A**: 
- é€šè¿‡å·¥ä½œæµè¿è¡ŒçŠ¶æ€API: `GET /api/v1/mlops/workflows/{workflow_id}`
- æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: `models/runs/{run_name}/` ç›®å½•
- æŸ¥çœ‹è®­ç»ƒæ›²çº¿: `models/runs/{run_name}/results.png`
- æŸ¥çœ‹å®æ—¶æ—¥å¿—: è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¾“å‡ºåˆ°æ§åˆ¶å°

### Q5: å¦‚ä½•åˆ¤æ–­æ¨¡å‹è®­ç»ƒæ˜¯å¦æˆåŠŸï¼Ÿ

**A**: 
æ£€æŸ¥è¯„ä¼°æŒ‡æ ‡:
- **ä¼˜ç§€**: mAP@0.5 â‰¥ 0.90, Precision â‰¥ 0.85, Recall â‰¥ 0.85
- **è‰¯å¥½**: mAP@0.5 â‰¥ 0.80, Precision â‰¥ 0.75, Recall â‰¥ 0.75
- **éœ€è¦æ”¹è¿›**: mAP@0.5 < 0.75

### Q6: è®­ç»ƒå®Œæˆåå¦‚ä½•éƒ¨ç½²æ¨¡å‹ï¼Ÿ

**A**: 
1. ä»å·¥ä½œæµè¾“å‡ºè·å–æ¨¡å‹è·¯å¾„ï¼ˆ`outputs[0].output.model_path`ï¼‰
2. å¤åˆ¶æ¨¡å‹åˆ° `models/hairnet_detection/` ç›®å½•
3. æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„
4. é‡å¯æ£€æµ‹æœåŠ¡

### Q7: è®­ç»ƒæ—¶GPUå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A**: 
- å‡å° `batch_size`ï¼ˆå¦‚ä»16æ”¹ä¸º8ï¼‰
- å‡å° `image_size`ï¼ˆå¦‚ä»640æ”¹ä¸º512ï¼‰
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚yolov8n.ptè€Œä¸æ˜¯yolov8s.ptï¼‰

### Q8: å¦‚ä½•ä»è®­ç»ƒç»“æœä¸­è·å–æœ€ä½³æ¨¡å‹ï¼Ÿ

**A**: 
- æœ€ä½³æ¨¡å‹è·¯å¾„: `models/runs/{run_name}/weights/best.pt`
- ä»å·¥ä½œæµè¾“å‡ºä¸­è·å–: `outputs[0].output.model_path`
- è®­ç»ƒæŠ¥å‘Šè·¯å¾„: `outputs[0].output.report_path`

### Q9: å·¥ä½œæµè¿è¡Œå¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: 
1. æŸ¥çœ‹å·¥ä½œæµè¿è¡Œæ—¥å¿—: `GET /api/v1/mlops/workflows/{workflow_id}/runs/{run_id}`
2. æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®
3. æ£€æŸ¥ `data.yaml` æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®
4. æ£€æŸ¥è®­ç»ƒå‚æ•°æ˜¯å¦åˆç†ï¼ˆbatch_sizeã€epochsç­‰ï¼‰
5. æŸ¥çœ‹åç«¯æ—¥å¿—æ–‡ä»¶

### Q10: å¦‚ä½•è¯„ä¼°è®­ç»ƒç¨‹åº¦ï¼Ÿ

**A**: 
**å‘ç½‘æ£€æµ‹è¯„ä¼°æŒ‡æ ‡**:
- **mAP@0.5**: å¹³å‡ç²¾åº¦ï¼ˆIoU=0.5ï¼‰ï¼Œç›®æ ‡ â‰¥ 0.90
- **Precision**: ç²¾ç¡®ç‡ï¼Œç›®æ ‡ â‰¥ 0.85
- **Recall**: å¬å›ç‡ï¼Œç›®æ ‡ â‰¥ 0.85
- **F1-Score**: F1åˆ†æ•°ï¼Œç›®æ ‡ â‰¥ 0.85

**æ‰‹éƒ¨æ£€æµ‹è¯„ä¼°æŒ‡æ ‡**:
- **Accuracy**: å‡†ç¡®ç‡ï¼Œç›®æ ‡ â‰¥ 0.90
- **Precision**: ç²¾ç¡®ç‡ï¼Œç›®æ ‡ â‰¥ 0.85
- **Recall**: å¬å›ç‡ï¼Œç›®æ ‡ â‰¥ 0.85
- **F1-Score**: F1åˆ†æ•°ï¼Œç›®æ ‡ â‰¥ 0.85

**è®­ç»ƒæ›²çº¿åˆ†æ**:
- Lossæ›²çº¿åº”è¯¥æŒç»­ä¸‹é™
- mAPæ›²çº¿åº”è¯¥æŒç»­ä¸Šå‡
- éªŒè¯é›†å’Œè®­ç»ƒé›†çš„æŒ‡æ ‡åº”è¯¥æ¥è¿‘ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [è®­ç»ƒä¸è¯„ä¼°æŒ‡å—](./TRAINING_AND_EVALUATION_GUIDE.md)
- [MLOps API æ–‡æ¡£](../src/api/routers/mlops.py)
- [å·¥ä½œæµå¼•æ“æ–‡æ¡£](../src/workflow/workflow_engine.py)
- [Roboflow å®˜ç½‘](https://roboflow.com)
- [YOLOv8 æ–‡æ¡£](https://docs.ultralytics.com/)

