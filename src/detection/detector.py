import logging
import os
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Optional

import cv2
import numpy as np
import torch
from ultralytics import YOLO

# å¯¼å…¥ç»Ÿä¸€å‚æ•°é…ç½®
from src.config.unified_params import get_unified_params

logger = logging.getLogger(__name__)


class BaseDetector(ABC):
    """æ£€æµ‹å™¨æŠ½è±¡åŸºç±»"""

    def __init__(self, model_path: str, device: str = "auto"):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.model_path = model_path
        self.device = self._get_device(device)
        self.model = self._load_model(model_path)

    def _get_device(self, device: str) -> str:
        """è·å–è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            try:
                # ä¼˜å…ˆ MPS (Apple Silicon) â†’ CUDA â†’ CPU
                mps_built = bool(getattr(torch.backends, "mps", None))
                mps_available = mps_built and bool(torch.backends.mps.is_available())
                if mps_available:
                    return "mps"
                if torch.cuda.is_available():
                    return "cuda"
            except Exception:
                pass
            return "cpu"
        return device

    @abstractmethod
    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""

    @abstractmethod
    def detect(self, image: np.ndarray) -> List[Dict]:
        """æ‰§è¡Œæ£€æµ‹"""

    def visualize(self, image: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        result_image = image.copy()
        for detection in detections:
            bbox = detection.get("bbox")
            if bbox:
                x1, y1, x2, y2 = [int(coord) for coord in bbox]
                cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 255, 0), 2)

            label = self._get_label(detection)
            if label:
                cv2.putText(
                    result_image,
                    label,
                    (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.9,
                    (0, 255, 0),
                    2,
                )
        return result_image

    def _get_label(self, detection: Dict) -> str:
        """è·å–ç”¨äºå¯è§†åŒ–çš„æ ‡ç­¾"""
        label = detection.get("class_name", "")
        confidence = detection.get("confidence")
        if confidence:
            label += f" {confidence:.2f}"
        return label.strip()


class HumanDetector(BaseDetector):
    """äººä½“æ£€æµ‹å™¨

    åŸºäºYOLOv8çš„äººä½“æ£€æµ‹æ¨¡å—ï¼Œæ”¯æŒå®æ—¶æ£€æµ‹å’Œæ‰¹é‡å¤„ç†
    """

    def __init__(
        self,
        model_path: Optional[str] = None,
        device: str = "auto",
        auto_convert_tensorrt: Optional[bool] = None,
    ):
        """
        åˆå§‹åŒ–äººä½“æ£€æµ‹å™¨

        Args:
            model_path: YOLOæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç»Ÿä¸€é…ç½®
            device: è®¡ç®—è®¾å¤‡ ('cpu', 'cuda', 'auto')
            auto_convert_tensorrt: æ˜¯å¦è‡ªåŠ¨è½¬æ¢ä¸ºTensorRTï¼ˆå¦‚æœå¯ç”¨ï¼‰
                                 å¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡AUTO_CONVERT_TENSORRTè¯»å–
                                 ç¯å¢ƒå˜é‡æœªè®¾ç½®æ—¶é»˜è®¤ä¸ºTrueï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        """
        # è·å–ç»Ÿä¸€å‚æ•°é…ç½®
        self.params = get_unified_params().human_detection

        # ä½¿ç”¨ç»Ÿä¸€é…ç½®æˆ–ä¼ å…¥å‚æ•°
        model_path = model_path if model_path is not None else self.params.model_path
        device = device if device != "auto" else self.params.device

        # ç¡®å®šæ˜¯å¦å¯ç”¨TensorRTè‡ªåŠ¨è½¬æ¢
        # ä¼˜å…ˆçº§ï¼šæ˜¾å¼å‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼Trueï¼ˆå‘åå…¼å®¹ï¼‰
        if auto_convert_tensorrt is None:
            env_value = os.getenv("AUTO_CONVERT_TENSORRT", "").strip().lower()
            if env_value:
                # ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼Œè§£æå€¼
                auto_convert_tensorrt = env_value in ("true", "1", "yes")
            else:
                # ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼Trueï¼ˆå‘åå…¼å®¹ï¼‰
                auto_convert_tensorrt = True
            # ä½¿ç”¨ INFO çº§åˆ«ï¼Œç¡®ä¿åœ¨ç”Ÿäº§ç¯å¢ƒå¯è§
            logger.info(
                f"TensorRTè‡ªåŠ¨è½¬æ¢é…ç½®: ç¯å¢ƒå˜é‡={env_value if env_value else '(æœªè®¾ç½®)'}, "
                f"å¯ç”¨çŠ¶æ€={auto_convert_tensorrt}"
            )

        # è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢TensorRTå¼•æ“
        if auto_convert_tensorrt:
            model_path = self._auto_convert_to_tensorrt(model_path, device)
        else:
            logger.info("TensorRTè‡ªåŠ¨è½¬æ¢å·²ç¦ç”¨ï¼Œä½¿ç”¨PyTorchæ¨¡å‹")

        super().__init__(model_path, device)

        # ä½¿ç”¨ç»Ÿä¸€å‚æ•°é…ç½®
        self.confidence_threshold = self.params.confidence_threshold
        self.iou_threshold = self.params.iou_threshold
        self.min_box_area = self.params.min_box_area
        self.max_box_ratio = self.params.max_box_ratio
        self.min_width = self.params.min_width
        self.min_height = self.params.min_height
        self.nms_threshold = self.params.nms_threshold
        self.max_detections = self.params.max_detections

        logger.info(
            f"HumanDetector initialized on {self.device} with unified params: "
            f"conf={self.confidence_threshold}, iou={self.iou_threshold}, "
            f"min_area={self.min_box_area}"
        )

    def _auto_convert_to_tensorrt(self, model_path: str, device: str) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢ä¸ºTensorRTå¼•æ“

        Args:
            model_path: åŸå§‹æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡

        Returns:
            ä¼˜åŒ–åçš„æ¨¡å‹è·¯å¾„ï¼ˆTensorRTå¼•æ“æˆ–åŸå§‹æ¨¡å‹ï¼‰
        """
        try:
            # åªåœ¨CUDAè®¾å¤‡ä¸Šä½¿ç”¨TensorRT
            if device != "cuda":
                logger.info(f"è®¾å¤‡ä¸º {device}ï¼Œè·³è¿‡TensorRTè½¬æ¢")
                return model_path

            # æ£€æŸ¥TensorRTæ˜¯å¦å¯ç”¨
            try:
                import tensorrt as trt

                logger.info(f"TensorRTå¯ç”¨ï¼Œç‰ˆæœ¬: {trt.__version__}")
            except ImportError:
                logger.info("TensorRTæœªå®‰è£…ï¼Œä½¿ç”¨PyTorchæ¨¡å‹")
                return model_path

            # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
            if not torch.cuda.is_available():
                logger.info("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨PyTorchæ¨¡å‹")
                return model_path

            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            pt_file = Path(model_path)
            if not pt_file.exists():
                logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return model_path

            # ç”ŸæˆTensorRTå¼•æ“è·¯å¾„
            engine_file = pt_file.with_suffix(".engine")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢
            needs_conversion = False

            if not engine_file.exists():
                logger.info(f"ğŸ“‹ TensorRTå¼•æ“ä¸å­˜åœ¨ï¼Œå¼€å§‹è½¬æ¢: {pt_file.name}")
                needs_conversion = True
            elif pt_file.stat().st_mtime > engine_file.stat().st_mtime:
                logger.info(f"ğŸ“‹ PyTorchæ¨¡å‹å·²æ›´æ–°ï¼Œé‡æ–°è½¬æ¢: {pt_file.name}")
                needs_conversion = True
            else:
                logger.info(f"âœ… TensorRTå¼•æ“å·²å­˜åœ¨: {engine_file.name}")
                return str(engine_file)

            # è½¬æ¢æ¨¡å‹
            if needs_conversion:
                logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ä¸ºTensorRT: {pt_file.name}")

                from ultralytics import YOLO

                # åŠ è½½æ¨¡å‹
                model = YOLO(str(pt_file))

                # å¯¼å‡ºä¸ºTensorRT FP16
                model.export(
                    format="engine",
                    device=0,
                    imgsz=640,
                    half=True,  # FP16ç²¾åº¦
                    workspace=4,  # 4GBå·¥ä½œç©ºé—´
                    simplify=True,
                    opset=12,
                    dynamic=False,
                    verbose=False,
                )

                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
                if engine_file.exists():
                    size_mb = engine_file.stat().st_size / (1024 * 1024)
                    logger.info(f"âœ… TensorRTè½¬æ¢æˆåŠŸ: {engine_file.name}")
                    logger.info(f"   æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
                    return str(engine_file)
                else:
                    logger.error("âŒ TensorRTè½¬æ¢å¤±è´¥: è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                    return model_path

            return str(engine_file)

        except Exception as e:
            logger.error(f"TensorRTè‡ªåŠ¨è½¬æ¢å¤±è´¥: {e}")
            logger.info("å›é€€åˆ°PyTorchæ¨¡å‹")
            return model_path

    def _load_model(self, model_path: str):
        """åŠ è½½YOLOæ¨¡å‹"""
        try:
            model = YOLO(model_path)
            # åœ¨æµ‹è¯•ç¯å¢ƒä¸­ä½¿ç”¨çš„ DummyYOLO å¯èƒ½ä¸å®ç° .to æ–¹æ³•ï¼Œè¿™é‡Œåšå…¼å®¹å¤„ç†
            if hasattr(model, "to"):
                model.to(self.device)
            logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_path} åˆ°è®¾å¤‡: {self.device}")
            return model
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.info("å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            return None

    def detect(self, image: np.ndarray) -> List[Dict]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„äººä½“

        Args:
            image: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«bboxã€confidenceã€class_idç­‰ä¿¡æ¯
        """
        if self.model is None:
            error_msg = "YOLOæ¨¡å‹æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è¿›è¡Œäººä½“æ£€æµ‹"
            logger.error(error_msg)
            raise RuntimeError(error_msg)

        try:
            logger.info(
                f"å¼€å§‹YOLOæ£€æµ‹ï¼Œå›¾åƒå°ºå¯¸: {image.shape}, ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}, IoUé˜ˆå€¼: {self.iou_threshold}"
            )

            results = self.model(
                image, conf=self.confidence_threshold, iou=self.iou_threshold
            )
            detections = []
            total_boxes = 0
            filtered_boxes = 0

            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    total_boxes += len(boxes)
                    logger.info(f"YOLOåŸå§‹æ£€æµ‹åˆ° {len(boxes)} ä¸ªç›®æ ‡")

                    for box in boxes:
                        # åªæ£€æµ‹äººä½“ (class_id = 0)
                        if int(box.cls[0]) == 0:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            confidence = float(box.conf[0].cpu().numpy())

                            # è®¡ç®—æ£€æµ‹æ¡†å±æ€§
                            width = x2 - x1
                            height = y2 - y1
                            area = width * height
                            aspect_ratio = max(width, height) / min(width, height)

                            logger.debug(
                                f"æ£€æµ‹æ¡†: ({x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}), ç½®ä¿¡åº¦: {confidence:.3f}, é¢ç§¯: {area:.1f}, å®½é«˜æ¯”: {aspect_ratio:.2f}"
                            )

                            # åº”ç”¨åå¤„ç†è¿‡æ»¤
                            if (
                                area >= self.min_box_area
                                and aspect_ratio <= self.max_box_ratio
                                and width > self.min_width
                                and height > self.min_height
                            ):  # ä½¿ç”¨é…ç½®çš„æœ€å°å°ºå¯¸è¦æ±‚
                                detection = {
                                    "bbox": [int(x1), int(y1), int(x2), int(y2)],
                                    "confidence": confidence,
                                    "class_id": 0,
                                    "class_name": "person",
                                }
                                detections.append(detection)
                                logger.debug(f"æ£€æµ‹æ¡†é€šè¿‡è¿‡æ»¤: {detection}")
                            else:
                                filtered_boxes += 1
                                logger.debug(
                                    f"æ£€æµ‹æ¡†è¢«è¿‡æ»¤: é¢ç§¯={area:.1f} (æœ€å°={self.min_box_area}), å®½é«˜æ¯”={aspect_ratio:.2f} (æœ€å¤§={self.max_box_ratio}), å°ºå¯¸={width:.1f}x{height:.1f}"
                                )

            logger.info(
                f"YOLOæ£€æµ‹å®Œæˆ: åŸå§‹æ£€æµ‹æ¡†={total_boxes}, è¿‡æ»¤å={len(detections)}, è¢«è¿‡æ»¤={filtered_boxes}"
            )
            return detections

        except Exception as e:
            error_msg = f"YOLOæ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(error_msg, exc_info=True)
            raise RuntimeError(error_msg) from e

    def detect_batch(self, images: List[np.ndarray]) -> List[List[Dict]]:
        """
        æ‰¹é‡æ£€æµ‹å¤šå¼ å›¾åƒ

        Args:
            images: å›¾åƒåˆ—è¡¨

        Returns:
            æ¯å¼ å›¾åƒçš„æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        for image in images:
            detections = self.detect(image)
            results.append(detections)
        return results

    def set_confidence_threshold(self, threshold: float):
        """è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼"""
        self.confidence_threshold = max(0.0, min(1.0, threshold))
        logger.info(f"Confidence threshold set to {self.confidence_threshold}")

    def set_iou_threshold(self, threshold: float):
        """è®¾ç½®IoUé˜ˆå€¼"""
        self.iou_threshold = max(0.0, min(1.0, threshold))
        logger.info(f"IoU threshold set to {self.iou_threshold}")

    def visualize_detections(
        self, image: np.ndarray, detections: List[Dict]
    ) -> np.ndarray:
        """
        åœ¨å›¾åƒä¸Šå¯è§†åŒ–æ£€æµ‹ç»“æœ

        Args:
            image: è¾“å…¥å›¾åƒ
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨

        Returns:
            å¸¦æœ‰æ£€æµ‹æ¡†çš„å›¾åƒ
        """
        return self.visualize(image, detections)
