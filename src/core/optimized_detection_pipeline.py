#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„æ£€æµ‹ç®¡é“ - å®ç°æ¨¡å‹å¤ç”¨ã€ç¼“å­˜å’Œç»Ÿä¸€å¤„ç†

ä¸»è¦ä¼˜åŒ–ç‚¹ï¼š
1. æ¨¡å‹åŠ è½½ç§»è‡³åˆå§‹åŒ–é˜¶æ®µï¼Œé¿å…é‡å¤åŠ è½½
2. æ„å»ºç»Ÿä¸€çš„BehaviorDetectionPipelineï¼Œå¤ç”¨ä¸­é—´ç»“æœ
3. æ˜ç¡®æ£€æµ‹é¡ºåºå’Œä¾èµ–å…³ç³»
4. å¢åŠ ç¼“å­˜æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯è§†é¢‘æµå¤„ç†
"""

import asyncio
import hashlib
import logging
import time
from collections import OrderedDict
from dataclasses import dataclass
from threading import Lock
from typing import Any, Dict, List, Optional

import cv2
import numpy as np

from src.config.unified_params import get_unified_params
from src.detection.pose_detector import PoseDetectorFactory

# å¯¼å…¥FrameMetadataç›¸å…³ç±»ï¼ˆå¯é€‰ï¼Œç”¨äºçŠ¶æ€ç®¡ç†å’Œå¼‚æ­¥å¤„ç†ï¼‰
try:
    from src.core.async_detection_pipeline import AsyncDetectionPipeline
    from src.core.frame_metadata import FrameMetadata, FrameSource
    from src.core.frame_metadata_manager import FrameMetadataManager
    from src.core.state_manager import StateManager

    FRAME_METADATA_AVAILABLE = True
except ImportError:
    FRAME_METADATA_AVAILABLE = False
    FrameMetadata = None
    FrameSource = None
    FrameMetadataManager = None
    StateManager = None
    AsyncDetectionPipeline = None

# çº§è”ç›¸å…³ä¾èµ–ï¼ˆå¯é€‰ï¼‰
try:
    from ultralytics import YOLO as _YOLOHeavy
except Exception:  # å‘ç”Ÿé”™è¯¯æ—¶å»¶è¿Ÿåˆ°è¿è¡ŒæœŸå†åˆ¤æ–­
    _YOLOHeavy = None  # type: ignore

try:
    from src.config.model_config import ModelConfig as _MC
except Exception:
    _MC = None  # type: ignore

logger = logging.getLogger(__name__)


@dataclass
class DetectionResult:
    """ç»Ÿä¸€çš„æ£€æµ‹ç»“æœæ•°æ®ç»“æ„"""

    person_detections: List[Dict]
    hairnet_results: List[Dict]
    handwash_results: List[Dict]
    sanitize_results: List[Dict]
    processing_times: Dict[str, float]
    hand_regions: Optional[List[Dict]] = None
    annotated_image: Optional[np.ndarray] = None
    frame_cache_key: Optional[str] = None


@dataclass
class CachedDetection:
    """ç¼“å­˜çš„æ£€æµ‹ç»“æœ"""

    result: DetectionResult
    timestamp: float
    frame_hash: str


class FrameCache:
    """å¸§ç¼“å­˜ç®¡ç†å™¨ - ç”¨äºè§†é¢‘æµå¤„ç†ä¼˜åŒ–"""

    def __init__(self, max_size: int = 100, ttl: float = 30.0):
        self.max_size = max_size
        self.ttl = ttl  # ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
        self.cache: OrderedDict[str, CachedDetection] = OrderedDict()
        self.lock = Lock()

    def _generate_frame_hash(self, frame: np.ndarray) -> str:
        """ç”Ÿæˆå¸§çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜é”®"""
        # ä½¿ç”¨å¸§çš„å½¢çŠ¶å’Œå›ºå®šæ­¥é•¿é‡‡æ ·ç”Ÿæˆç¨³å®šå“ˆå¸Œ
        h, w = frame.shape[:2]
        if h == 0 or w == 0:
            return f"{h}x{w}_empty"

        if h < 10 or w < 10:
            sampled = frame
        else:
            sampled = frame[::10, ::10]

        if sampled.size == 0:
            sampled = frame

        hasher = hashlib.md5()
        hasher.update(f"{h}x{w}_{frame.dtype}".encode("utf-8"))
        hasher.update(sampled.tobytes())
        return f"{h}x{w}_{hasher.hexdigest()}"

    def get(self, frame: np.ndarray) -> Optional[DetectionResult]:
        """ä»ç¼“å­˜è·å–æ£€æµ‹ç»“æœ"""
        frame_hash = self._generate_frame_hash(frame)

        with self.lock:
            if frame_hash in self.cache:
                cached = self.cache[frame_hash]
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if time.time() - cached.timestamp <= self.ttl:
                    # ç§»åˆ°æœ€åï¼ˆLRUï¼‰
                    self.cache.move_to_end(frame_hash)
                    logger.debug(f"ç¼“å­˜å‘½ä¸­: {frame_hash}")
                    return cached.result
                else:
                    # è¿‡æœŸï¼Œåˆ é™¤
                    del self.cache[frame_hash]

        return None

    def put(self, frame: np.ndarray, result: DetectionResult):
        """å°†æ£€æµ‹ç»“æœæ”¾å…¥ç¼“å­˜"""
        frame_hash = self._generate_frame_hash(frame)

        with self.lock:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„
            while len(self.cache) >= self.max_size:
                self.cache.popitem(last=False)

            cached = CachedDetection(
                result=result, timestamp=time.time(), frame_hash=frame_hash
            )
            self.cache[frame_hash] = cached
            logger.debug(f"ç¼“å­˜å­˜å‚¨: {frame_hash}")

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            logger.info("ç¼“å­˜å·²æ¸…ç©º")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                "cache_size": len(self.cache),
                "max_size": self.max_size,
                "ttl": self.ttl,
            }


class OptimizedDetectionPipeline:
    """ä¼˜åŒ–çš„æ£€æµ‹ç®¡é“ - ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ£€æµ‹ä»»åŠ¡"""

    def __init__(
        self,
        human_detector=None,
        hairnet_detector=None,
        behavior_recognizer=None,
        pose_detector=None,  # æ–°å¢å‚æ•°
        enable_cache: bool = True,
        cache_size: int = 100,
        cache_ttl: float = 30.0,
        cascade_config: Optional[Dict[str, Any]] = None,
        enable_state_management: bool = True,  # æ˜¯å¦å¯ç”¨çŠ¶æ€ç®¡ç†
        frame_metadata_manager: Optional[
            FrameMetadataManager
        ] = None,  # å¯é€‰çš„FrameMetadataManager
        enable_async: bool = False,  # æ˜¯å¦å¯ç”¨å¼‚æ­¥æ£€æµ‹ï¼ˆä»»åŠ¡1.3ï¼‰
        max_workers: int = 2,  # å¼‚æ­¥æ£€æµ‹çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–æ£€æµ‹ç®¡é“

        Args:
            human_detector: äººä½“æ£€æµ‹å™¨
            hairnet_detector: å‘ç½‘æ£€æµ‹å™¨
            behavior_recognizer: è¡Œä¸ºè¯†åˆ«å™¨
            pose_detector: å§¿æ€æ£€æµ‹å™¨å®ä¾‹ (å¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨æ­¤å®ä¾‹)
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            cache_size: ç¼“å­˜å¤§å°
            cache_ttl: ç¼“å­˜ç”Ÿå­˜æ—¶é—´
        """
        self.human_detector = human_detector
        self.hairnet_detector = hairnet_detector
        self.behavior_recognizer = behavior_recognizer

        # å¦‚æœæ²¡æœ‰æä¾›äººä½“æ£€æµ‹å™¨ï¼Œå°è¯•åˆå§‹åŒ–ä¸€ä¸ªé»˜è®¤çš„
        if self.human_detector is None:
            try:
                from src.detection.detector import HumanDetector

                self.human_detector = HumanDetector()
                logger.info("é»˜è®¤äººä½“æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"é»˜è®¤äººä½“æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.human_detector = None

        # åˆå§‹åŒ–å§¿æ€æ£€æµ‹å™¨
        if pose_detector is not None:
            self.pose_detector = pose_detector
            logger.info("å§¿æ€æ£€æµ‹å™¨ (å¤–éƒ¨æä¾›) åˆå§‹åŒ–æˆåŠŸ")
        else:
            try:
                params = get_unified_params()
                pose_backend = params.pose_detection.backend
                pose_params = params.pose_detection

                self.pose_detector = PoseDetectorFactory.create(
                    backend=pose_backend,
                    model_path=pose_params.model_path,
                    device=pose_params.device,
                )
                logger.info(f"å§¿æ€æ£€æµ‹å™¨ ({pose_backend}) åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"å§¿æ€æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.pose_detector = None

        # åˆå§‹åŒ–ç¼“å­˜
        self.enable_cache = enable_cache
        if enable_cache:
            self.frame_cache = FrameCache(max_size=cache_size, ttl=cache_ttl)
        else:
            self.frame_cache = None

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_detections": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_processing_time": 0.0,
        }

        logger.info(f"ä¼˜åŒ–æ£€æµ‹ç®¡é“åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜: {'å¯ç”¨' if enable_cache else 'ç¦ç”¨'}")

        # çº§è”ç›¸å…³
        self.cascade: Dict[str, Any] = cascade_config or {}
        self._cascade_model = None  # æƒ°æ€§åŠ è½½é‡æ¨¡å‹
        self.cascade_stats = {
            "triggers": 0,
            "refined": 0,
            "time_total": 0.0,
        }

        # çŠ¶æ€ç®¡ç†ç›¸å…³ï¼ˆä»»åŠ¡1.1ï¼‰
        self.enable_state_management = (
            enable_state_management and FRAME_METADATA_AVAILABLE
        )
        if self.enable_state_management:
            # åˆå§‹åŒ–FrameMetadataManagerï¼ˆä¸ä»»åŠ¡1.3å…±äº«ï¼‰
            self.frame_metadata_manager = (
                frame_metadata_manager
                or FrameMetadataManager(max_history=1000, sync_window=0.1)
            )

            # åˆå§‹åŒ–StateManager
            params = get_unified_params()
            state_params = getattr(params, "state_management", None)
            if state_params:
                stability_frames = getattr(state_params, "stability_frames", 5)
                confidence_threshold = getattr(
                    state_params, "confidence_threshold", 0.7
                )
            else:
                stability_frames = 5
                confidence_threshold = 0.7

            self.state_manager = StateManager(
                stability_frames=stability_frames,
                confidence_threshold=confidence_threshold,
                frame_metadata_manager=self.frame_metadata_manager,
            )
            logger.info("çŠ¶æ€ç®¡ç†å·²å¯ç”¨")
        else:
            self.frame_metadata_manager = None
            self.state_manager = None
            if enable_state_management:
                logger.warning("çŠ¶æ€ç®¡ç†è¢«è¯·æ±‚ä½†FrameMetadataä¸å¯ç”¨ï¼Œå·²ç¦ç”¨")

        # ä¿å­˜ç»Ÿä¸€å‚æ•°é…ç½®ï¼ˆç”¨äºå¯è§†åŒ–ç½®ä¿¡åº¦é˜ˆå€¼ï¼‰
        try:
            self.params = get_unified_params()
        except Exception as e:
            logger.warning(f"åŠ è½½ç»Ÿä¸€å‚æ•°é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self.params = None

        # å¼‚æ­¥æ£€æµ‹ç›¸å…³ï¼ˆä»»åŠ¡1.3ï¼‰
        self.enable_async = enable_async and FRAME_METADATA_AVAILABLE
        if self.enable_async:
            if AsyncDetectionPipeline is None:
                logger.warning("å¼‚æ­¥æ£€æµ‹è¢«è¯·æ±‚ä½†AsyncDetectionPipelineä¸å¯ç”¨ï¼Œå·²ç¦ç”¨")
                self.enable_async = False
                self.async_pipeline = None
            else:
                # åˆå§‹åŒ–AsyncDetectionPipelineï¼ˆå…±äº«FrameMetadataManagerï¼‰
                self.async_pipeline = AsyncDetectionPipeline(
                    human_detector=self.human_detector,
                    hairnet_detector=self.hairnet_detector,
                    pose_detector=self.pose_detector,
                    behavior_recognizer=self.behavior_recognizer,
                    frame_metadata_manager=self.frame_metadata_manager,  # å…±äº«
                    max_workers=max_workers,
                )
                logger.info(f"å¼‚æ­¥æ£€æµ‹å·²å¯ç”¨: max_workers={max_workers}")
        else:
            self.async_pipeline = None
            if enable_async:
                logger.warning("å¼‚æ­¥æ£€æµ‹è¢«è¯·æ±‚ä½†FrameMetadataä¸å¯ç”¨ï¼Œå·²ç¦ç”¨")

    def detect(self, image: np.ndarray, **kwargs) -> DetectionResult:
        """æ£€æµ‹æ–¹æ³• - detect_comprehensiveçš„åˆ«åï¼Œä¿æŒæ¥å£å…¼å®¹æ€§"""
        return self.detect_comprehensive(
            image,
            enable_hairnet=kwargs.get("enable_hairnet", True),
            enable_handwash=kwargs.get("enable_handwash", True),
            enable_sanitize=kwargs.get("enable_sanitize", True),
        )

    def detect_comprehensive(
        self,
        image: np.ndarray,
        enable_hairnet: bool = True,
        enable_handwash: bool = True,
        enable_sanitize: bool = True,
        force_refresh: bool = False,
        camera_id: str = "default",  # ç”¨äºFrameMetadata
    ) -> DetectionResult:
        """
        ç»¼åˆæ£€æµ‹ - ç»Ÿä¸€å…¥å£ç‚¹

        Args:
            image: è¾“å…¥å›¾åƒ
            enable_hairnet: æ˜¯å¦å¯ç”¨å‘ç½‘æ£€æµ‹
            enable_handwash: æ˜¯å¦å¯ç”¨æ´—æ‰‹æ£€æµ‹
            enable_sanitize: æ˜¯å¦å¯ç”¨æ¶ˆæ¯’æ£€æµ‹
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            camera_id: æ‘„åƒå¤´IDï¼ˆç”¨äºFrameMetadataï¼‰

        Returns:
            DetectionResult: ç»¼åˆæ£€æµ‹ç»“æœ
        """
        start_time = time.time()

        # æ£€æŸ¥ç¼“å­˜
        if self.enable_cache and self.frame_cache is not None and not force_refresh:
            cached_result = self.frame_cache.get(image)
            if cached_result is not None:
                self.stats["cache_hits"] += 1
                logger.debug("ä½¿ç”¨ç¼“å­˜çš„æ£€æµ‹ç»“æœ")
                return cached_result
            else:
                self.stats["cache_misses"] += 1

        # æ‰§è¡Œæ£€æµ‹æµæ°´çº¿ï¼ˆæ”¯æŒå¼‚æ­¥å’ŒåŒæ­¥ä¸¤ç§æ¨¡å¼ï¼‰
        if self.enable_async and self.async_pipeline:
            # ä½¿ç”¨å¼‚æ­¥æ£€æµ‹ï¼ˆä»»åŠ¡1.3ï¼‰
            result = self._execute_detection_pipeline_async(
                image, camera_id, enable_hairnet, enable_handwash, enable_sanitize
            )
        else:
            # ä½¿ç”¨åŒæ­¥æ£€æµ‹ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            result = self._execute_detection_pipeline(
                image, enable_hairnet, enable_handwash, enable_sanitize
            )

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        self.stats["total_detections"] += 1
        self.stats["avg_processing_time"] = (
            self.stats["avg_processing_time"] * (self.stats["total_detections"] - 1)
            + total_time
        ) / self.stats["total_detections"]

        # å­˜å…¥ç¼“å­˜
        if self.enable_cache and self.frame_cache is not None:
            self.frame_cache.put(image, result)

        return result

    def _execute_detection_pipeline_async(
        self,
        image: np.ndarray,
        camera_id: str,
        enable_hairnet: bool,
        enable_handwash: bool,
        enable_sanitize: bool,
    ) -> DetectionResult:
        """
        ä½¿ç”¨å¼‚æ­¥æ£€æµ‹ç®¡é“æ‰§è¡Œæ£€æµ‹

        Args:
            image: è¾“å…¥å›¾åƒ
            camera_id: æ‘„åƒå¤´ID
            enable_hairnet: æ˜¯å¦å¯ç”¨å‘ç½‘æ£€æµ‹
            enable_handwash: æ˜¯å¦å¯ç”¨æ´—æ‰‹æ£€æµ‹
            enable_sanitize: æ˜¯å¦å¯ç”¨æ¶ˆæ¯’æ£€æµ‹

        Returns:
            DetectionResult: ç»¼åˆæ£€æµ‹ç»“æœ
        """
        # åˆ›å»ºFrameMetadata
        frame_meta = self.frame_metadata_manager.create_frame_metadata(
            frame=image, camera_id=camera_id, source=FrameSource.REALTIME_STREAM
        )

        # æ‰§è¡Œå¼‚æ­¥æ£€æµ‹
        frame_meta = asyncio.run(
            self.async_pipeline.detect_comprehensive_async(
                frame_meta, enable_hairnet, enable_handwash, enable_sanitize
            )
        )

        # åº”ç”¨çŠ¶æ€ç¨³å®šåˆ¤å®šï¼ˆä»»åŠ¡1.1ï¼‰
        if self.enable_state_management and self.state_manager:
            for hairnet_result in frame_meta.hairnet_results:
                hairnet_confidence = hairnet_result.get("hairnet_confidence", 0.0)
                has_hairnet = hairnet_result.get("has_hairnet", False)

                # å¦‚æœæœªä½©æˆ´å‘ç½‘ï¼Œä½¿ç”¨ç½®ä¿¡åº¦ä½œä¸ºè¿è§„ç½®ä¿¡åº¦
                if has_hairnet is False:
                    violation_confidence = hairnet_confidence
                else:
                    violation_confidence = 0.0

                self.state_manager.update_state(frame_meta, violation_confidence)

        # è½¬æ¢ä¸ºDetectionResultï¼ˆå‘åå…¼å®¹ï¼‰
        # ä¼ é€’åŸå§‹å›¾åƒç”¨äºåˆ›å»ºå¯è§†åŒ–å›¾ç‰‡
        return self._frame_meta_to_detection_result(frame_meta, image)

    def _frame_meta_to_detection_result(
        self,
        frame_meta: FrameMetadata,
        image: Optional[np.ndarray] = None,
    ) -> DetectionResult:
        """
        å°†FrameMetadataè½¬æ¢ä¸ºDetectionResultï¼ˆå‘åå…¼å®¹ï¼‰

        Args:
            frame_meta: å¸§å…ƒæ•°æ®
            image: åŸå§‹å›¾åƒï¼ˆç”¨äºåˆ›å»ºå¯è§†åŒ–å›¾ç‰‡ï¼Œå¦‚æœframe_meta.frameä¸ºNoneï¼‰

        Returns:
            DetectionResult: æ£€æµ‹ç»“æœ
        """
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_times = frame_meta.processing_times.copy()
        if "total" not in processing_times:
            processing_times["total"] = sum(processing_times.values())

        # åˆ›å»ºå¯è§†åŒ–å›¾ç‰‡ï¼ˆå¦‚æœåŸå§‹å›¾åƒå¯ç”¨ï¼‰
        annotated_image = None
        source_image = frame_meta.frame if frame_meta.frame is not None else image
        if source_image is not None:
            try:
                # ä»é…ç½®ä¸­è·å–å¯è§†åŒ–æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.5ï¼‰
                min_confidence = 0.5
                if hasattr(self, "params") and self.params is not None:
                    # ä½¿ç”¨äººä½“æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ä½œä¸ºå¯è§†åŒ–é˜ˆå€¼ï¼Œä½†ä¸ä½äº0.5
                    human_conf = self.params.human_detection.confidence_threshold
                    min_confidence = max(0.5, human_conf)

                annotated_image = self._create_annotated_image(
                    source_image,
                    frame_meta.person_detections,
                    frame_meta.hairnet_results,
                    frame_meta.handwash_results,
                    frame_meta.sanitize_results,
                    hand_regions=None,
                    min_confidence=min_confidence,  # ä¼ é€’å¯è§†åŒ–ç½®ä¿¡åº¦é˜ˆå€¼
                )
            except Exception as e:
                logger.warning(f"åˆ›å»ºå¯è§†åŒ–å›¾ç‰‡å¤±è´¥: {e}", exc_info=True)

        return DetectionResult(
            person_detections=frame_meta.person_detections,
            hairnet_results=frame_meta.hairnet_results,
            handwash_results=frame_meta.handwash_results,
            sanitize_results=frame_meta.sanitize_results,
            processing_times=processing_times,
            hand_regions=None,
            annotated_image=annotated_image,
            frame_cache_key=frame_meta.frame_hash,
        )

    def _execute_detection_pipeline(
        self,
        image: np.ndarray,
        enable_hairnet: bool,
        enable_handwash: bool,
        enable_sanitize: bool,
    ) -> DetectionResult:
        """
        æ‰§è¡Œæ£€æµ‹æµæ°´çº¿ - æŒ‰ä¼˜åŒ–çš„é¡ºåºæ‰§è¡Œå„é¡¹æ£€æµ‹

        æ£€æµ‹é¡ºåºä¼˜åŒ–ï¼š
        1. äººä½“æ£€æµ‹ï¼ˆåŸºç¡€ï¼Œå…¶ä»–æ£€æµ‹ä¾èµ–æ­¤ç»“æœï¼‰
        2. å‘ç½‘æ£€æµ‹ï¼ˆä¾èµ–äººä½“æ£€æµ‹çš„å¤´éƒ¨åŒºåŸŸï¼‰
        3. è¡Œä¸ºæ£€æµ‹ï¼ˆæ´—æ‰‹ã€æ¶ˆæ¯’ï¼Œä¾èµ–äººä½“æ£€æµ‹ç»“æœï¼‰
        """
        processing_times = {}

        # é˜¶æ®µ1: äººä½“æ£€æµ‹ï¼ˆå¿…é¡»ï¼Œå…¶ä»–æ£€æµ‹çš„åŸºç¡€ï¼‰
        person_start = time.time()
        person_detections = self._detect_persons(image)
        processing_times["person_detection"] = time.time() - person_start

        logger.info(f"äººä½“æ£€æµ‹å®Œæˆ: æ£€æµ‹åˆ° {len(person_detections)} ä¸ªäºº")

        # å¯é€‰ï¼šçº§è”äºŒæ¬¡æ£€æµ‹ï¼Œå¯¹è¾¹ç•Œåˆ†æ•°æ®µæˆ–ROIå†…çš„ç›®æ ‡è¿›è¡Œé‡æ£€
        try:
            t0 = time.time()
            person_detections = self._cascade_refine_persons(image, person_detections)
            processing_times["cascade_refine"] = time.time() - t0
        except Exception as e:
            processing_times["cascade_refine"] = 0.0
            logger.debug(f"çº§è”ç»†åŒ–è·³è¿‡: {e}")

        # é˜¶æ®µ2: å‘ç½‘æ£€æµ‹ï¼ˆåŸºäºäººä½“æ£€æµ‹ç»“æœï¼‰
        hairnet_results = []
        if enable_hairnet and len(person_detections) > 0:
            hairnet_start = time.time()
            logger.debug(
                f"ğŸ”µ å¼€å§‹å‘ç½‘æ£€æµ‹: äººæ•°={len(person_detections)}, "
                f"hairnet_detector={'å­˜åœ¨' if self.hairnet_detector else 'ä¸å­˜åœ¨'}, "
                f"ç±»å‹={type(self.hairnet_detector).__name__ if self.hairnet_detector else 'None'}"
            )
            hairnet_results = self._detect_hairnet_for_persons(image, person_detections)
            processing_times["hairnet_detection"] = time.time() - hairnet_start
            logger.debug(
                f"ğŸ”µ å‘ç½‘æ£€æµ‹å®Œæˆ: å¤„ç†äº† {len(hairnet_results)} ä¸ªäºº, "
                f"è€—æ—¶={processing_times['hairnet_detection']:.3f}s"
            )

            # åº”ç”¨çŠ¶æ€ç¨³å®šåˆ¤å®šï¼ˆä»»åŠ¡1.1ï¼‰
            if self.enable_state_management and self.state_manager:
                state_start = time.time()
                hairnet_results = self._apply_state_management_to_hairnet_results(
                    hairnet_results, image
                )
                processing_times["state_management"] = time.time() - state_start
        else:
            processing_times["hairnet_detection"] = 0.0

        # é˜¶æ®µ3: è¡Œä¸ºæ£€æµ‹ï¼ˆåŸºäºäººä½“æ£€æµ‹ç»“æœï¼‰
        handwash_results = []
        sanitize_results = []
        hand_regions_map: Dict[int, List[Dict]] = {}
        hand_regions_flat: List[Dict] = []

        if (enable_handwash or enable_sanitize) and len(person_detections) > 0:
            behavior_start = time.time()

            # é¢„è®¡ç®—æ‰‹éƒ¨åŒºåŸŸï¼Œé¿å…é‡å¤æ¨ç†
            for i, detection in enumerate(person_detections):
                person_id = i + 1
                bbox = detection.get("bbox", [0, 0, 0, 0])
                regions = self._get_actual_hand_regions(image, bbox)
                hand_regions_map[person_id] = regions
                for region in regions:
                    region_with_id = region.copy()
                    region_with_id["person_id"] = person_id
                    hand_regions_flat.append(region_with_id)

            if enable_handwash:
                handwash_results = self._detect_handwash_for_persons(
                    image, person_detections, hand_regions_map=hand_regions_map
                )

            if enable_sanitize:
                sanitize_results = self._detect_sanitize_for_persons(
                    image, person_detections, hand_regions_map=hand_regions_map
                )

            processing_times["behavior_detection"] = time.time() - behavior_start
            logger.info(
                f"è¡Œä¸ºæ£€æµ‹å®Œæˆ: æ´—æ‰‹={len(handwash_results)}, æ¶ˆæ¯’={len(sanitize_results)}, "
                f"äººå‘˜æ•°={len(person_detections)}, è€—æ—¶={processing_times['behavior_detection']:.3f}s"
            )
        else:
            processing_times["behavior_detection"] = 0.0

        # é˜¶æ®µ4: ç»“æœå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
        viz_start = time.time()
        # ä»é…ç½®ä¸­è·å–å¯è§†åŒ–æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.5ï¼‰
        min_confidence = 0.5
        if hasattr(self, "params") and self.params is not None:
            # ä½¿ç”¨äººä½“æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ä½œä¸ºå¯è§†åŒ–é˜ˆå€¼ï¼Œä½†ä¸ä½äº0.5
            human_conf = self.params.human_detection.confidence_threshold
            min_confidence = max(0.5, human_conf)

        annotated_image = self._create_annotated_image(
            image,
            person_detections,
            hairnet_results,
            handwash_results,
            sanitize_results,
            hand_regions=hand_regions_flat,
            min_confidence=min_confidence,  # ä¼ é€’å¯è§†åŒ–ç½®ä¿¡åº¦é˜ˆå€¼
        )
        processing_times["visualization"] = time.time() - viz_start

        # è®¡ç®—æ€»å¤„ç†æ—¶é—´
        processing_times["total"] = sum(processing_times.values())

        return DetectionResult(
            person_detections=person_detections,
            hairnet_results=hairnet_results,
            handwash_results=handwash_results,
            sanitize_results=sanitize_results,
            processing_times=processing_times,
            hand_regions=hand_regions_flat,
            annotated_image=annotated_image,
        )

    def _apply_state_management_to_hairnet_results(
        self,
        hairnet_results: List[Dict],
        image: np.ndarray,
        camera_id: str = "default",
    ) -> List[Dict]:
        """
        å¯¹å‘ç½‘æ£€æµ‹ç»“æœåº”ç”¨çŠ¶æ€ç®¡ç†

        Args:
            hairnet_results: å‘ç½‘æ£€æµ‹ç»“æœåˆ—è¡¨
            image: è¾“å…¥å›¾åƒ
            camera_id: æ‘„åƒå¤´ID

        Returns:
            æ›´æ–°åçš„å‘ç½‘æ£€æµ‹ç»“æœåˆ—è¡¨ï¼ˆåŒ…å«ç¨³å®šçŠ¶æ€ä¿¡æ¯ï¼‰
        """
        if not self.enable_state_management or not self.state_manager:
            return hairnet_results

        updated_results = []

        for hairnet_result in hairnet_results:
            # è·å–track_idï¼ˆä»hairnet_resultæˆ–person_idï¼‰
            track_id = (
                hairnet_result.get("track_id")
                or f"person_{hairnet_result.get('person_id', 0)}"
            )

            # åˆ›å»ºFrameMetadataï¼ˆç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ç»Ÿä¸€çš„frame_metaï¼‰
            from datetime import datetime

            frame_meta = FrameMetadata(
                frame_id=f"{camera_id}_{time.time():.6f}",
                timestamp=datetime.utcnow(),
                camera_id=camera_id,
                source=FrameSource.REALTIME_STREAM,
                frame=image,
                metadata={"track_id": track_id},
                hairnet_results=[hairnet_result],
            )

            # è·å–å‘ç½‘ç½®ä¿¡åº¦
            hairnet_confidence = hairnet_result.get("hairnet_confidence", 0.0)
            has_hairnet = hairnet_result.get("has_hairnet", False)

            # å¦‚æœæœªä½©æˆ´å‘ç½‘ï¼Œä½¿ç”¨ç½®ä¿¡åº¦ä½œä¸ºè¿è§„ç½®ä¿¡åº¦ï¼›å¦‚æœä½©æˆ´ï¼Œè¿è§„ç½®ä¿¡åº¦ä¸º0
            if has_hairnet is False:
                violation_confidence = hairnet_confidence
            else:
                violation_confidence = 0.0

            # æ›´æ–°çŠ¶æ€
            stable_state, stable_confidence = self.state_manager.update_state(
                frame_meta, violation_confidence
            )

            # æ›´æ–°hairnet_result
            hairnet_result["stable_state"] = stable_state
            hairnet_result["stable_confidence"] = stable_confidence
            updated_results.append(hairnet_result)

        return updated_results

    # ----------------------- çº§è”é€»è¾‘ -----------------------
    def _cascade_refine_persons(
        self, image: np.ndarray, person_detections: List[Dict]
    ) -> List[Dict]:
        """æŒ‰é…ç½®å¯¹æŒ‡å®šç›®æ ‡è¿›è¡Œçº§è”é‡æ£€å¹¶ç»†åŒ–æ¡†/åˆ†æ•°ã€‚

        ç­–ç•¥ï¼š
        - è‹¥ cascade.enable=False æˆ–ç¼ºå°‘ heavy_weightsï¼Œåˆ™ç›´æ¥è¿”å›åŸç»“æœï¼›
        - è‹¥é…ç½®äº† trigger_confidence_range=[lo,hi]ï¼Œä»…å¯¹è½å…¥åŒºé—´çš„ç›®æ ‡è§¦å‘ï¼›
        - è‹¥é…ç½®äº† trigger_roiï¼ˆå¤šè¾¹å½¢ï¼‰ï¼Œä»…å¯¹ä¸­å¿ƒç‚¹è½å…¥ ROI çš„ç›®æ ‡è§¦å‘ï¼›
        - åœ¨ROIï¼ˆäººæ¡†æˆ–æŒ‡å®šROIï¼‰å†…ä½¿ç”¨é‡æ¨¡å‹æ£€æµ‹ person ç±»ï¼Œå–æœ€é«˜åˆ†ï¼Œæ˜ å°„å›å…¨å›¾æ›´æ–° bbox/scoreï¼›
        - è®°å½•è§¦å‘æ¬¡æ•°ã€æˆåŠŸç»†åŒ–æ¬¡æ•°ä¸è€—æ—¶ã€‚
        """

        cfg = self.cascade or {}
        if not bool(cfg.get("enable", False)):
            return person_detections

        heavy_weights: Optional[str] = cfg.get("heavy_weights")
        if not heavy_weights:
            logger.warning("çº§è”å¯ç”¨ä½†æœªæä¾› heavy_weightsï¼Œè·³è¿‡çº§è”")
            return person_detections

        # æƒ°æ€§åŠ è½½é‡æ¨¡å‹
        if self._cascade_model is None:
            if _YOLOHeavy is None:
                logger.warning("æœªå®‰è£… ultralyticsï¼Œæ— æ³•æ‰§è¡Œçº§è”é‡æ£€")
                return person_detections
            try:
                self._cascade_model = _YOLOHeavy(heavy_weights)
                # è®¾å¤‡é€‰æ‹©ï¼ˆå°½é‡ä¸ç»Ÿä¸€ç­–ç•¥ä¸€è‡´ï¼‰
                if _MC is not None:
                    dev = _MC().select_device(requested=None)
                    if hasattr(self._cascade_model, "to"):
                        self._cascade_model.to(dev)
                logger.info(f"çº§è”é‡æ¨¡å‹å·²åŠ è½½: {heavy_weights}")
            except Exception as e:
                logger.warning(f"çº§è”é‡æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡çº§è”: {e}")
                return person_detections

        trig_range = cfg.get("trigger_confidence_range") or None
        roi_poly = cfg.get("trigger_roi") or None  # [[x,y], ...]

        def _in_range(score: float) -> bool:
            try:
                if not trig_range or len(trig_range) != 2:
                    return True
                lo, hi = float(trig_range[0]), float(trig_range[1])
                return lo <= float(score) <= hi
            except Exception:
                return True

        def _pt_in_poly(px: float, py: float, poly: List[List[float]]) -> bool:
            # å°„çº¿æ³•
            inside = False
            n = len(poly)
            for i in range(n):
                x1, y1 = poly[i]
                x2, y2 = poly[(i + 1) % n]
                cond = ((y1 > py) != (y2 > py)) and (
                    px < (x2 - x1) * (py - y1) / (y2 - y1 + 1e-6) + x1
                )
                if cond:
                    inside = not inside
            return inside

        refined: List[Dict] = []
        img_h, img_w = image.shape[:2]
        t_begin = time.time()
        triggers = 0
        refined_cnt = 0

        for det in person_detections:
            try:
                bbox = det.get("bbox", [0, 0, 0, 0])
                score = float(det.get("confidence", 1.0))
                x1, y1, x2, y2 = [int(v) for v in bbox]
                if x2 <= x1 or y2 <= y1:
                    refined.append(det)
                    continue

                # è§¦å‘æ¡ä»¶ï¼šåˆ†æ•°åŒºé—´ + ROIï¼ˆå¯é€‰ï¼‰
                cx = (x1 + x2) / 2.0
                cy = (y1 + y2) / 2.0
                if not _in_range(score):
                    refined.append(det)
                    continue
                if isinstance(roi_poly, list) and len(roi_poly) >= 3:
                    if not _pt_in_poly(cx, cy, roi_poly):
                        refined.append(det)
                        continue

                triggers += 1

                # åœ¨äººæ¡†ROIä¸Šæ‰§è¡Œé‡æ£€
                roi = image[y1:y2, x1:x2]
                if roi.size == 0:
                    refined.append(det)
                    continue

                res = self._cascade_model(roi)
                best = None
                for r in res:
                    boxes = getattr(r, "boxes", None)
                    if boxes is None:
                        continue
                    for b in boxes:
                        try:
                            if int(b.cls[0]) != 0:  # ä»…person
                                continue
                            conf = float(b.conf[0].cpu().numpy())
                            bx1, by1, bx2, by2 = [
                                float(v) for v in b.xyxy[0].cpu().numpy()
                            ]
                            if best is None or conf > best[0]:
                                best = (conf, bx1, by1, bx2, by2)
                        except Exception:
                            continue

                if best is None:
                    refined.append(det)
                    continue

                conf_h, bx1, by1, bx2, by2 = best
                # æ˜ å°„å›å…¨å›¾åæ ‡
                gx1 = int(x1 + max(0.0, bx1))
                gy1 = int(y1 + max(0.0, by1))
                gx2 = int(x1 + min(float(x2 - x1), bx2))
                gy2 = int(y1 + min(float(y2 - y1), by2))
                if gx2 > gx1 and gy2 > gy1:
                    det = det.copy()
                    det["bbox"] = [gx1, gy1, gx2, gy2]
                    det["confidence"] = max(
                        float(det.get("confidence", 0.0)), float(conf_h)
                    )
                    det["cascade_refined"] = True
                    refined_cnt += 1
                refined.append(det)
            except Exception:
                refined.append(det)

        self.cascade_stats["triggers"] += triggers
        self.cascade_stats["refined"] += refined_cnt
        self.cascade_stats["time_total"] += max(0.0, time.time() - t_begin)

        if triggers:
            logger.info(
                f"çº§è”ï¼šè§¦å‘={triggers}, ç»†åŒ–={refined_cnt}, æ€»è€—æ—¶+={time.time() - t_begin:.3f}s"
            )

        return refined

    def _detect_persons(self, image: np.ndarray) -> List[Dict]:
        """äººä½“æ£€æµ‹ - æ‰€æœ‰å…¶ä»–æ£€æµ‹çš„åŸºç¡€

        Args:
            image: è¾“å…¥å›¾åƒ

        Returns:
            List[Dict]: äººä½“æ£€æµ‹ç»“æœåˆ—è¡¨

        Raises:
            RuntimeError: å½“äººä½“æ£€æµ‹å™¨æœªåˆå§‹åŒ–æˆ–æ£€æµ‹å¤±è´¥æ—¶
        """
        if self.human_detector is None:
            raise RuntimeError(
                "äººä½“æ£€æµ‹å™¨æœªåˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥ï¼š\n" "1. æ£€æµ‹æœåŠ¡æ˜¯å¦æ­£ç¡®å¯åŠ¨\n" "2. äººä½“æ£€æµ‹æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n" "3. ç³»ç»Ÿä¾èµ–æ˜¯å¦å®Œæ•´"
            )

        detections = self.human_detector.detect(image)
        return detections if detections else []

    def _detect_hairnet_for_persons(
        self, image: np.ndarray, person_detections: List[Dict]
    ) -> List[Dict]:
        """ä¸ºæ£€æµ‹åˆ°çš„äººå‘˜è¿›è¡Œå‘ç½‘æ£€æµ‹

        Args:
            image: è¾“å…¥å›¾åƒ
            person_detections: äººä½“æ£€æµ‹ç»“æœåˆ—è¡¨

        Returns:
            List[Dict]: å‘ç½‘æ£€æµ‹ç»“æœåˆ—è¡¨

        Raises:
            RuntimeError: å½“å‘ç½‘æ£€æµ‹å™¨æœªåˆå§‹åŒ–æ—¶
        """
        if self.hairnet_detector is None:
            raise RuntimeError(
                "å‘ç½‘æ£€æµ‹å™¨æœªåˆå§‹åŒ–ã€‚è¯·æ£€æŸ¥ï¼š\n" "1. æ£€æµ‹æœåŠ¡æ˜¯å¦æ­£ç¡®å¯åŠ¨\n" "2. å‘ç½‘æ£€æµ‹æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n" "3. ç³»ç»Ÿä¾èµ–æ˜¯å¦å®Œæ•´"
            )

        hairnet_results = []

        try:
            # å¯¹äºYOLOHairnetDetectorï¼Œç›´æ¥ä¼ é€’å®Œæ•´å›¾åƒè¿›è¡Œæ£€æµ‹
            if hasattr(self.hairnet_detector, "detect_hairnet_compliance"):
                logger.debug(
                    f"ğŸ”µ è°ƒç”¨YOLOHairnetDetector.detect_hairnet_compliance: "
                    f"äººæ•°={len(person_detections)}, å›¾åƒå¤§å°={image.shape}"
                )
                # ä½¿ç”¨YOLOHairnetDetectorçš„detect_hairnet_complianceæ–¹æ³•ï¼Œä¼ é€’å·²æœ‰çš„äººä½“æ£€æµ‹ç»“æœé¿å…é‡å¤æ£€æµ‹
                compliance_result = self.hairnet_detector.detect_hairnet_compliance(
                    image, person_detections
                )
                logger.debug(
                    f"ğŸ”µ YOLOHairnetDetectorè¿”å›ç»“æœ: "
                    f"total_persons={compliance_result.get('total_persons', 0)}, "
                    f"persons_with_hairnet={compliance_result.get('persons_with_hairnet', 0)}, "
                    f"detectionsæ•°é‡={len(compliance_result.get('detections', []))}"
                )

                # ä»åˆè§„æ£€æµ‹ç»“æœä¸­æå–æ¯ä¸ªäººçš„å‘ç½‘ä¿¡æ¯
                detections = compliance_result.get("detections", [])

                for i, person_detection in enumerate(person_detections):
                    person_bbox = person_detection.get("bbox", [0, 0, 0, 0])

                    # æŸ¥æ‰¾ä¸è¯¥äººå‘˜å¯¹åº”çš„å‘ç½‘æ£€æµ‹ç»“æœ
                    has_hairnet = False
                    hairnet_confidence = 0.0
                    hairnet_bbox = person_bbox

                    # åœ¨åˆè§„æ£€æµ‹ç»“æœä¸­æŸ¥æ‰¾å¯¹åº”çš„äººå‘˜
                    if i < len(detections):
                        detection_info = detections[i]
                        has_hairnet = detection_info.get("has_hairnet", False)
                        hairnet_confidence = detection_info.get(
                            "hairnet_confidence", 0.0
                        )
                        hairnet_bbox = detection_info.get("bbox", person_bbox)

                    # è®¡ç®—å¤´éƒ¨åŒºåŸŸåæ ‡ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
                    # ä¼˜åŒ–ï¼šä½¿ç”¨35%é«˜åº¦ï¼Œä¸YOLOHairnetDetectorä¿æŒä¸€è‡´
                    x1, y1, x2, y2 = map(int, person_bbox)
                    person_height = y2 - y1
                    person_width = x2 - x1
                    head_height = int(person_height * 0.35)  # ä»30%å¢åŠ åˆ°35%
                    padding_height = int(head_height * 0.2)  # 20%padding
                    padding_width = int(person_width * 0.1)  # 10%paddingå®½åº¦

                    head_y1 = max(0, y1 - padding_height)
                    head_y2 = min(image.shape[0], y1 + head_height + padding_height)
                    head_x1 = max(0, x1 - padding_width)
                    head_x2 = min(image.shape[1], x2 + padding_width)

                    hairnet_results.append(
                        {
                            "person_id": i + 1,
                            "person_bbox": person_bbox,
                            "head_bbox": [head_x1, head_y1, head_x2, head_y2],
                            "has_hairnet": has_hairnet,
                            "hairnet_confidence": hairnet_confidence,
                            "hairnet_bbox": hairnet_bbox,
                        }
                    )
            else:
                # å¯¹äºä¼ ç»Ÿçš„å‘ç½‘æ£€æµ‹å™¨ï¼Œä½¿ç”¨å¤´éƒ¨åŒºåŸŸæ£€æµ‹
                for i, detection in enumerate(person_detections):
                    try:
                        bbox = detection.get("bbox", [0, 0, 0, 0])
                        x1, y1, x2, y2 = map(int, bbox)

                        # æå–å¤´éƒ¨åŒºåŸŸ
                        # ä¼˜åŒ–ï¼šä½¿ç”¨35%é«˜åº¦ï¼Œä¸YOLOHairnetDetectorä¿æŒä¸€è‡´
                        person_height = y2 - y1
                        person_width = x2 - x1
                        head_height = int(person_height * 0.35)  # ä»30%å¢åŠ åˆ°35%
                        padding_height = int(head_height * 0.2)  # 20%padding
                        padding_width = int(person_width * 0.1)  # 10%paddingå®½åº¦

                        head_y1 = max(0, y1 - padding_height)
                        head_y2 = min(image.shape[0], y1 + head_height + padding_height)
                        head_x1 = max(0, x1 - padding_width)
                        head_x2 = min(image.shape[1], x2 + padding_width)

                        if head_y2 > head_y1 and head_x2 > head_x1:
                            head_region = image[head_y1:head_y2, head_x1:head_x2]
                            hairnet_result = (
                                self.hairnet_detector.detect_hairnet_compliance(
                                    head_region
                                )
                            )

                            hairnet_results.append(
                                {
                                    "person_id": i + 1,
                                    "person_bbox": bbox,
                                    "head_bbox": [head_x1, head_y1, head_x2, head_y2],
                                    "has_hairnet": hairnet_result.get(
                                        "wearing_hairnet", False
                                    ),
                                    "hairnet_confidence": hairnet_result.get(
                                        "confidence", 0.0
                                    ),
                                    "hairnet_bbox": hairnet_result.get(
                                        "head_roi_coords",
                                        [head_x1, head_y1, head_x2, head_y2],
                                    ),
                                }
                            )
                    except Exception as e:
                        logger.error(f"äººå‘˜ {i+1} å‘ç½‘æ£€æµ‹å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"å‘ç½‘æ£€æµ‹è¿‡ç¨‹å¤±è´¥: {e}")

        return hairnet_results

    def _detect_handwash_for_persons(
        self,
        image: np.ndarray,
        person_detections: List[Dict],
        hand_regions_map: Optional[Dict[int, List[Dict]]] = None,
    ) -> List[Dict]:
        """ä¸ºæ£€æµ‹åˆ°çš„äººå‘˜è¿›è¡Œæ´—æ‰‹è¡Œä¸ºæ£€æµ‹"""
        if self.behavior_recognizer is None:
            logger.warning("è¡Œä¸ºè¯†åˆ«å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
            # ä½¿ç”¨æ¨¡æ‹Ÿç»“æœï¼Œå‡è®¾æ‰€æœ‰äººéƒ½åœ¨æ´—æ‰‹
            return [
                {
                    "person_id": i + 1,
                    "person_bbox": detection.get("bbox", [0, 0, 0, 0]),
                    "is_handwashing": True,  # æ¨¡æ‹Ÿæ‰€æœ‰äººéƒ½åœ¨æ´—æ‰‹
                    "handwashing": True,  # å…¼å®¹æ€§å­—æ®µ
                    "handwash_confidence": 0.85,
                }
                for i, detection in enumerate(person_detections)
            ]

        handwash_results = []

        for i, detection in enumerate(person_detections):
            try:
                # è°ƒç”¨å®é™…çš„æ´—æ‰‹æ£€æµ‹é€»è¾‘
                bbox = detection.get("bbox", [0, 0, 0, 0])
                x1, y1, x2, y2 = map(int, bbox)

                # æå–äººä½“åŒºåŸŸè¿›è¡Œè¡Œä¸ºåˆ†æ
                person_region = image[y1:y2, x1:x2]

                if person_region.size > 0:
                    # ä½¿ç”¨è¡Œä¸ºè¯†åˆ«å™¨æ£€æµ‹æ´—æ‰‹è¡Œä¸º
                    # è·å–å®é™…çš„æ‰‹éƒ¨åŒºåŸŸä¿¡æ¯
                    if hand_regions_map is not None:
                        hand_regions = hand_regions_map.get(i + 1, [])
                    else:
                        hand_regions = self._get_actual_hand_regions(image, bbox)

                    # ä¼ é€’å®Œæ•´å›¾åƒå¸§ç»™è¡Œä¸ºè¯†åˆ«å™¨ä»¥æ”¯æŒMediaPipeæ£€æµ‹
                    confidence = self.behavior_recognizer.detect_handwashing(
                        bbox, hand_regions, track_id=i + 1, frame=image
                    )
                    is_handwashing = (
                        confidence >= self.behavior_recognizer.confidence_threshold
                    )

                    # æ·»åŠ è°ƒè¯•æ—¥å¿—
                    logger.info(
                        f"äººå‘˜ {i+1} æ´—æ‰‹æ£€æµ‹: ç½®ä¿¡åº¦={confidence:.3f}, é˜ˆå€¼={self.behavior_recognizer.confidence_threshold}, ç»“æœ={is_handwashing}"
                    )
                else:
                    is_handwashing = False
                    confidence = 0.0

                handwash_results.append(
                    {
                        "person_id": i + 1,
                        "person_bbox": bbox,
                        "is_handwashing": is_handwashing,
                        "handwashing": is_handwashing,  # å…¼å®¹æ€§å­—æ®µ
                        "handwash_confidence": confidence,
                    }
                )
            except Exception as e:
                logger.error(f"äººå‘˜ {i+1} æ´—æ‰‹æ£€æµ‹å¤±è´¥: {e}")
                # æ·»åŠ é»˜è®¤ç»“æœ
                handwash_results.append(
                    {
                        "person_id": i + 1,
                        "person_bbox": detection.get("bbox", [0, 0, 0, 0]),
                        "is_handwashing": True,  # é»˜è®¤å‡è®¾åœ¨æ´—æ‰‹
                        "handwashing": True,
                        "handwash_confidence": 0.5,
                    }
                )

        return handwash_results

    def _detect_sanitize_for_persons(
        self,
        image: np.ndarray,
        person_detections: List[Dict],
        hand_regions_map: Optional[Dict[int, List[Dict]]] = None,
    ) -> List[Dict]:
        """ä¸ºæ£€æµ‹åˆ°çš„äººå‘˜è¿›è¡Œæ¶ˆæ¯’è¡Œä¸ºæ£€æµ‹"""
        if self.behavior_recognizer is None:
            logger.warning("è¡Œä¸ºè¯†åˆ«å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
            # ä½¿ç”¨æ¨¡æ‹Ÿç»“æœï¼Œå‡è®¾æ‰€æœ‰äººéƒ½åœ¨æ¶ˆæ¯’
            return [
                {
                    "person_id": i + 1,
                    "person_bbox": detection.get("bbox", [0, 0, 0, 0]),
                    "is_sanitizing": True,  # æ¨¡æ‹Ÿæ‰€æœ‰äººéƒ½åœ¨æ¶ˆæ¯’
                    "sanitizing": True,  # å…¼å®¹æ€§å­—æ®µ
                    "sanitize_confidence": 0.85,
                }
                for i, detection in enumerate(person_detections)
            ]

        sanitize_results = []

        for i, detection in enumerate(person_detections):
            try:
                # è°ƒç”¨å®é™…çš„æ¶ˆæ¯’æ£€æµ‹é€»è¾‘
                bbox = detection.get("bbox", [0, 0, 0, 0])
                x1, y1, x2, y2 = map(int, bbox)

                # æå–äººä½“åŒºåŸŸè¿›è¡Œè¡Œä¸ºåˆ†æ
                person_region = image[y1:y2, x1:x2]

                if person_region.size > 0:
                    # ä½¿ç”¨è¡Œä¸ºè¯†åˆ«å™¨æ£€æµ‹æ¶ˆæ¯’è¡Œä¸º
                    # è·å–å®é™…çš„æ‰‹éƒ¨åŒºåŸŸä¿¡æ¯
                    if hand_regions_map is not None:
                        hand_regions = hand_regions_map.get(i + 1, [])
                    else:
                        hand_regions = self._get_actual_hand_regions(image, bbox)

                    # ä¼ é€’å®Œæ•´å›¾åƒå¸§ç»™è¡Œä¸ºè¯†åˆ«å™¨ä»¥æ”¯æŒMediaPipeæ£€æµ‹
                    confidence = self.behavior_recognizer.detect_sanitizing(
                        bbox, hand_regions, track_id=i + 1, frame=image
                    )
                    is_sanitizing = (
                        confidence >= self.behavior_recognizer.confidence_threshold
                    )
                else:
                    is_sanitizing = False
                    confidence = 0.0

                sanitize_results.append(
                    {
                        "person_id": i + 1,
                        "person_bbox": bbox,
                        "is_sanitizing": is_sanitizing,
                        "sanitizing": is_sanitizing,  # å…¼å®¹æ€§å­—æ®µ
                        "sanitize_confidence": confidence,
                    }
                )
            except Exception as e:
                logger.error(f"äººå‘˜ {i+1} æ¶ˆæ¯’æ£€æµ‹å¤±è´¥: {e}")
                # æ·»åŠ é»˜è®¤ç»“æœ
                sanitize_results.append(
                    {
                        "person_id": i + 1,
                        "person_bbox": detection.get("bbox", [0, 0, 0, 0]),
                        "is_sanitizing": True,  # é»˜è®¤å‡è®¾åœ¨æ¶ˆæ¯’
                        "sanitizing": True,
                        "sanitize_confidence": 0.5,
                    }
                )

        return sanitize_results

    def _estimate_hand_regions(self, person_bbox: List[int]) -> List[Dict]:
        """
        ä¼°ç®—äººä½“çš„æ‰‹éƒ¨åŒºåŸŸï¼Œä¼˜å…ˆä½¿ç”¨å§¿æ€æ£€æµ‹å™¨

        Args:
            person_bbox: äººä½“è¾¹ç•Œæ¡† [x1, y1, x2, y2]

        Returns:
            æ‰‹éƒ¨åŒºåŸŸåˆ—è¡¨
        """
        # å¦‚æœæœ‰å§¿æ€æ£€æµ‹å™¨ï¼Œå°è¯•ä½¿ç”¨å®é™…çš„æ‰‹éƒ¨æ£€æµ‹
        if self.pose_detector is not None:
            try:
                # ä»äººä½“åŒºåŸŸæå–å›¾åƒè¿›è¡Œæ‰‹éƒ¨æ£€æµ‹
                x1, y1, x2, y2 = person_bbox
                # è¿™é‡Œéœ€è¦å®Œæ•´å›¾åƒï¼Œæ‰€ä»¥è¿”å›ä¼°ç®—ç»“æœ
                # å®é™…çš„æ‰‹éƒ¨æ£€æµ‹åœ¨å…¶ä»–åœ°æ–¹è¿›è¡Œ
            except Exception as e:
                logger.info(f"å§¿æ€æ£€æµ‹å™¨æ‰‹éƒ¨æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•: {e}")

        # ä½¿ç”¨ä¼°ç®—æ–¹æ³•
        x1, y1, x2, y2 = person_bbox
        width = x2 - x1
        height = y2 - y1

        # ä¼°ç®—æ‰‹éƒ¨å¤§å°ï¼ˆç›¸å¯¹äºäººä½“å°ºå¯¸ï¼‰
        hand_box_h = int(0.15 * height)
        hand_box_w = int(0.25 * width)

        # ä¼°ç®—å·¦å³æ‰‹ä½ç½®ï¼ˆåœ¨äººä½“ä¸­ä¸‹éƒ¨ï¼‰
        hand_y = y1 + int(0.55 * height)

        left_hand_bbox = [x1, hand_y, x1 + hand_box_w, hand_y + hand_box_h]
        right_hand_bbox = [x2 - hand_box_w, hand_y, x2, hand_y + hand_box_h]

        return [{"bbox": left_hand_bbox}, {"bbox": right_hand_bbox}]

    def _get_actual_hand_regions(
        self, image: np.ndarray, person_bbox: List[int]
    ) -> List[Dict]:
        """
        è·å–å®é™…çš„æ‰‹éƒ¨åŒºåŸŸï¼Œä¼˜å…ˆä½¿ç”¨å§¿æ€æ£€æµ‹å™¨

        Args:
            image: å®Œæ•´å›¾åƒ
            person_bbox: äººä½“è¾¹ç•Œæ¡† [x1, y1, x2, y2]

        Returns:
            æ‰‹éƒ¨åŒºåŸŸåˆ—è¡¨
        """
        hand_regions = []

        # å¦‚æœæœ‰å§¿æ€æ£€æµ‹å™¨ï¼Œä¼˜å…ˆåœ¨äººæ¡†ROIä¸Šæ‰§è¡Œæ‰‹éƒ¨æ£€æµ‹ï¼Œå¹¶æ˜ å°„å›å…¨å›¾åæ ‡
        if self.pose_detector is not None:
            try:
                x1, y1, x2, y2 = [int(v) for v in person_bbox]
                # å¤–æ‰©20%è¾¹è·ï¼Œå¹¶è£å›å›¾åƒèŒƒå›´
                w = x2 - x1
                h = y2 - y1
                pad_x = int(0.2 * w)
                pad_y = int(0.2 * h)
                x1 = max(0, x1 - pad_x)
                y1 = max(0, y1 - pad_y)
                x2 = min(image.shape[1], x2 + pad_x)
                y2 = min(image.shape[0], y2 + pad_y)

                if x2 > x1 and y2 > y1:
                    roi = image[y1:y2, x1:x2]
                    roi_h, roi_w = roi.shape[:2]

                    # é¢„å¤„ç†ï¼šCLAHEå¢å¼ºäº®åº¦ã€è½»åº¦é”åŒ–
                    def _enhance(img: np.ndarray) -> np.ndarray:
                        try:
                            lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
                            l, a, b = cv2.split(lab)
                            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                            l2 = clahe.apply(l)
                            lab2 = cv2.merge((l2, a, b))
                            enhanced = cv2.cvtColor(lab2, cv2.COLOR_LAB2BGR)
                            # è½»åº¦é”åŒ–
                            kernel = np.array(
                                [[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=np.float32
                            )
                            sharpened = cv2.filter2D(enhanced, -1, kernel)
                            return sharpened
                        except Exception:
                            return img

                    # ä¿è¯æœ€å°ROIè¾¹é•¿ï¼Œå¹¶åšå¤šå°ºåº¦ï¼ˆ1.0 å’Œ 1.25å€ï¼‰
                    min_side_target = 160
                    base_scale = 1.0
                    min_side = max(1, min(roi_w, roi_h))
                    if min_side < min_side_target:
                        base_scale = float(min_side_target) / float(min_side)
                    scales = [base_scale, min(2.0, base_scale * 1.25)]

                    detected_any = False
                    for scale in scales:
                        # ç¼©æ”¾ROI
                        scaled_w = max(1, int(round(roi_w * scale)))
                        scaled_h = max(1, int(round(roi_h * scale)))
                        scaled_roi = cv2.resize(
                            roi, (scaled_w, scaled_h), interpolation=cv2.INTER_CUBIC
                        )
                        scaled_roi = _enhance(scaled_roi)

                        # è°ƒç”¨æ‰‹éƒ¨æ£€æµ‹ï¼ˆåœ¨ç¼©æ”¾ROIä¸Šï¼‰- æ£€æŸ¥æ–¹æ³•æ˜¯å¦å­˜åœ¨
                        roi_hands = []
                        if hasattr(self.pose_detector, "detect_hands"):
                            try:
                                roi_hands = self.pose_detector.detect_hands(scaled_roi)
                            except Exception as e:
                                logger.debug(f"ROIæ‰‹éƒ¨æ£€æµ‹å¤±è´¥: {e}")
                                roi_hands = []
                        else:
                            # YOLOv8PoseDetector æ²¡æœ‰ detect_hands æ–¹æ³•ï¼Œä½¿ç”¨å§¿æ€å…³é”®ç‚¹æå–æ‰‹éƒ¨åŒºåŸŸ
                            roi_hands = self._extract_hand_regions_from_pose(
                                scaled_roi, person_bbox
                            )

                        for hres in roi_hands:
                            # è¯»å–ç¼©æ”¾ROIå†…çš„åƒç´ bboxï¼Œå¹¶æ˜ å°„å›å…¨å›¾
                            bbox = hres.get("bbox", [0, 0, 0, 0])
                            bx1, by1, bx2, by2 = [int(b) for b in bbox]
                            # å…ˆè¿˜åŸåˆ°åŸROIåæ ‡ç³»
                            ox1 = bx1 / scale
                            oy1 = by1 / scale
                            ox2 = bx2 / scale
                            oy2 = by2 / scale
                            gx1, gy1 = int(round(x1 + ox1)), int(round(y1 + oy1))
                            gx2, gy2 = int(round(x1 + ox2)), int(round(y1 + oy2))

                            mapped = {
                                "bbox": [gx1, gy1, gx2, gy2],
                                "confidence": float(hres.get("confidence", 0.0)),
                            }

                            # æ˜ å°„å…³é”®ç‚¹ï¼ˆhres.landmarks ç›¸å¯¹ç¼©æ”¾ROIçš„å½’ä¸€åŒ–åæ ‡ï¼‰
                            if "landmarks" in hres and hres["landmarks"]:
                                mapped_landmarks = []
                                sw, sh = scaled_w, scaled_h
                                for lm in hres["landmarks"]:
                                    px = lm.get("x", 0.0) * sw  # åƒç´ åæ ‡ï¼ˆç¼©æ”¾ROIï¼‰
                                    py = lm.get("y", 0.0) * sh
                                    ox = px / scale  # è¿˜åŸåˆ°åŸROIåƒç´ 
                                    oy = py / scale
                                    mapped_landmarks.append(
                                        {
                                            "x": (x1 + ox) / image.shape[1],
                                            "y": (y1 + oy) / image.shape[0],
                                        }
                                    )
                                mapped["landmarks"] = mapped_landmarks

                            # é€ä¼ æ¥æºä¸æ ‡ç­¾ï¼ˆè‹¥å­˜åœ¨ï¼‰
                            if "class_name" in hres:
                                mapped["class_name"] = hres["class_name"]
                            if "source" in hres:
                                mapped["source"] = hres["source"]

                            # ä»…ä¿ç•™æ‰‹ä¸­å¿ƒåœ¨è¯¥äººä½“æ¡†å†…çš„ç»“æœ
                            cx = (gx1 + gx2) / 2
                            cy = (gy1 + gy2) / 2
                            if x1 <= cx <= x2 and y1 <= cy <= y2:
                                hand_regions.append(mapped)

                if hand_regions:
                    detected_any = True

                    if detected_any:
                        logger.info(
                            f"ROIæ‰‹æ£€æ£€æµ‹åˆ° {len(hand_regions)} ä¸ªæ‰‹éƒ¨åŒºåŸŸ (å¤šå°ºåº¦/å¢å¼º), person_bbox={person_bbox}"
                        )
                        return hand_regions

                # ROIä¸ºç©ºæˆ–æœªæ£€å‡ºæ—¶ï¼Œé€€å›æ•´å¸§æ‰‹æ£€å¹¶è¿‡æ»¤åˆ°è¯¥äººä½“æ¡†
                full_hands = []
                if hasattr(self.pose_detector, "detect_hands"):
                    try:
                        full_hands = self.pose_detector.detect_hands(image)
                    except Exception as e:
                        logger.debug(f"æ•´å¸§æ‰‹éƒ¨æ£€æµ‹å¤±è´¥: {e}")
                        full_hands = []
                else:
                    # YOLOv8PoseDetector æ²¡æœ‰ detect_hands æ–¹æ³•ï¼Œä½¿ç”¨å§¿æ€å…³é”®ç‚¹æå–æ‰‹éƒ¨åŒºåŸŸ
                    full_hands = self._extract_hand_regions_from_pose(
                        image, person_bbox
                    )
                for hres in full_hands:
                    bbox = hres.get("bbox", [0, 0, 0, 0])
                    hx1, hy1, hx2, hy2 = [int(b) for b in bbox]
                    cx = (hx1 + hx2) / 2
                    cy = (hy1 + hy2) / 2
                    if x1 <= cx <= x2 and y1 <= cy <= y2:
                        hand_regions.append(hres)

                if hand_regions:
                    logger.info(
                        f"æ•´å¸§æ‰‹æ£€è¿‡æ»¤åˆ° {len(hand_regions)} ä¸ªæ‰‹éƒ¨åŒºåŸŸ, person_bbox={person_bbox}"
                    )
                    return hand_regions

            except Exception as e:
                logger.info(f"å§¿æ€æ£€æµ‹å™¨æ‰‹éƒ¨æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•: {e}")

        # å›é€€åˆ°ä¼°ç®—æ–¹æ³•
        estimated_regions = self._estimate_hand_regions(person_bbox)
        logger.info(
            f"ä½¿ç”¨ä¼°ç®—çš„æ‰‹éƒ¨åŒºåŸŸ, person_bbox={person_bbox}, ä¼°ç®—æ‰‹éƒ¨æ•°={len(estimated_regions)}"
        )
        return estimated_regions

    def _extract_hand_regions_from_pose(
        self, image: np.ndarray, person_bbox: List[int]
    ) -> List[Dict]:
        """
        ä»å§¿æ€å…³é”®ç‚¹ä¸­æå–æ‰‹éƒ¨åŒºåŸŸï¼ˆé€‚ç”¨äºYOLOv8PoseDetectorï¼‰

        Args:
            image: è¾“å…¥å›¾åƒ
            person_bbox: äººä½“è¾¹ç•Œæ¡† [x1, y1, x2, y2]

        Returns:
            æ‰‹éƒ¨åŒºåŸŸåˆ—è¡¨
        """
        hand_regions = []

        if self.pose_detector is None:
            return hand_regions

        try:
            # ä½¿ç”¨å§¿æ€æ£€æµ‹å™¨æ£€æµ‹äººä½“å…³é”®ç‚¹
            pose_detections = self.pose_detector.detect(image)

            x1, y1, x2, y2 = [int(v) for v in person_bbox]
            cx = (x1 + x2) / 2
            cy = (y1 + y2) / 2

            # æ‰¾åˆ°æœ€æ¥è¿‘çš„äººä½“å§¿æ€æ£€æµ‹ç»“æœ
            best_pose = None
            min_distance = float("inf")

            for pose in pose_detections:
                pose_bbox = pose.get("bbox", [0, 0, 0, 0])
                px1, py1, px2, py2 = pose_bbox
                pcx = (px1 + px2) / 2
                pcy = (py1 + py2) / 2
                distance = ((cx - pcx) ** 2 + (cy - pcy) ** 2) ** 0.5

                if distance < min_distance:
                    min_distance = distance
                    best_pose = pose

            if best_pose and "keypoints" in best_pose:
                keypoints = best_pose["keypoints"]
                if "xy" in keypoints and "conf" in keypoints:
                    kpts_xy = np.array(keypoints["xy"])
                    kpts_conf = np.array(keypoints["conf"])

                    # COCOå§¿æ€å…³é”®ç‚¹ç´¢å¼•ï¼š
                    # 9: å·¦æ‰‹è…• (left_wrist)
                    # 10: å³æ‰‹è…• (right_wrist)
                    # 7: å·¦è‚˜ (left_elbow)
                    # 8: å³è‚˜ (right_elbow)

                    left_wrist_idx = 9
                    right_wrist_idx = 10
                    left_elbow_idx = 7
                    right_elbow_idx = 8

                    # æå–å·¦æ‰‹åŒºåŸŸ
                    if (
                        left_wrist_idx < len(kpts_xy)
                        and left_elbow_idx < len(kpts_xy)
                        and kpts_conf[left_wrist_idx] > 0.3
                        and kpts_conf[left_elbow_idx] > 0.3
                    ):
                        wrist = kpts_xy[left_wrist_idx]
                        elbow = kpts_xy[left_elbow_idx]

                        # ä¼°ç®—æ‰‹éƒ¨åŒºåŸŸï¼ˆä»¥æ‰‹è…•ä¸ºä¸­å¿ƒï¼Œå¤§å°åŸºäºè‚˜éƒ¨åˆ°æ‰‹è…•çš„è·ç¦»ï¼‰
                        hand_size = np.linalg.norm(wrist - elbow) * 0.8
                        hand_w = int(hand_size)
                        hand_h = int(hand_size)

                        hand_x1 = max(0, int(wrist[0] - hand_w / 2))
                        hand_y1 = max(0, int(wrist[1] - hand_h / 2))
                        hand_x2 = min(image.shape[1], int(wrist[0] + hand_w / 2))
                        hand_y2 = min(image.shape[0], int(wrist[1] + hand_h / 2))

                        # æ£€æŸ¥æ‰‹éƒ¨ä¸­å¿ƒæ˜¯å¦åœ¨äººä½“æ¡†å†…
                        if x1 <= wrist[0] <= x2 and y1 <= wrist[1] <= y2:
                            hand_regions.append(
                                {
                                    "bbox": [hand_x1, hand_y1, hand_x2, hand_y2],
                                    "confidence": float(kpts_conf[left_wrist_idx]),
                                    "landmarks": [
                                        {
                                            "x": wrist[0] / image.shape[1],
                                            "y": wrist[1] / image.shape[0],
                                        }
                                    ],
                                    "source": "yolov8_pose_keypoints",
                                    "hand_label": "left",
                                }
                            )

                    # æå–å³æ‰‹åŒºåŸŸ
                    if (
                        right_wrist_idx < len(kpts_xy)
                        and right_elbow_idx < len(kpts_xy)
                        and kpts_conf[right_wrist_idx] > 0.3
                        and kpts_conf[right_elbow_idx] > 0.3
                    ):
                        wrist = kpts_xy[right_wrist_idx]
                        elbow = kpts_xy[right_elbow_idx]

                        # ä¼°ç®—æ‰‹éƒ¨åŒºåŸŸ
                        hand_size = np.linalg.norm(wrist - elbow) * 0.8
                        hand_w = int(hand_size)
                        hand_h = int(hand_size)

                        hand_x1 = max(0, int(wrist[0] - hand_w / 2))
                        hand_y1 = max(0, int(wrist[1] - hand_h / 2))
                        hand_x2 = min(image.shape[1], int(wrist[0] + hand_w / 2))
                        hand_y2 = min(image.shape[0], int(wrist[1] + hand_h / 2))

                        # æ£€æŸ¥æ‰‹éƒ¨ä¸­å¿ƒæ˜¯å¦åœ¨äººä½“æ¡†å†…
                        if x1 <= wrist[0] <= x2 and y1 <= wrist[1] <= y2:
                            hand_regions.append(
                                {
                                    "bbox": [hand_x1, hand_y1, hand_x2, hand_y2],
                                    "confidence": float(kpts_conf[right_wrist_idx]),
                                    "landmarks": [
                                        {
                                            "x": wrist[0] / image.shape[1],
                                            "y": wrist[1] / image.shape[0],
                                        }
                                    ],
                                    "source": "yolov8_pose_keypoints",
                                    "hand_label": "right",
                                }
                            )

                    if hand_regions:
                        logger.info(
                            f"ä»å§¿æ€å…³é”®ç‚¹æå–åˆ° {len(hand_regions)} ä¸ªæ‰‹éƒ¨åŒºåŸŸ, person_bbox={person_bbox}"
                        )

        except Exception as e:
            logger.debug(f"ä»å§¿æ€å…³é”®ç‚¹æå–æ‰‹éƒ¨åŒºåŸŸå¤±è´¥: {e}")

        return hand_regions

    # --- Public helper for external callers (e.g., tracking-driven pipelines) ---
    def get_hand_regions_for_person(
        self, image: np.ndarray, person_bbox: List[int]
    ) -> List[Dict]:
        """å¯¹å¤–å…¬å¼€ï¼šæ ¹æ®äººä½“æ¡†è¿”å›æ‰‹éƒ¨åŒºåŸŸï¼ˆå¯èƒ½åŒ…å«landmarksä¸æ¥æºï¼‰"""
        return self._get_actual_hand_regions(image, person_bbox)

    def _create_annotated_image(
        self,
        image: np.ndarray,
        person_detections: List[Dict],
        hairnet_results: List[Dict],
        handwash_results: List[Dict],
        sanitize_results: List[Dict],
        hand_regions: Optional[List[Dict]] = None,
        min_confidence: float = 0.5,  # å¯è§†åŒ–æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
    ) -> np.ndarray:
        """åˆ›å»ºå¸¦æ³¨é‡Šçš„ç»“æœå›¾åƒ

        Args:
            image: è¾“å…¥å›¾åƒ
            person_detections: äººä½“æ£€æµ‹ç»“æœåˆ—è¡¨
            hairnet_results: å‘ç½‘æ£€æµ‹ç»“æœåˆ—è¡¨
            handwash_results: æ´—æ‰‹æ£€æµ‹ç»“æœåˆ—è¡¨
            sanitize_results: æ¶ˆæ¯’æ£€æµ‹ç»“æœåˆ—è¡¨
            hand_regions: é¢„è®¡ç®—çš„æ‰‹éƒ¨åŒºåŸŸåˆ—è¡¨ï¼ˆé¿å…é‡å¤æ¨ç†ï¼‰
            min_confidence: å¯è§†åŒ–æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.5ï¼Œè¿‡æ»¤ä½ç½®ä¿¡åº¦æ£€æµ‹ï¼‰

        Returns:
            å¸¦æ³¨é‡Šçš„å›¾åƒ
        """
        annotated = image.copy()

        try:
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„äººä½“æ£€æµ‹ï¼ˆåªæ˜¾ç¤ºé«˜ç½®ä¿¡åº¦çš„æ£€æµ‹ï¼‰
            filtered_person_detections = [
                det
                for det in person_detections
                if det.get("confidence", 0.0) >= min_confidence
            ]

            # ç»˜åˆ¶äººä½“æ£€æµ‹æ¡†
            for detection in filtered_person_detections:
                bbox = detection.get("bbox", [0, 0, 0, 0])
                x1, y1, x2, y2 = map(int, bbox)
                confidence = detection.get("confidence", 0.0)
                track_id = detection.get("track_id")

                # ç»˜åˆ¶äººä½“è¾¹ç•Œæ¡†ï¼ˆç»¿è‰²ï¼‰
                cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)

                # ç»˜åˆ¶æ ‡ç­¾
                label = f"Person {confidence:.2f}"
                if track_id is not None:
                    label += f" ID:{track_id}"
                cv2.putText(
                    annotated,
                    label,
                    (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.5,
                    (0, 255, 0),
                    2,
                )

            # å‘ç½‘æ£€æµ‹ä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå› ä¸ºå‘ç½‘æ£€æµ‹æœ¬èº«ç½®ä¿¡åº¦å¯èƒ½è¾ƒä½ï¼‰
            # ä½¿ç”¨å‘ç½‘æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½†ä¸ä½äº0.2ï¼ˆç¡®ä¿èƒ½çœ‹åˆ°æ›´å¤šæ£€æµ‹ç»“æœï¼‰
            hairnet_min_confidence = 0.2  # ä»0.3é™ä½åˆ°0.2ï¼Œæé«˜æ•æ„Ÿåº¦
            if hasattr(self, "params") and self.params is not None:
                hairnet_conf = self.params.hairnet_detection.confidence_threshold
                # ä½¿ç”¨70%çš„å‘ç½‘æ£€æµ‹é˜ˆå€¼ï¼Œä½†ä¸ä½äº0.2
                hairnet_min_confidence = max(0.2, hairnet_conf * 0.7)

            # ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„äººä½“ç»˜åˆ¶å¤´éƒ¨æ¡†ï¼ˆæ— è®ºæ˜¯å¦æœ‰å‘ç½‘æ£€æµ‹ç»“æœï¼‰
            # åˆ›å»ºperson_idåˆ°å‘ç½‘æ£€æµ‹ç»“æœçš„æ˜ å°„
            hairnet_map = {}
            filtered_hairnet_results = [
                result
                for result in hairnet_results
                if result.get("hairnet_confidence", 0.0) >= hairnet_min_confidence
            ]
            for result in filtered_hairnet_results:
                person_id = result.get("person_id")
                if person_id:
                    hairnet_map[person_id] = result

            # ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„äººä½“ç»˜åˆ¶å¤´éƒ¨æ¡†
            for i, detection in enumerate(filtered_person_detections):
                person_bbox = detection.get("bbox", [0, 0, 0, 0])
                if person_bbox == [0, 0, 0, 0]:
                    continue

                x1, y1, x2, y2 = map(int, person_bbox)
                # è®¡ç®—å¤´éƒ¨åŒºåŸŸï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨35%é«˜åº¦ï¼Œä¸YOLOHairnetDetectorä¿æŒä¸€è‡´ï¼‰
                person_height = y2 - y1
                person_width = x2 - x1
                head_height = int(person_height * 0.35)  # ä»30%å¢åŠ åˆ°35%
                padding_height = int(head_height * 0.2)  # 20%padding
                padding_width = int(person_width * 0.1)  # 10%paddingå®½åº¦

                head_y1 = max(0, y1 - padding_height)
                head_y2 = min(image.shape[0], y1 + head_height + padding_height)
                head_x1 = max(0, x1 - padding_width)
                head_x2 = min(image.shape[1], x2 + padding_width)

                # æŸ¥æ‰¾å¯¹åº”çš„å‘ç½‘æ£€æµ‹ç»“æœ
                person_id = i + 1
                hairnet_result = hairnet_map.get(person_id)

                if hairnet_result:
                    # å¦‚æœæœ‰å‘ç½‘æ£€æµ‹ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨æ£€æµ‹ç»“æœä¸­çš„head_bboxï¼ˆæ›´å‡†ç¡®ï¼‰
                    head_bbox = hairnet_result.get(
                        "head_bbox", [head_x1, head_y1, head_x2, head_y2]
                    )
                    if (
                        head_bbox == [0, 0, 0, 0]
                        or (head_bbox[2] - head_bbox[0] <= 0)
                        or (head_bbox[3] - head_bbox[1] <= 0)
                    ):
                        # å¦‚æœhead_bboxæ— æ•ˆï¼Œä½¿ç”¨è®¡ç®—çš„head_bbox
                        head_bbox = [head_x1, head_y1, head_x2, head_y2]
                    else:
                        # ä½¿ç”¨æ£€æµ‹ç»“æœä¸­çš„head_bboxï¼ˆæ¥è‡ªYOLOHairnetDetectorï¼Œæ›´å‡†ç¡®ï¼‰
                        head_x1, head_y1, head_x2, head_y2 = map(int, head_bbox)

                    has_hairnet = hairnet_result.get("has_hairnet", False)
                    confidence = hairnet_result.get("hairnet_confidence", 0.0)
                else:
                    # å¦‚æœæ²¡æœ‰å‘ç½‘æ£€æµ‹ç»“æœï¼Œé»˜è®¤æ˜¾ç¤ºä¸ºæ— å‘ç½‘ï¼ˆçº¢è‰²ï¼‰
                    has_hairnet = False
                    confidence = 0.0

                # ç»¿è‰²=æœ‰å‘ç½‘ï¼Œçº¢è‰²=æ— å‘ç½‘
                color = (0, 255, 0) if has_hairnet else (0, 0, 255)
                # ç»˜åˆ¶å¤´éƒ¨æ¡†ï¼ˆçº¿æ¡ç²—ç»†3åƒç´ ï¼‰
                cv2.rectangle(
                    annotated, (head_x1, head_y1), (head_x2, head_y2), color, 3
                )

                # ç»˜åˆ¶èƒŒæ™¯æ¡†ï¼ˆæé«˜æ ‡ç­¾å¯è¯»æ€§ï¼‰
                label = f"{'æœ‰å‘ç½‘' if has_hairnet else 'æ— å‘ç½‘'}"
                if confidence > 0:
                    label += f" {confidence:.2f}"
                (label_width, label_height), baseline = cv2.getTextSize(
                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
                )
                # ç»˜åˆ¶èƒŒæ™¯
                cv2.rectangle(
                    annotated,
                    (head_x1, head_y1 - label_height - 10),
                    (head_x1 + label_width + 4, head_y1),
                    color,
                    -1,  # å¡«å……
                )

                # ç»˜åˆ¶æ ‡ç­¾ï¼ˆä½¿ç”¨æ›´å¤§çš„å­—ä½“å’Œæ›´ç²—çš„çº¿æ¡ï¼‰
                cv2.putText(
                    annotated,
                    label,
                    (head_x1 + 2, head_y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,  # å­—ä½“å¤§å°
                    (255, 255, 255),  # ç™½è‰²æ–‡å­—ï¼Œæé«˜å¯¹æ¯”åº¦
                    2,
                )

            # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„æ´—æ‰‹æ£€æµ‹ï¼ˆåªæ˜¾ç¤ºé«˜ç½®ä¿¡åº¦çš„æ£€æµ‹ï¼‰
            filtered_handwash_results = [
                result
                for result in handwash_results
                if result.get("is_handwashing", False)
                and result.get("handwash_confidence", 0.0) >= min_confidence
            ]

            # ç»˜åˆ¶æ´—æ‰‹æ£€æµ‹ç»“æœ
            for result in filtered_handwash_results:
                person_bbox = result.get("person_bbox", [0, 0, 0, 0])
                x1, y1, x2, y2 = map(int, person_bbox)
                confidence = result.get("handwash_confidence", 0.0)

                # åœ¨äººä½“æ¡†ä¸Šæ–¹ç»˜åˆ¶æ´—æ‰‹æ ‡ç­¾ï¼ˆé»„è‰²ï¼‰
                label = f"æ´—æ‰‹ä¸­ {confidence:.2f}"
                cv2.putText(
                    annotated,
                    label,
                    (x1, y1 - 30),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (0, 255, 255),
                    2,
                )

            # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„æ¶ˆæ¯’æ£€æµ‹ï¼ˆåªæ˜¾ç¤ºé«˜ç½®ä¿¡åº¦çš„æ£€æµ‹ï¼‰
            filtered_sanitize_results = [
                result
                for result in sanitize_results
                if result.get("is_sanitizing", False)
                and result.get("sanitize_confidence", 0.0) >= min_confidence
            ]

            # ç»˜åˆ¶æ¶ˆæ¯’æ£€æµ‹ç»“æœ
            for result in filtered_sanitize_results:
                person_bbox = result.get("person_bbox", [0, 0, 0, 0])
                x1, y1, x2, y2 = map(int, person_bbox)
                confidence = result.get("sanitize_confidence", 0.0)

                # åœ¨äººä½“æ¡†ä¸Šæ–¹ç»˜åˆ¶æ¶ˆæ¯’æ ‡ç­¾ï¼ˆé’è‰²ï¼‰
                label = f"æ¶ˆæ¯’ä¸­ {confidence:.2f}"
                cv2.putText(
                    annotated,
                    label,
                    (x1, y1 - 50),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.6,
                    (255, 255, 0),
                    2,
                )

            # æ‰‹éƒ¨å¯è§†åŒ–ï¼šæ— è®ºæ˜¯å¦æ£€æµ‹åˆ°äººä½“ï¼Œéƒ½å°è¯•ç»˜åˆ¶æ‰‹éƒ¨ï¼ˆä¾¿äºæ‰‹éƒ¨è¿‘æ™¯è§†é¢‘è°ƒè¯•ï¼‰
            if hand_regions:
                hands_results = hand_regions

                # ç»˜åˆ¶æ‰‹éƒ¨ï¼šä¼˜å…ˆç»˜åˆ¶bboxä¸æ¥æºæ ‡ç­¾ï¼›å¦‚æœ‰å…³é”®ç‚¹åˆ™å†ç»˜åˆ¶éª¨æ¶
                for hand_result in hands_results:
                    bbox = hand_result.get("bbox", [0, 0, 0, 0])
                    if (
                        bbox == [0, 0, 0, 0]
                        or (bbox[2] - bbox[0] <= 0)
                        or (bbox[3] - bbox[1] <= 0)
                    ):
                        continue

                    hx1, hy1, hx2, hy2 = map(int, bbox)
                    label = hand_result.get("class_name", "hand")
                    hand_result.get("source", "auto")
                    confidence = hand_result.get("confidence", 0.0)

                    # ç»˜åˆ¶æ‰‹éƒ¨è¾¹ç•Œæ¡†ï¼ˆé»„è‰²ï¼Œçº¿æ¡ç²—ç»†3åƒç´ ï¼‰
                    cv2.rectangle(annotated, (hx1, hy1), (hx2, hy2), (0, 255, 255), 3)

                    # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
                    hand_label = f"æ‰‹éƒ¨: {label}"
                    if confidence > 0:
                        hand_label += f" {confidence:.2f}"
                    (label_width, label_height), baseline = cv2.getTextSize(
                        hand_label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
                    )
                    # ç»˜åˆ¶èƒŒæ™¯
                    cv2.rectangle(
                        annotated,
                        (hx1, hy1 - label_height - 10),
                        (hx1 + label_width + 4, hy1),
                        (0, 255, 255),  # é»„è‰²èƒŒæ™¯
                        -1,  # å¡«å……
                    )

                    # ç»˜åˆ¶æ ‡ç­¾
                    cv2.putText(
                        annotated,
                        hand_label,
                        (hx1 + 2, hy1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.6,  # å­—ä½“å¤§å°
                        (0, 0, 0),  # é»‘è‰²æ–‡å­—ï¼Œæé«˜å¯¹æ¯”åº¦
                        2,
                    )

                    # è‹¥æœ‰å…³é”®ç‚¹åˆ™ç»˜åˆ¶éª¨æ¶
                    if "landmarks" in hand_result and hand_result["landmarks"]:
                        landmarks = hand_result["landmarks"]
                        h, w = image.shape[:2]
                        for i, landmark in enumerate(landmarks):
                            x = int(landmark["x"] * w)
                            y = int(landmark["y"] * h)
                            cv2.circle(annotated, (x, y), 3, (0, 255, 255), -1)

                        if len(landmarks) >= 21:
                            wrist = (
                                int(landmarks[0]["x"] * w),
                                int(landmarks[0]["y"] * h),
                            )
                            finger_bases = [5, 9, 13, 17]
                            for base_idx in finger_bases:
                                if base_idx < len(landmarks):
                                    base = (
                                        int(landmarks[base_idx]["x"] * w),
                                        int(landmarks[base_idx]["y"] * h),
                                    )
                                    cv2.line(annotated, wrist, base, (0, 255, 255), 1)

                            finger_connections = [
                                [1, 2, 3, 4],
                                [5, 6, 7, 8],
                                [9, 10, 11, 12],
                                [13, 14, 15, 16],
                                [17, 18, 19, 20],
                            ]

                            for finger in finger_connections:
                                for j in range(len(finger) - 1):
                                    if finger[j] < len(landmarks) and finger[
                                        j + 1
                                    ] < len(landmarks):
                                        pt1 = (
                                            int(landmarks[finger[j]]["x"] * w),
                                            int(landmarks[finger[j]]["y"] * h),
                                        )
                                        pt2 = (
                                            int(landmarks[finger[j + 1]]["x"] * w),
                                            int(landmarks[finger[j + 1]]["y"] * h),
                                        )
                                        cv2.line(annotated, pt1, pt2, (0, 255, 255), 1)

            # åœ¨å·¦ä¸Šè§’æ˜¾ç¤ºå¸§ä¿¡æ¯
            # é¡¶å±‚æ¸²æŸ“ä¸­æ–‡ä¿¡æ¯

        except Exception as e:
            logger.error(f"å›¾åƒæ³¨é‡Šå¤±è´¥: {e}")

        return annotated

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç®¡é“ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()

        if self.enable_cache and self.frame_cache is not None:
            cache_stats = self.frame_cache.get_stats()
            stats.update(
                {
                    "cache_stats": cache_stats,
                    "cache_hit_rate": (
                        self.stats["cache_hits"]
                        / max(1, self.stats["cache_hits"] + self.stats["cache_misses"])
                    ),
                }
            )

        return stats

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.enable_cache and self.frame_cache is not None:
            self.frame_cache.clear()

    def update_models(
        self, human_detector=None, hairnet_detector=None, behavior_recognizer=None
    ):
        """æ›´æ–°æ¨¡å‹ï¼ˆçƒ­æ›´æ–°æ”¯æŒï¼‰"""
        if human_detector is not None:
            self.human_detector = human_detector
            logger.info("äººä½“æ£€æµ‹å™¨å·²æ›´æ–°")

        if hairnet_detector is not None:
            self.hairnet_detector = hairnet_detector
            logger.info("å‘ç½‘æ£€æµ‹å™¨å·²æ›´æ–°")

        if behavior_recognizer is not None:
            self.behavior_recognizer = behavior_recognizer
            logger.info("è¡Œä¸ºè¯†åˆ«å™¨å·²æ›´æ–°")

        # æ¸…ç©ºç¼“å­˜ä»¥ç¡®ä¿ä½¿ç”¨æ–°æ¨¡å‹
        self.clear_cache()


class VideoStreamOptimizer:
    """è§†é¢‘æµå¤„ç†ä¼˜åŒ–å™¨ - ä¸“é—¨ç”¨äºè§†é¢‘æµçš„ä¼˜åŒ–å¤„ç†"""

    def __init__(
        self,
        detection_pipeline: OptimizedDetectionPipeline,
        frame_skip: int = 3,  # æ¯3å¸§å¤„ç†ä¸€æ¬¡
        similarity_threshold: float = 0.95,
    ):  # å¸§ç›¸ä¼¼åº¦é˜ˆå€¼
        self.detection_pipeline = detection_pipeline
        self.frame_skip = frame_skip
        self.similarity_threshold = similarity_threshold

        self.frame_count = 0
        self.last_processed_frame = None
        self.last_result = None

        logger.info(f"è§†é¢‘æµä¼˜åŒ–å™¨åˆå§‹åŒ–: è·³å¸§={frame_skip}, ç›¸ä¼¼åº¦é˜ˆå€¼={similarity_threshold}")

    def process_frame(
        self, frame: np.ndarray, force_process: bool = False
    ) -> Optional[DetectionResult]:
        """å¤„ç†è§†é¢‘å¸§ï¼ˆå¸¦ä¼˜åŒ–ï¼‰"""
        self.frame_count += 1

        # è·³å¸§ä¼˜åŒ–
        if not force_process and self.frame_count % self.frame_skip != 0:
            return self.last_result

        # å¸§ç›¸ä¼¼åº¦æ£€æŸ¥
        if not force_process and self.last_processed_frame is not None:
            similarity = self._calculate_frame_similarity(
                frame, self.last_processed_frame
            )
            if similarity > self.similarity_threshold:
                logger.debug(f"å¸§ç›¸ä¼¼åº¦è¿‡é«˜ ({similarity:.3f})ï¼Œè·³è¿‡å¤„ç†")
                return self.last_result

        # æ‰§è¡Œæ£€æµ‹
        result = self.detection_pipeline.detect_comprehensive(frame)

        # æ›´æ–°çŠ¶æ€
        self.last_processed_frame = frame.copy()
        self.last_result = result

        return result

    def _calculate_frame_similarity(
        self, frame1: np.ndarray, frame2: np.ndarray
    ) -> float:
        """è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦"""
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)

            # è®¡ç®—ç»“æ„ç›¸ä¼¼æ€§
            # è¿™é‡Œä½¿ç”¨ç®€å•çš„å‡æ–¹è¯¯å·®ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡
            mse = np.mean((gray1.astype(float) - gray2.astype(float)) ** 2)
            max_mse = 255.0**2
            similarity = 1.0 - (mse / max_mse)

            return float(similarity)
        except Exception as e:
            logger.error(f"è®¡ç®—å¸§ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
