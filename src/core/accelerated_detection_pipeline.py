"""
GPUåŠ é€Ÿæ£€æµ‹æµæ°´çº¿
Accelerated Detection Pipeline

æä¾›GPUä¼˜åŒ–çš„é«˜æ€§èƒ½æ£€æµ‹æµæ°´çº¿ï¼š
1. è‡ªåŠ¨GPUè®¾å¤‡é€‰æ‹©å’Œä¼˜åŒ–
2. æ‰¹å¤„ç†æ¨ç†
3. å¼‚æ­¥å¹¶è¡Œå¤„ç†
4. å†…å­˜ä¼˜åŒ–ç®¡ç†
5. æ€§èƒ½ç›‘æ§
"""

import logging
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from queue import Empty, Queue
from typing import Any, Dict, List, Optional

import numpy as np

from ..utils.gpu_acceleration import get_gpu_manager
from .optimized_detection_pipeline import DetectionResult, OptimizedDetectionPipeline

logger = logging.getLogger(__name__)


@dataclass
class AcceleratedDetectionResult(DetectionResult):
    """åŠ é€Ÿæ£€æµ‹ç»“æœ"""

    device_used: str = "cpu"
    batch_size: int = 1
    gpu_memory_used_mb: float = 0.0
    inference_time_ms: float = 0.0
    total_processing_time_ms: float = 0.0


class BatchProcessor:
    """æ‰¹å¤„ç†å™¨"""

    def __init__(self, max_batch_size: int = 8, max_wait_time: float = 0.016):
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time  # 16ms = 60FPS

        self.batch_queue = Queue()
        self.result_futures = {}
        self.processing = False

        self.stats = {
            "total_batches": 0,
            "total_frames": 0,
            "avg_batch_time": 0.0,
            "avg_frames_per_batch": 0.0,
        }

    def add_frame(self, frame: np.ndarray, frame_id: str) -> threading.Event:
        """æ·»åŠ å¸§åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—"""
        result_event = threading.Event()

        self.batch_queue.put(
            {
                "frame": frame,
                "frame_id": frame_id,
                "timestamp": time.time(),
                "result_event": result_event,
            }
        )

        return result_event

    def start_processing(self, process_batch_func):
        """å¼€å§‹æ‰¹å¤„ç†"""
        self.processing = True
        self.process_batch_func = process_batch_func

        self.process_thread = threading.Thread(target=self._process_loop, daemon=True)
        self.process_thread.start()

    def stop_processing(self):
        """åœæ­¢æ‰¹å¤„ç†"""
        self.processing = False

    def _process_loop(self):
        """æ‰¹å¤„ç†å¾ªç¯"""
        batch = []
        last_process_time = time.time()

        while self.processing:
            try:
                # å°è¯•è·å–å¸§
                try:
                    item = self.batch_queue.get(timeout=0.001)
                    batch.append(item)
                except Empty:
                    pass

                current_time = time.time()
                should_process = len(batch) >= self.max_batch_size or (
                    batch and (current_time - last_process_time) >= self.max_wait_time
                )

                if should_process and batch:
                    self._process_current_batch(batch)
                    batch = []
                    last_process_time = current_time

            except Exception as e:
                logger.error(f"æ‰¹å¤„ç†å¾ªç¯é”™è¯¯: {e}")

        # å¤„ç†å‰©ä½™æ‰¹æ¬¡
        if batch:
            self._process_current_batch(batch)

    def _process_current_batch(self, batch: List[Dict]):
        """å¤„ç†å½“å‰æ‰¹æ¬¡"""
        start_time = time.time()

        try:
            # æå–å¸§æ•°æ®
            frames = [item["frame"] for item in batch]
            frame_ids = [item["frame_id"] for item in batch]

            # æ‰¹é‡å¤„ç†
            results = self.process_batch_func(frames, frame_ids)

            # é€šçŸ¥ç»“æœ
            for i, item in enumerate(batch):
                item["result"] = results[i] if i < len(results) else None
                item["result_event"].set()

            # æ›´æ–°ç»Ÿè®¡
            batch_time = time.time() - start_time
            self.stats["total_batches"] += 1
            self.stats["total_frames"] += len(batch)
            self.stats["avg_batch_time"] = (
                self.stats["avg_batch_time"] * (self.stats["total_batches"] - 1)
                + batch_time
            ) / self.stats["total_batches"]
            self.stats["avg_frames_per_batch"] = (
                self.stats["total_frames"] / self.stats["total_batches"]
            )

            logger.debug(
                f"æ‰¹å¤„ç†å®Œæˆ: {len(batch)}å¸§, {batch_time*1000:.1f}ms, "
                f"{len(batch)/batch_time:.1f} FPS"
            )

        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†å¤±è´¥: {e}")
            # é€šçŸ¥æ‰€æœ‰ç­‰å¾…çš„çº¿ç¨‹
            for item in batch:
                item["result"] = None
                item["result_event"].set()


class AcceleratedDetectionPipeline:
    """GPUåŠ é€Ÿæ£€æµ‹æµæ°´çº¿"""

    def __init__(
        self,
        enable_batch_processing: bool = True,
        enable_async_processing: bool = True,
        max_batch_size: Optional[int] = None,
        enable_performance_monitoring: bool = True,
    ):
        """
        åˆå§‹åŒ–åŠ é€Ÿæ£€æµ‹æµæ°´çº¿

        Args:
            enable_batch_processing: å¯ç”¨æ‰¹å¤„ç†
            enable_async_processing: å¯ç”¨å¼‚æ­¥å¤„ç†
            max_batch_size: æœ€å¤§æ‰¹å¤„ç†å¤§å°
            enable_performance_monitoring: å¯ç”¨æ€§èƒ½ç›‘æ§
        """
        logger.info("ğŸš€ åˆå§‹åŒ–GPUåŠ é€Ÿæ£€æµ‹æµæ°´çº¿...")

        # åˆå§‹åŒ–GPUç®¡ç†å™¨
        self.gpu_manager = get_gpu_manager()
        gpu_info = self.gpu_manager.initialize_gpu_acceleration()

        self.device = gpu_info["device"]
        self.gpu_available = gpu_info["gpu_available"]

        # è·å–ä¼˜åŒ–é…ç½®
        self.config = self.gpu_manager.get_optimized_model_config("yolo")

        # è®¾ç½®æ‰¹å¤„ç†
        self.enable_batch_processing = enable_batch_processing and self.gpu_available
        if self.enable_batch_processing:
            self.max_batch_size = max_batch_size or self.config["batch_size"]
            self.batch_processor = BatchProcessor(
                max_batch_size=self.max_batch_size, max_wait_time=0.016  # 60 FPS
            )
            self.batch_processor.start_processing(self._process_batch)
        else:
            self.max_batch_size = 1

        # è®¾ç½®å¼‚æ­¥å¤„ç†
        self.enable_async_processing = enable_async_processing
        if self.enable_async_processing:
            self.thread_pool = ThreadPoolExecutor(
                max_workers=self.config["num_workers"]
            )

        # åˆå§‹åŒ–åŸºç¡€æµæ°´çº¿
        self.base_pipeline = OptimizedDetectionPipeline()

        # æ€§èƒ½ç›‘æ§
        self.enable_monitoring = enable_performance_monitoring
        self.performance_stats = {
            "total_detections": 0,
            "total_inference_time": 0.0,
            "total_processing_time": 0.0,
            "avg_fps": 0.0,
            "gpu_utilization": [],
            "memory_usage": [],
        }

        # ä¼˜åŒ–æ¨¡å‹
        self._optimize_models()

        logger.info(
            f"âœ… GPUåŠ é€Ÿæµæ°´çº¿åˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}, "
            f"æ‰¹å¤„ç†: {self.enable_batch_processing}, "
            f"æ‰¹å¤§å°: {self.max_batch_size}"
        )

    def _optimize_models(self):
        """ä¼˜åŒ–æ¨¡å‹"""
        try:
            # ä¼˜åŒ–äººä½“æ£€æµ‹å™¨
            if hasattr(self.base_pipeline, "human_detector"):
                if hasattr(self.base_pipeline.human_detector, "model"):
                    self.base_pipeline.human_detector.model = (
                        self.gpu_manager.optimize_model(
                            self.base_pipeline.human_detector.model, "yolo"
                        )
                    )

            # ä¼˜åŒ–å‘ç½‘æ£€æµ‹å™¨
            if hasattr(self.base_pipeline, "hairnet_detector"):
                if hasattr(self.base_pipeline.hairnet_detector, "model"):
                    self.base_pipeline.hairnet_detector.model = (
                        self.gpu_manager.optimize_model(
                            self.base_pipeline.hairnet_detector.model, "yolo"
                        )
                    )

            logger.info("âœ… æ¨¡å‹GPUä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.warning(f"æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")

    def detect_single(self, frame: np.ndarray, **kwargs) -> AcceleratedDetectionResult:
        """å•å¸§æ£€æµ‹"""
        start_time = time.time()

        try:
            # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            gpu_memory_used = self._get_gpu_memory_usage()

            # æ‰§è¡Œæ£€æµ‹
            inference_start = time.time()

            if self.enable_batch_processing:
                # ä½¿ç”¨æ‰¹å¤„ç†ï¼ˆå³ä½¿åªæœ‰ä¸€å¸§ï¼‰
                frame_id = f"single_{int(time.time() * 1000000)}"
                result_event = self.batch_processor.add_frame(frame, frame_id)
                result_event.wait(timeout=1.0)  # ç­‰å¾…ç»“æœ

                # è·å–ç»“æœï¼ˆè¿™é‡Œéœ€è¦ä¸€ä¸ªæœºåˆ¶æ¥å­˜å‚¨å’Œæ£€ç´¢å•å¸§ç»“æœï¼‰
                base_result = self._get_single_frame_result(frame_id)
            else:
                # ç›´æ¥å¤„ç†
                base_result = self.base_pipeline.detect_comprehensive(frame, **kwargs)

            inference_time = time.time() - inference_start
            total_time = time.time() - start_time

            # åˆ›å»ºåŠ é€Ÿæ£€æµ‹ç»“æœ
            result = AcceleratedDetectionResult(
                **base_result.__dict__,
                device_used=self.device,
                batch_size=1,
                gpu_memory_used_mb=gpu_memory_used,
                inference_time_ms=inference_time * 1000,
                total_processing_time_ms=total_time * 1000,
            )

            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self._update_performance_stats(inference_time, total_time)

            return result

        except Exception as e:
            logger.error(f"å•å¸§æ£€æµ‹å¤±è´¥: {e}")
            # è¿”å›ç©ºç»“æœ
            return AcceleratedDetectionResult(
                device_used=self.device, batch_size=1, error=str(e)
            )

    def detect_batch(
        self, frames: List[np.ndarray], **kwargs
    ) -> List[AcceleratedDetectionResult]:
        """æ‰¹é‡æ£€æµ‹"""
        if not self.enable_batch_processing:
            # é€å¸§å¤„ç†
            return [self.detect_single(frame, **kwargs) for frame in frames]

        start_time = time.time()

        try:
            # æ‰¹é‡å¤„ç†
            frame_ids = [
                f"batch_{i}_{int(time.time() * 1000000)}" for i in range(len(frames))
            ]
            base_results = self._process_batch(frames, frame_ids)

            total_time = time.time() - start_time
            gpu_memory_used = self._get_gpu_memory_usage()

            # åˆ›å»ºç»“æœ
            results = []
            for i, base_result in enumerate(base_results):
                result = AcceleratedDetectionResult(
                    **base_result.__dict__,
                    device_used=self.device,
                    batch_size=len(frames),
                    gpu_memory_used_mb=gpu_memory_used,
                    inference_time_ms=(total_time * 1000) / len(frames),
                    total_processing_time_ms=total_time * 1000,
                )
                results.append(result)

            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            self._update_performance_stats(total_time / len(frames), total_time)

            return results

        except Exception as e:
            logger.error(f"æ‰¹é‡æ£€æµ‹å¤±è´¥: {e}")
            return [
                AcceleratedDetectionResult(device_used=self.device, error=str(e))
                for _ in frames
            ]

    def _process_batch(
        self, frames: List[np.ndarray], frame_ids: List[str]
    ) -> List[DetectionResult]:
        """å¤„ç†æ‰¹é‡å¸§"""
        results = []

        try:
            # TODO: å®ç°çœŸæ­£çš„æ‰¹é‡æ¨ç†
            # å½“å‰é€å¸§å¤„ç†ï¼Œåç»­å¯ä»¥ä¼˜åŒ–ä¸ºçœŸæ­£çš„æ‰¹é‡æ¨ç†
            for frame in frames:
                result = self.base_pipeline.detect_comprehensive(frame)
                results.append(result)

        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            # è¿”å›ç©ºç»“æœ
            for _ in frames:
                results.append(DetectionResult())

        return results

    def _get_single_frame_result(self, frame_id: str) -> DetectionResult:
        """è·å–å•å¸§ç»“æœï¼ˆä¸´æ—¶å®ç°ï¼‰"""
        # TODO: å®ç°ç»“æœç¼“å­˜å’Œæ£€ç´¢æœºåˆ¶
        return DetectionResult()

    def _get_gpu_memory_usage(self) -> float:
        """è·å–GPUå†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        if not self.gpu_available:
            return 0.0

        try:
            import torch

            if torch.cuda.is_available():
                return torch.cuda.memory_allocated(0) / (1024 * 1024)
        except:
            pass

        return 0.0

    def _update_performance_stats(self, inference_time: float, total_time: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        if not self.enable_monitoring:
            return

        self.performance_stats["total_detections"] += 1
        self.performance_stats["total_inference_time"] += inference_time
        self.performance_stats["total_processing_time"] += total_time

        # è®¡ç®—å¹³å‡FPS
        if self.performance_stats["total_processing_time"] > 0:
            self.performance_stats["avg_fps"] = (
                self.performance_stats["total_detections"]
                / self.performance_stats["total_processing_time"]
            )

        # è®°å½•GPUåˆ©ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import torch

            if torch.cuda.is_available():
                utilization = torch.cuda.utilization(0)
                self.performance_stats["gpu_utilization"].append(utilization)

                memory_info = torch.cuda.mem_get_info(0)
                memory_used_gb = (memory_info[1] - memory_info[0]) / (1024**3)
                self.performance_stats["memory_usage"].append(memory_used_gb)
        except:
            pass

    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        stats = self.performance_stats.copy()

        # è®¡ç®—å¹³å‡å€¼
        if stats["gpu_utilization"]:
            stats["avg_gpu_utilization"] = np.mean(stats["gpu_utilization"])
            stats["max_gpu_utilization"] = np.max(stats["gpu_utilization"])

        if stats["memory_usage"]:
            stats["avg_memory_usage_gb"] = np.mean(stats["memory_usage"])
            stats["max_memory_usage_gb"] = np.max(stats["memory_usage"])

        # æ·»åŠ é…ç½®ä¿¡æ¯
        stats["configuration"] = {
            "device": self.device,
            "batch_processing_enabled": self.enable_batch_processing,
            "max_batch_size": self.max_batch_size,
            "async_processing_enabled": self.enable_async_processing,
            "gpu_available": self.gpu_available,
        }

        return stats

    def optimize_for_video_stream(self, target_fps: int = 30) -> Dict[str, Any]:
        """ä¸ºè§†é¢‘æµä¼˜åŒ–"""
        logger.info(f"ğŸ¥ ä¼˜åŒ–è§†é¢‘æµå¤„ç† - ç›®æ ‡FPS: {target_fps}")

        # è®¡ç®—æœ€ä½³è®¾ç½®
        1.0 / target_fps

        optimization_config = {
            "frame_skip": max(1, int(30 / target_fps)),  # è·³å¸§ç­–ç•¥
            "batch_size": min(self.max_batch_size, max(1, target_fps // 10)),
            "quality_level": "balanced",  # å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
            "enable_caching": True,
            "cache_size": target_fps * 2,  # ç¼“å­˜2ç§’
        }

        logger.info(f"è§†é¢‘æµä¼˜åŒ–é…ç½®: {optimization_config}")
        return optimization_config

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†GPUåŠ é€Ÿæµæ°´çº¿...")

        try:
            # åœæ­¢æ‰¹å¤„ç†
            if hasattr(self, "batch_processor"):
                self.batch_processor.stop_processing()

            # å…³é—­çº¿ç¨‹æ± 
            if hasattr(self, "thread_pool"):
                self.thread_pool.shutdown(wait=True)

            # æ¸…ç†GPUç¼“å­˜
            if self.gpu_available:
                try:
                    import torch

                    torch.cuda.empty_cache()
                except:
                    pass

            logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f"èµ„æºæ¸…ç†å¤±è´¥: {e}")


def create_accelerated_pipeline(**kwargs) -> AcceleratedDetectionPipeline:
    """åˆ›å»ºåŠ é€Ÿæ£€æµ‹æµæ°´çº¿ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    return AcceleratedDetectionPipeline(**kwargs)
