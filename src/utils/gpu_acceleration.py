"""
GPUåŠ é€Ÿä¼˜åŒ–æ¨¡å—
GPU Acceleration Optimization Module

æä¾›è·¨å¹³å°GPUåŠ é€Ÿä¼˜åŒ–ï¼Œè‡ªåŠ¨æ£€æµ‹å’Œé…ç½®æœ€ä½³æ€§èƒ½è®¾ç½®
"""

import logging
import os
import sys
from typing import Any, Dict

logger = logging.getLogger(__name__)


class GPUAccelerationManager:
    """GPUåŠ é€Ÿç®¡ç†å™¨"""

    def __init__(self):
        self.device = "cpu"
        self.gpu_info = {}
        self.optimization_applied = False
        self.performance_config = {}

    def initialize_gpu_acceleration(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–GPUåŠ é€Ÿ"""
        logger.info("ğŸš€ åˆå§‹åŒ–GPUåŠ é€Ÿä¼˜åŒ–...")

        results = {
            "platform": sys.platform,
            "device": "cpu",
            "gpu_available": False,
            "optimizations_applied": [],
            "warnings": [],
            "performance_config": {},
        }

        try:
            pass

            # 1. æ£€æµ‹å¯ç”¨è®¾å¤‡
            device_info = self._detect_best_device()
            results.update(device_info)

            # 2. åº”ç”¨å¹³å°ç‰¹å®šä¼˜åŒ–
            if device_info["device"] != "cpu":
                platform_opts = self._apply_platform_optimizations(device_info)
                results["optimizations_applied"].extend(platform_opts)

            # 3. é…ç½®PyTorchä¼˜åŒ–
            torch_opts = self._configure_torch_optimizations(device_info)
            results["optimizations_applied"].extend(torch_opts)

            # 4. ç”Ÿæˆæ€§èƒ½é…ç½®
            perf_config = self._generate_performance_config(device_info)
            results["performance_config"] = perf_config

            self.device = device_info["device"]
            self.gpu_info = device_info
            self.optimization_applied = True
            self.performance_config = perf_config

            logger.info(f"âœ… GPUåŠ é€Ÿåˆå§‹åŒ–å®Œæˆ - è®¾å¤‡: {self.device}")

        except ImportError:
            results["warnings"].append("PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨CPUæ¨¡å¼")
            logger.warning("PyTorchæœªå®‰è£…ï¼Œæ— æ³•å¯ç”¨GPUåŠ é€Ÿ")
        except Exception as e:
            results["warnings"].append(f"GPUåŠ é€Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(f"GPUåŠ é€Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

        return results

    def _detect_best_device(self) -> Dict[str, Any]:
        """æ£€æµ‹æœ€ä½³è®¡ç®—è®¾å¤‡"""
        import torch

        device_info = {
            "device": "cpu",
            "gpu_available": False,
            "gpu_count": 0,
            "gpu_memory_gb": 0,
            "gpu_name": None,
            "compute_capability": None,
            "backend": "cpu",
        }

        # 1. CUDAæ£€æµ‹ (NVIDIA GPU)
        if torch.cuda.is_available():
            device_info.update(
                {
                    "device": "cuda",
                    "gpu_available": True,
                    "gpu_count": torch.cuda.device_count(),
                    "gpu_name": torch.cuda.get_device_name(0),
                    "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory
                    / (1024**3),
                    "compute_capability": torch.cuda.get_device_capability(0),
                    "backend": "cuda",
                }
            )
            logger.info(
                f"âœ… CUDA GPUå¯ç”¨: {device_info['gpu_name']} ({device_info['gpu_memory_gb']:.1f}GB)"
            )

        # 2. MPSæ£€æµ‹ (Apple Silicon)
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            device_info.update(
                {"device": "mps", "gpu_available": True, "backend": "mps"}
            )
            logger.info("âœ… Apple MPSå¯ç”¨")

        # 3. CPUå›é€€
        else:
            logger.info("ä½¿ç”¨CPUè®¡ç®—")

        return device_info

    def _apply_platform_optimizations(self, device_info: Dict[str, Any]) -> list:
        """åº”ç”¨å¹³å°ç‰¹å®šä¼˜åŒ–"""
        optimizations = []

        if device_info["backend"] == "cuda":
            # CUDAä¼˜åŒ–
            cuda_opts = self._apply_cuda_optimizations(device_info)
            optimizations.extend(cuda_opts)

        elif device_info["backend"] == "mps":
            # MPSä¼˜åŒ–
            mps_opts = self._apply_mps_optimizations()
            optimizations.extend(mps_opts)

        return optimizations

    def _apply_cuda_optimizations(self, device_info: Dict[str, Any]) -> list:
        """åº”ç”¨CUDAä¼˜åŒ–è®¾ç½®"""
        optimizations = []

        try:
            import torch

            # 1. ç¯å¢ƒå˜é‡ä¼˜åŒ–
            cuda_env = {
                "CUDA_LAUNCH_BLOCKING": "0",  # å¼‚æ­¥æ‰§è¡Œ
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:512,roundup_power2_divisions:16",
                "CUBLAS_WORKSPACE_CONFIG": ":16:8",
                "CUDA_MODULE_LOADING": "LAZY",
                "TORCH_CUDNN_V8_API_ENABLED": "1",
            }

            for key, value in cuda_env.items():
                if key not in os.environ:
                    os.environ[key] = value
                    optimizations.append(f"è®¾ç½®ç¯å¢ƒå˜é‡ {key}={value}")

            # 2. CuDNNä¼˜åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            optimizations.append("å¯ç”¨CuDNNåŸºå‡†æµ‹è¯•ä¼˜åŒ–")

            # 3. TF32ä¼˜åŒ– (Ampereæ¶æ„)
            if hasattr(torch.backends.cuda.matmul, "allow_tf32"):
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                optimizations.append("å¯ç”¨TF32ç²¾åº¦ä¼˜åŒ–")

            # 4. å†…å­˜ç®¡ç†
            torch.cuda.empty_cache()
            optimizations.append("æ¸…ç†GPUå†…å­˜ç¼“å­˜")

            # 5. å¤šGPUä¼˜åŒ–
            if device_info["gpu_count"] > 1:
                optimizations.append(f"æ£€æµ‹åˆ°{device_info['gpu_count']}ä¸ªGPUï¼Œå¯å¯ç”¨å¤šGPUå¹¶è¡Œ")

        except Exception as e:
            logger.warning(f"CUDAä¼˜åŒ–åº”ç”¨å¤±è´¥: {e}")

        return optimizations

    def _apply_mps_optimizations(self) -> list:
        """åº”ç”¨MPSä¼˜åŒ–è®¾ç½®"""
        optimizations = []

        try:
            # MPSç‰¹å®šä¼˜åŒ–
            os.environ.setdefault("PYTORCH_ENABLE_MPS_FALLBACK", "1")
            optimizations.append("å¯ç”¨MPS fallbackæœºåˆ¶")

        except Exception as e:
            logger.warning(f"MPSä¼˜åŒ–åº”ç”¨å¤±è´¥: {e}")

        return optimizations

    def _configure_torch_optimizations(self, device_info: Dict[str, Any]) -> list:
        """é…ç½®PyTorchä¼˜åŒ–"""
        optimizations = []

        try:
            import torch

            # 1. çº¿ç¨‹ä¼˜åŒ–
            if device_info["backend"] == "cpu":
                num_threads = min(os.cpu_count() or 4, 8)
                torch.set_num_threads(num_threads)
                os.environ.setdefault("OMP_NUM_THREADS", str(num_threads))
                os.environ.setdefault("MKL_NUM_THREADS", str(num_threads))
                optimizations.append(f"è®¾ç½®CPUçº¿ç¨‹æ•°: {num_threads}")

            # 2. ç¼–è¯‘ä¼˜åŒ– (PyTorch 2.0+)
            if hasattr(torch, "compile"):
                optimizations.append("PyTorch 2.0ç¼–è¯‘ä¼˜åŒ–å¯ç”¨")

            # 3. JITä¼˜åŒ–
            if hasattr(torch.jit, "set_num_threads"):
                torch.jit.set_num_threads(os.cpu_count() or 4)
                optimizations.append("é…ç½®JITçº¿ç¨‹æ•°")

        except Exception as e:
            logger.warning(f"PyTorchä¼˜åŒ–é…ç½®å¤±è´¥: {e}")

        return optimizations

    def _generate_performance_config(
        self, device_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½é…ç½®"""
        config = {
            "device": device_info["device"],
            "mixed_precision": device_info["backend"] in ["cuda", "mps"],
            "compile_model": hasattr(__import__("torch"), "compile"),
            "batch_size": self._calculate_optimal_batch_size(device_info),
            "num_workers": self._calculate_optimal_workers(device_info),
            "pin_memory": device_info["gpu_available"],
            "non_blocking": device_info["gpu_available"],
        }

        # CUDAç‰¹å®šé…ç½®
        if device_info["backend"] == "cuda":
            config.update(
                {
                    "cudnn_benchmark": True,
                    "allow_tf32": True,
                    "channels_last": True,  # ä½¿ç”¨channels_lastå†…å­˜æ ¼å¼
                    "gradient_checkpointing": device_info["gpu_memory_gb"] < 8,  # å°æ˜¾å­˜å¯ç”¨
                }
            )

        return config

    def _calculate_optimal_batch_size(self, device_info: Dict[str, Any]) -> int:
        """è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°"""
        if device_info["backend"] == "cuda":
            memory_gb = device_info.get("gpu_memory_gb", 4)
            if memory_gb >= 24:
                return 32
            elif memory_gb >= 16:
                return 24
            elif memory_gb >= 12:
                return 16
            elif memory_gb >= 8:
                return 12
            elif memory_gb >= 6:
                return 8
            else:
                return 4
        elif device_info["backend"] == "mps":
            return 8  # MPSä¿å®ˆè®¾ç½®
        else:
            return min(os.cpu_count() or 4, 8)

    def _calculate_optimal_workers(self, device_info: Dict[str, Any]) -> int:
        """è®¡ç®—æœ€ä¼˜å·¥ä½œçº¿ç¨‹æ•°"""
        if device_info["gpu_available"]:
            return min(device_info.get("gpu_count", 1) * 2, 8)
        else:
            return min(os.cpu_count() or 4, 8)

    def get_optimized_model_config(self, model_type: str = "yolo") -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–çš„æ¨¡å‹é…ç½®"""
        if not self.optimization_applied:
            self.initialize_gpu_acceleration()

        base_config = self.performance_config.copy()

        # æ¨¡å‹ç‰¹å®šä¼˜åŒ–
        if model_type.lower() == "yolo":
            base_config.update(
                {
                    "imgsz": 640,
                    "conf": 0.4,
                    "iou": 0.6,
                    "half": self.device in ["cuda", "mps"],  # åŠç²¾åº¦æ¨ç†
                    "dnn": True,  # OpenCV DNNåç«¯
                    "augment": False,  # æ¨ç†æ—¶ä¸å¯ç”¨æ•°æ®å¢å¼º
                    "agnostic_nms": False,  # ç±»åˆ«ç‰¹å®šçš„NMS
                    "retina_masks": True,  # é«˜è´¨é‡mask
                }
            )
        elif model_type.lower() == "mediapipe":
            base_config.update(
                {
                    "model_complexity": 1,
                    "min_detection_confidence": 0.5,
                    "min_tracking_confidence": 0.5,
                    "max_num_hands": 2,
                    "static_image_mode": False,
                }
            )

        return base_config

    def optimize_model(self, model, model_type: str = "pytorch"):
        """ä¼˜åŒ–æ¨¡å‹"""
        if not self.optimization_applied:
            self.initialize_gpu_acceleration()

        try:
            import torch

            # 1. ç§»åŠ¨åˆ°æœ€ä½³è®¾å¤‡
            if hasattr(model, "to"):
                model = model.to(self.device)

            # 2. è®¾ç½®è¯„ä¼°æ¨¡å¼
            if hasattr(model, "eval"):
                model.eval()

            # 3. åŠç²¾åº¦ä¼˜åŒ–
            if self.device in ["cuda", "mps"] and hasattr(model, "half"):
                model = model.half()
                logger.info("å¯ç”¨åŠç²¾åº¦æ¨ç†")

            # 4. ç¼–è¯‘ä¼˜åŒ– (PyTorch 2.0+)
            if (
                hasattr(torch, "compile")
                and self.performance_config.get("compile_model", False)
                and model_type == "pytorch"
            ):
                try:
                    model = torch.compile(
                        model, mode="reduce-overhead", fullgraph=False, dynamic=True
                    )
                    logger.info("å¯ç”¨PyTorch 2.0ç¼–è¯‘ä¼˜åŒ–")
                except Exception as e:
                    logger.warning(f"æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")

            return model

        except Exception as e:
            logger.warning(f"æ¨¡å‹ä¼˜åŒ–å¤±è´¥: {e}")
            return model

    def create_optimized_dataloader(self, dataset, **kwargs):
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        if not self.optimization_applied:
            self.initialize_gpu_acceleration()

        try:
            import torch.utils.data as data

            # ä½¿ç”¨æ€§èƒ½é…ç½®
            dataloader_config = {
                "batch_size": kwargs.get(
                    "batch_size", self.performance_config["batch_size"]
                ),
                "num_workers": kwargs.get(
                    "num_workers", self.performance_config["num_workers"]
                ),
                "pin_memory": kwargs.get(
                    "pin_memory", self.performance_config["pin_memory"]
                ),
                "persistent_workers": kwargs.get("persistent_workers", True),
                "prefetch_factor": kwargs.get("prefetch_factor", 2),
            }

            # åˆå¹¶ç”¨æˆ·é…ç½®
            dataloader_config.update(kwargs)

            return data.DataLoader(dataset, **dataloader_config)

        except ImportError:
            logger.warning("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨")
            return None


# å…¨å±€å•ä¾‹
_gpu_manager = None


def get_gpu_manager() -> GPUAccelerationManager:
    """è·å–GPUç®¡ç†å™¨å•ä¾‹"""
    global _gpu_manager
    if _gpu_manager is None:
        _gpu_manager = GPUAccelerationManager()
    return _gpu_manager


def initialize_gpu_acceleration() -> Dict[str, Any]:
    """åˆå§‹åŒ–GPUåŠ é€Ÿï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    manager = get_gpu_manager()
    return manager.initialize_gpu_acceleration()


def get_optimized_device() -> str:
    """è·å–ä¼˜åŒ–çš„è®¾å¤‡ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    manager = get_gpu_manager()
    if not manager.optimization_applied:
        manager.initialize_gpu_acceleration()
    return manager.device


def optimize_model_for_inference(model, model_type: str = "pytorch"):
    """ä¼˜åŒ–æ¨¡å‹ç”¨äºæ¨ç†ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    manager = get_gpu_manager()
    return manager.optimize_model(model, model_type)


def get_optimal_batch_size() -> int:
    """è·å–æœ€ä¼˜æ‰¹å¤„ç†å¤§å°ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    manager = get_gpu_manager()
    if not manager.optimization_applied:
        manager.initialize_gpu_acceleration()
    return manager.performance_config.get("batch_size", 4)


# è‡ªåŠ¨åˆå§‹åŒ–ï¼ˆåœ¨æ¨¡å—å¯¼å…¥æ—¶ï¼‰
if __name__ != "__main__":
    try:
        # åœ¨éæµ‹è¯•ç¯å¢ƒä¸‹è‡ªåŠ¨åˆå§‹åŒ–
        if "pytest" not in sys.modules:
            initialize_gpu_acceleration()
    except Exception:
        pass  # é™é»˜å¤±è´¥ï¼Œé¿å…å½±å“å¯¼å…¥
