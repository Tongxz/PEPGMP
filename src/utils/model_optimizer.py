#!/usr/bin/env python
"""
æ¨¡å‹ä¼˜åŒ–å™¨

è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢æ¨¡å‹ä¸ºTensorRTå¼•æ“ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


class ModelOptimizer:
    """æ¨¡å‹ä¼˜åŒ–å™¨

    è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨TensorRTå¼•æ“åˆ™è‡ªåŠ¨è½¬æ¢ã€‚
    """

    def __init__(
        self,
        models_dir: str = "models",
        auto_convert: bool = True,
        tensorrt_precision: str = "fp16",
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¼˜åŒ–å™¨

        Args:
            models_dir: æ¨¡å‹ç›®å½•
            auto_convert: æ˜¯å¦è‡ªåŠ¨è½¬æ¢
            tensorrt_precision: TensorRTç²¾åº¦ ('fp32', 'fp16', 'int8')
        """
        self.models_dir = Path(models_dir)
        self.auto_convert = auto_convert
        self.tensorrt_precision = tensorrt_precision
        self.converted_models = []

        logger.info("æ¨¡å‹ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  æ¨¡å‹ç›®å½•: {self.models_dir}")
        logger.info(f"  è‡ªåŠ¨è½¬æ¢: {self.auto_convert}")
        logger.info(f"  TensorRTç²¾åº¦: {self.tensorrt_precision}")

    def check_tensorrt_available(self) -> bool:
        """æ£€æŸ¥TensorRTæ˜¯å¦å¯ç”¨"""
        try:
            import tensorrt as trt
            import torch

            # æ£€æŸ¥CUDA
            if not torch.cuda.is_available():
                logger.warning("CUDAä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨TensorRT")
                return False

            logger.info(f"âœ… TensorRTå¯ç”¨ï¼Œç‰ˆæœ¬: {trt.__version__}")
            logger.info(f"   CUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}")
            return True

        except ImportError:
            logger.warning("TensorRTæœªå®‰è£…ï¼Œå°†ä½¿ç”¨PyTorchæ¨¡å‹")
            return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥TensorRTå¤±è´¥: {e}")
            return False

    def find_model_files(self) -> List[Tuple[Path, Path]]:
        """
        æŸ¥æ‰¾æ‰€æœ‰éœ€è¦è½¬æ¢çš„æ¨¡å‹æ–‡ä»¶

        Returns:
            [(pt_file, engine_file), ...] åˆ—è¡¨
        """
        model_pairs = []

        # æŸ¥æ‰¾æ‰€æœ‰.ptæ–‡ä»¶
        for pt_file in self.models_dir.rglob("*.pt"):
            # è·³è¿‡æŸäº›ç›®å½•
            if "training" in pt_file.parts or "weights" in pt_file.parts:
                continue

            # ç”Ÿæˆå¯¹åº”çš„.engineæ–‡ä»¶è·¯å¾„
            engine_file = pt_file.with_suffix(".engine")

            model_pairs.append((pt_file, engine_file))

        return model_pairs

    def needs_conversion(self, pt_file: Path, engine_file: Path) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢

        Args:
            pt_file: PyTorchæ¨¡å‹æ–‡ä»¶
            engine_file: TensorRTå¼•æ“æ–‡ä»¶

        Returns:
            æ˜¯å¦éœ€è¦è½¬æ¢
        """
        # å¦‚æœ.engineæ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦è½¬æ¢
        if not engine_file.exists():
            logger.info(f"ğŸ“‹ éœ€è¦è½¬æ¢: {pt_file.name} (å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨)")
            return True

        # å¦‚æœ.ptæ–‡ä»¶æ¯”.engineæ–‡ä»¶æ–°ï¼Œéœ€è¦è½¬æ¢
        if pt_file.stat().st_mtime > engine_file.stat().st_mtime:
            logger.info(f"ğŸ“‹ éœ€è¦è½¬æ¢: {pt_file.name} (PyTorchæ¨¡å‹å·²æ›´æ–°)")
            return True

        logger.info(f"âœ… å·²å­˜åœ¨: {engine_file.name}")
        return False

    def convert_model(self, pt_file: Path, engine_file: Path, imgsz: int = 640) -> bool:
        """
        è½¬æ¢å•ä¸ªæ¨¡å‹ä¸ºTensorRTå¼•æ“

        Args:
            pt_file: PyTorchæ¨¡å‹æ–‡ä»¶
            engine_file: è¾“å‡ºTensorRTå¼•æ“æ–‡ä»¶
            imgsz: è¾“å…¥å›¾åƒå¤§å°

        Returns:
            è½¬æ¢æ˜¯å¦æˆåŠŸ
        """
        try:
            from ultralytics import YOLO

            logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢: {pt_file.name}")
            logger.info(f"   è¾“å…¥: {pt_file}")
            logger.info(f"   è¾“å‡º: {engine_file}")
            logger.info(f"   ç²¾åº¦: {self.tensorrt_precision}")

            # åŠ è½½æ¨¡å‹
            model = YOLO(str(pt_file))

            # è®¾ç½®ç²¾åº¦
            half = self.tensorrt_precision == "fp16"

            # å¯¼å‡ºä¸ºTensorRT
            model.export(
                format="engine",
                device=0,
                imgsz=imgsz,
                half=half,
                workspace=4,
                simplify=True,
                opset=12,
                dynamic=False,
                verbose=False,
            )

            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            if engine_file.exists():
                size_mb = engine_file.stat().st_size / (1024 * 1024)
                logger.info(f"âœ… è½¬æ¢æˆåŠŸ: {engine_file.name}")
                logger.info(f"   æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
                return True
            else:
                logger.error("âŒ è½¬æ¢å¤±è´¥: è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                return False

        except Exception as e:
            logger.error(f"âŒ è½¬æ¢å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return False

    def optimize_all_models(self) -> Dict[str, bool]:
        """
        ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹

        Returns:
            è½¬æ¢ç»“æœå­—å…¸ {æ¨¡å‹å: æ˜¯å¦æˆåŠŸ}
        """
        results = {}

        # æ£€æŸ¥TensorRTæ˜¯å¦å¯ç”¨
        if not self.check_tensorrt_available():
            logger.warning("TensorRTä¸å¯ç”¨ï¼Œè·³è¿‡æ¨¡å‹ä¼˜åŒ–")
            return results

        # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
        model_pairs = self.find_model_files()

        if not model_pairs:
            logger.info("æœªæ‰¾åˆ°éœ€è¦è½¬æ¢çš„æ¨¡å‹æ–‡ä»¶")
            return results

        logger.info(f"\n{'='*60}")
        logger.info(f"æ‰¾åˆ° {len(model_pairs)} ä¸ªæ¨¡å‹æ–‡ä»¶")
        logger.info(f"{'='*60}")

        # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹
        needs_conversion = []
        for pt_file, engine_file in model_pairs:
            if self.needs_conversion(pt_file, engine_file):
                needs_conversion.append((pt_file, engine_file))

        if not needs_conversion:
            logger.info("âœ… æ‰€æœ‰æ¨¡å‹å·²æ˜¯æœ€æ–°çŠ¶æ€ï¼Œæ— éœ€è½¬æ¢")
            return results

        logger.info(f"\néœ€è¦è½¬æ¢ {len(needs_conversion)} ä¸ªæ¨¡å‹")

        # è½¬æ¢æ¨¡å‹
        for i, (pt_file, engine_file) in enumerate(needs_conversion, 1):
            logger.info(f"\n[{i}/{len(needs_conversion)}] è½¬æ¢æ¨¡å‹: {pt_file.name}")

            success = self.convert_model(pt_file, engine_file)
            results[pt_file.name] = success

            if success:
                self.converted_models.append(str(engine_file))

        # æ‰“å°æ‘˜è¦
        self._print_summary(results)

        return results

    def _print_summary(self, results: Dict[str, bool]):
        """æ‰“å°è½¬æ¢ç»“æœæ‘˜è¦"""
        if not results:
            return

        logger.info(f"\n{'='*60}")
        logger.info("æ¨¡å‹ä¼˜åŒ–æ‘˜è¦")
        logger.info(f"{'='*60}")

        success_count = sum(1 for success in results.values() if success)
        total_count = len(results)

        for name, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            logger.info(f"{name}: {status}")

        logger.info(f"\næ€»è®¡: {success_count}/{total_count} ä¸ªæ¨¡å‹è½¬æ¢æˆåŠŸ")

        if success_count == total_count:
            logger.info("ğŸ‰ æ‰€æœ‰æ¨¡å‹ä¼˜åŒ–å®Œæˆï¼")
        else:
            logger.warning(f"âš ï¸  {total_count - success_count} ä¸ªæ¨¡å‹è½¬æ¢å¤±è´¥")

    def get_model_path(
        self, model_name: str, prefer_tensorrt: bool = True
    ) -> Optional[str]:
        """
        è·å–æ¨¡å‹è·¯å¾„ï¼ˆä¼˜å…ˆTensorRTå¼•æ“ï¼‰

        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ 'yolov8n'ï¼‰
            prefer_tensorrt: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨TensorRTå¼•æ“

        Returns:
            æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        # å°è¯•æŸ¥æ‰¾.ptæ–‡ä»¶
        pt_file = self.models_dir / model_name
        if not pt_file.suffix:
            pt_file = pt_file.with_suffix(".pt")

        # å°è¯•æŸ¥æ‰¾.engineæ–‡ä»¶
        engine_file = pt_file.with_suffix(".engine")

        # ä¼˜å…ˆä½¿ç”¨TensorRTå¼•æ“
        if prefer_tensorrt and engine_file.exists():
            logger.info(f"ä½¿ç”¨TensorRTå¼•æ“: {engine_file}")
            return str(engine_file)

        # å›é€€åˆ°PyTorchæ¨¡å‹
        if pt_file.exists():
            logger.info(f"ä½¿ç”¨PyTorchæ¨¡å‹: {pt_file}")
            return str(pt_file)

        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_name}")
        return None


# å…¨å±€æ¨¡å‹ä¼˜åŒ–å™¨å®ä¾‹
_global_optimizer: Optional[ModelOptimizer] = None


def initialize_model_optimizer(
    models_dir: str = "models",
    auto_convert: bool = True,
    tensorrt_precision: str = "fp16",
) -> ModelOptimizer:
    """
    åˆå§‹åŒ–å…¨å±€æ¨¡å‹ä¼˜åŒ–å™¨

    Args:
        models_dir: æ¨¡å‹ç›®å½•
        auto_convert: æ˜¯å¦è‡ªåŠ¨è½¬æ¢
        tensorrt_precision: TensorRTç²¾åº¦

    Returns:
        æ¨¡å‹ä¼˜åŒ–å™¨å®ä¾‹
    """
    global _global_optimizer

    _global_optimizer = ModelOptimizer(
        models_dir=models_dir,
        auto_convert=auto_convert,
        tensorrt_precision=tensorrt_precision,
    )

    # å¦‚æœå¯ç”¨è‡ªåŠ¨è½¬æ¢ï¼Œç«‹å³ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹
    if auto_convert:
        _global_optimizer.optimize_all_models()

    return _global_optimizer


def get_model_optimizer() -> Optional[ModelOptimizer]:
    """è·å–å…¨å±€æ¨¡å‹ä¼˜åŒ–å™¨å®ä¾‹"""
    return _global_optimizer


def optimize_models_on_startup(
    models_dir: str = "models", tensorrt_precision: str = "fp16"
):
    """
    å¯åŠ¨æ—¶è‡ªåŠ¨ä¼˜åŒ–æ¨¡å‹ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        models_dir: æ¨¡å‹ç›®å½•
        tensorrt_precision: TensorRTç²¾åº¦
    """
    logger.info("=" * 60)
    logger.info("æ¨¡å‹ä¼˜åŒ–å™¨å¯åŠ¨")
    logger.info("=" * 60)

    optimizer = initialize_model_optimizer(
        models_dir=models_dir, auto_convert=True, tensorrt_precision=tensorrt_precision
    )

    logger.info("æ¨¡å‹ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    return optimizer
