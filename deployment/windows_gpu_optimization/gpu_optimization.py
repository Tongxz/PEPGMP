# Windows GPUä¼˜åŒ–é…ç½®
# åœ¨main.pyå¼€å¤´æ·»åŠ ä»¥ä¸‹ä»£ç 

import os

import torch


def setup_windows_gpu_optimization():
    """è®¾ç½®Windows GPUä¼˜åŒ–"""
    print("ğŸš€ å¯ç”¨Windows GPUä¼˜åŒ–...")

    # ç¯å¢ƒå˜é‡è®¾ç½®
    os.environ.update(
        {
            "CUDA_LAUNCH_BLOCKING": "0",
            "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:512,roundup_power2_divisions:16",
            "CUBLAS_WORKSPACE_CONFIG": ":16:8",
            "CUDA_MODULE_LOADING": "LAZY",
            "TORCH_CUDNN_V8_API_ENABLED": "1",
        }
    )

    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.device_count()}ä¸ªGPU")

        # PyTorchä¼˜åŒ–è®¾ç½®
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

        # æ˜¾å­˜ä¼˜åŒ–
        torch.cuda.empty_cache()

        # æ··åˆç²¾åº¦è®¾ç½®
        if hasattr(torch.backends.cudnn, "benchmark"):
            torch.backends.cudnn.benchmark = True

        print("âœ… GPUä¼˜åŒ–è®¾ç½®å®Œæˆ")

        # æ˜¾ç¤ºGPUä¿¡æ¯
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            memory_gb = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"  GPU {i}: {gpu_name} ({memory_gb:.1f}GB)")

    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é©±åŠ¨å’ŒCUDAå®‰è£…")


# åœ¨ç¨‹åºå¼€å§‹æ—¶è°ƒç”¨
setup_windows_gpu_optimization()
