#!/usr/bin/env python3
"""
æµ‹è¯•ä¸åŒç¡¬ä»¶ç¯å¢ƒä¸‹çš„è‡ªé€‚åº”ä¼˜åŒ–æ•ˆæœ
"""

import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils.adaptive_optimizer import AdaptiveOptimizer


def simulate_hardware_environments():
    """æ¨¡æ‹Ÿä¸åŒç¡¬ä»¶ç¯å¢ƒä¸‹çš„ä¼˜åŒ–é…ç½®"""

    # æ¨¡æ‹Ÿä¸åŒç¡¬ä»¶ç¯å¢ƒ
    test_environments = [
        {
            "name": "RTX 4090 (æ——èˆ°)",
            "env": {
                "has_cuda": True,
                "gpu_name": "NVIDIA GeForce RTX 4090",
                "vram_gb": 24.0,
                "cpu_cores": 32,
            },
        },
        {
            "name": "RTX 3070 (é«˜ç«¯)",
            "env": {
                "has_cuda": True,
                "gpu_name": "NVIDIA GeForce RTX 3070",
                "vram_gb": 8.0,
                "cpu_cores": 16,
            },
        },
        {
            "name": "GTX 1660 (ä¸­ç«¯)",
            "env": {
                "has_cuda": True,
                "gpu_name": "NVIDIA GeForce GTX 1660",
                "vram_gb": 6.0,
                "cpu_cores": 8,
            },
        },
        {
            "name": "GTX 1050 (å…¥é—¨)",
            "env": {
                "has_cuda": True,
                "gpu_name": "NVIDIA GeForce GTX 1050",
                "vram_gb": 2.0,
                "cpu_cores": 4,
            },
        },
        {
            "name": "CPU Only (æ— GPU)",
            "env": {"has_cuda": False, "gpu_name": "", "vram_gb": 0, "cpu_cores": 8},
        },
    ]

    print("=== ç¡¬ä»¶è‡ªé€‚åº”ä¼˜åŒ–æµ‹è¯• ===\n")

    for test_env in test_environments:
        print(f"ğŸ–¥ï¸  {test_env['name']}")
        print("-" * 50)

        # åˆ›å»ºä¼˜åŒ–å™¨å¹¶æ¨¡æ‹Ÿç¯å¢ƒ
        optimizer = AdaptiveOptimizer()
        optimizer.env = test_env["env"]  # æ¨¡æ‹Ÿç¡¬ä»¶ç¯å¢ƒ

        # è·å–ä¼˜åŒ–é…ç½®
        tier = optimizer.detect_hardware_tier()
        config = optimizer.get_optimization_config()

        print(f"ç¡¬ä»¶æ¡£ä½: {tier}")
        print(f"æ‰¹å¤§å°: {config['batch_size']}")
        print(f"è¾“å…¥å°ºå¯¸: {config['imgsz']}")
        print(f"æ··åˆç²¾åº¦: {config['enable_amp']}")
        print(f"TensorRT: {config['enable_tensorrt']}")
        print(f"æ¨èäººä½“æ¨¡å‹: {config['model_recommendations']['human_model']}")
        print(f"æ¨èå§¿æ€æ¨¡å‹: {config['model_recommendations']['pose_model']}")
        print(f"åŸå› : {config['model_recommendations']['reason']}")

        # è®¡ç®—é¢„æœŸæ€§èƒ½æå‡
        performance_prediction = predict_performance(config, test_env["env"])
        print(f"é¢„æœŸFPS: {performance_prediction['expected_fps']}")
        print(f"å†…å­˜ä½¿ç”¨: {performance_prediction['memory_usage']}MB")
        print(f"é€‚ç”¨åœºæ™¯: {performance_prediction['use_case']}")

        print()


def predict_performance(config, env):
    """é¢„æµ‹æ€§èƒ½è¡¨ç°"""
    base_fps = 25  # åŸºå‡†FPS

    # æ ¹æ®ç¡¬ä»¶æ¡£ä½è°ƒæ•´FPS
    tier_multipliers = {
        "flagship_gpu": 6.0,  # RTX 4090 ç­‰
        "high_end_gpu": 3.5,  # RTX 3070 ç­‰
        "mid_range_gpu": 2.0,  # GTX 1660 ç­‰
        "entry_gpu": 1.2,  # GTX 1050 ç­‰
        "cpu_optimized": 0.3,  # CPU only
    }

    tier = config["hardware_tier"]
    expected_fps = base_fps * tier_multipliers.get(tier, 1.0)

    # æ ¹æ®æ‰¹å¤§å°è°ƒæ•´ï¼ˆæ‰¹å¤„ç†æ•ˆç‡ï¼‰
    batch_efficiency = 1.0 + (config["batch_size"] - 1) * 0.15
    expected_fps *= batch_efficiency

    # å†…å­˜ä½¿ç”¨ä¼°ç®—
    base_memory = 200  # åŸºç¡€å†…å­˜ä½¿ç”¨
    batch_memory = config["batch_size"] * 100
    model_memory = {
        "yolov8n.pt": 50,
        "yolov8s.pt": 100,
        "yolov8m.pt": 200,
        "yolov8l.pt": 400,
    }

    human_model = config["model_recommendations"]["human_model"]
    memory_usage = base_memory + batch_memory + model_memory.get(human_model, 100)

    # ä½¿ç”¨åœºæ™¯æ¨è
    if expected_fps >= 60:
        use_case = "å®æ—¶é«˜ç²¾åº¦æ£€æµ‹ã€4Kè§†é¢‘å¤„ç†"
    elif expected_fps >= 30:
        use_case = "å®æ—¶æ£€æµ‹ã€1080pè§†é¢‘å¤„ç†"
    elif expected_fps >= 15:
        use_case = "å‡†å®æ—¶æ£€æµ‹ã€è§†é¢‘æ–‡ä»¶å¤„ç†"
    else:
        use_case = "ç¦»çº¿æ‰¹å¤„ç†ã€å•å¼ å›¾ç‰‡æ£€æµ‹"

    return {
        "expected_fps": round(expected_fps, 1),
        "memory_usage": memory_usage,
        "use_case": use_case,
    }


def test_performance_scaling():
    """æµ‹è¯•æ€§èƒ½æ‰©å±•æ€§"""
    print("=== æ€§èƒ½æ‰©å±•æ€§æµ‹è¯• ===\n")

    # æ¨¡æ‹Ÿä¸åŒæ‰¹å¤§å°çš„å½±å“
    batch_sizes = [1, 2, 4, 8, 16, 32]

    print("RTX 4090 - ä¸åŒæ‰¹å¤§å°æ€§èƒ½é¢„æµ‹:")
    print("æ‰¹å¤§å°\té¢„æœŸFPS\tåŠ é€Ÿæ¯”\tå†…å­˜ä½¿ç”¨")
    print("-" * 40)

    for batch_size in batch_sizes:
        # æ¨¡æ‹Ÿé…ç½®
        config = {
            "hardware_tier": "flagship_gpu",
            "batch_size": batch_size,
            "model_recommendations": {"human_model": "yolov8s.pt"},
        }

        env = {"has_cuda": True, "vram_gb": 24.0}
        perf = predict_performance(config, env)

        speedup = perf["expected_fps"] / 25  # ç›¸å¯¹åŸºå‡†çš„åŠ é€Ÿæ¯”

        print(
            f"{batch_size}\t{perf['expected_fps']}\t{speedup:.1f}x\t{perf['memory_usage']}MB"
        )

        # æ£€æŸ¥å†…å­˜é™åˆ¶
        if perf["memory_usage"] > 20000:  # è¶…è¿‡20GBæ˜¾å­˜
            print(f"  âš ï¸  æ˜¾å­˜ä¸è¶³é£é™©")


if __name__ == "__main__":
    simulate_hardware_environments()
    print("\n" + "=" * 60 + "\n")
    test_performance_scaling()
