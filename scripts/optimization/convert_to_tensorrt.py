#!/usr/bin/env python
"""
TensorRTæ¨¡å‹è½¬æ¢è„šæœ¬

å°†é¡¹ç›®ä¸­çš„YOLOæ¨¡å‹è½¬æ¢ä¸ºTensorRTå¼•æ“ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚
"""

import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def check_tensorrt_installation() -> bool:
    """æ£€æŸ¥TensorRTæ˜¯å¦å·²å®‰è£…"""
    try:
        import tensorrt as trt

        logger.info(f"âœ… TensorRTå·²å®‰è£…ï¼Œç‰ˆæœ¬: {trt.__version__}")
        return True
    except ImportError:
        logger.error("âŒ TensorRTæœªå®‰è£…")
        logger.info("è¯·è¿è¡Œ: pip install nvidia-tensorrt")
        return False


def check_cuda_available() -> bool:
    """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨"""
    try:
        import torch

        if torch.cuda.is_available():
            logger.info(f"âœ… CUDAå¯ç”¨ï¼Œè®¾å¤‡: {torch.cuda.get_device_name(0)}")
            logger.info(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
            return True
        else:
            logger.error("âŒ CUDAä¸å¯ç”¨")
            return False
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥CUDAå¤±è´¥: {e}")
        return False


def convert_model_to_tensorrt(
    model_path: str,
    output_path: Optional[str] = None,
    imgsz: int = 640,
    precision: str = "fp16",
    workspace: int = 4,
    device: int = 0,
) -> bool:
    """
    å°†YOLOæ¨¡å‹è½¬æ¢ä¸ºTensorRTå¼•æ“

    Args:
        model_path: PyTorchæ¨¡å‹è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸è¾“å…¥è·¯å¾„ç›¸åŒï¼Œæ‰©å±•åä¸º.engineï¼‰
        imgsz: è¾“å…¥å›¾åƒå¤§å°
        precision: ç²¾åº¦ ('fp32', 'fp16', 'int8')
        workspace: å·¥ä½œç©ºé—´å¤§å°(GB)
        device: GPUè®¾å¤‡ç¼–å·

    Returns:
        è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    try:
        from ultralytics import YOLO

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = Path(model_path)
        if not model_path.exists():
            logger.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False

        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if output_path is None:
            output_path = model_path.with_suffix(".engine")
        else:
            output_path = Path(output_path)

        logger.info(f"å¼€å§‹è½¬æ¢æ¨¡å‹: {model_path}")
        logger.info(f"è¾“å‡ºè·¯å¾„: {output_path}")
        logger.info(f"ç²¾åº¦: {precision}, å›¾åƒå¤§å°: {imgsz}, å·¥ä½œç©ºé—´: {workspace}GB")

        # åŠ è½½æ¨¡å‹
        model = YOLO(str(model_path))

        # è®¾ç½®ç²¾åº¦æ ‡å¿—
        half = precision == "fp16"

        # å¯¼å‡ºä¸ºTensorRT
        model.export(
            format="engine",
            device=device,
            imgsz=imgsz,
            half=half,
            workspace=workspace,
            simplify=True,
            opset=12,
            dynamic=False,
            verbose=True,
        )

        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        if output_path.exists():
            size_mb = output_path.stat().st_size / (1024 * 1024)
            logger.info("âœ… æ¨¡å‹è½¬æ¢æˆåŠŸï¼")
            logger.info(f"   è¾“å‡ºæ–‡ä»¶: {output_path}")
            logger.info(f"   æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")
            return True
        else:
            logger.error(f"âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {output_path}")
            return False

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹è½¬æ¢å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


def convert_all_models(
    models: List[Dict[str, str]],
    imgsz: int = 640,
    precision: str = "fp16",
    workspace: int = 4,
    device: int = 0,
) -> Dict[str, bool]:
    """
    æ‰¹é‡è½¬æ¢æ‰€æœ‰æ¨¡å‹

    Args:
        models: æ¨¡å‹é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'name', 'path', 'output'
        imgsz: è¾“å…¥å›¾åƒå¤§å°
        precision: ç²¾åº¦
        workspace: å·¥ä½œç©ºé—´å¤§å°(GB)
        device: GPUè®¾å¤‡ç¼–å·

    Returns:
        è½¬æ¢ç»“æœå­—å…¸ {æ¨¡å‹å: æ˜¯å¦æˆåŠŸ}
    """
    results = {}

    logger.info(f"å¼€å§‹æ‰¹é‡è½¬æ¢ {len(models)} ä¸ªæ¨¡å‹...")
    logger.info(f"é…ç½®: ç²¾åº¦={precision}, å›¾åƒå¤§å°={imgsz}, å·¥ä½œç©ºé—´={workspace}GB")

    for i, model_info in enumerate(models, 1):
        name = model_info["name"]
        path = model_info["path"]
        output = model_info.get("output")

        logger.info(f"\n{'='*60}")
        logger.info(f"[{i}/{len(models)}] è½¬æ¢æ¨¡å‹: {name}")
        logger.info(f"{'='*60}")

        success = convert_model_to_tensorrt(
            model_path=path,
            output_path=output,
            imgsz=imgsz,
            precision=precision,
            workspace=workspace,
            device=device,
        )

        results[name] = success

        if success:
            logger.info(f"âœ… {name} è½¬æ¢æˆåŠŸ")
        else:
            logger.error(f"âŒ {name} è½¬æ¢å¤±è´¥")

    return results


def print_summary(results: Dict[str, bool]):
    """æ‰“å°è½¬æ¢ç»“æœæ‘˜è¦"""
    logger.info(f"\n{'='*60}")
    logger.info("è½¬æ¢ç»“æœæ‘˜è¦")
    logger.info(f"{'='*60}")

    success_count = sum(1 for success in results.values() if success)
    total_count = len(results)

    for name, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        logger.info(f"{name}: {status}")

    logger.info(f"\næ€»è®¡: {success_count}/{total_count} ä¸ªæ¨¡å‹è½¬æ¢æˆåŠŸ")

    if success_count == total_count:
        logger.info("ğŸ‰ æ‰€æœ‰æ¨¡å‹è½¬æ¢æˆåŠŸï¼")
    else:
        logger.warning(f"âš ï¸  {total_count - success_count} ä¸ªæ¨¡å‹è½¬æ¢å¤±è´¥")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("TensorRTæ¨¡å‹è½¬æ¢å·¥å…·")
    logger.info("=" * 60)

    # æ£€æŸ¥ç¯å¢ƒ
    logger.info("\n1. æ£€æŸ¥ç¯å¢ƒ...")
    if not check_tensorrt_installation():
        logger.error("è¯·å…ˆå®‰è£…TensorRT")
        return 1

    if not check_cuda_available():
        logger.error("è¯·ç¡®ä¿CUDAå¯ç”¨")
        return 1

    # å®šä¹‰è¦è½¬æ¢çš„æ¨¡å‹
    logger.info("\n2. å‡†å¤‡è½¬æ¢æ¨¡å‹åˆ—è¡¨...")
    models = [
        {
            "name": "äººä½“æ£€æµ‹ (YOLOv8n)",
            "path": "models/yolo/yolov8n.pt",
            "output": "models/yolo/yolov8n.engine",
        },
        {
            "name": "å‘ç½‘æ£€æµ‹",
            "path": "models/hairnet_detection/hairnet_detection.pt",
            "output": "models/hairnet_detection/hairnet_detection.engine",
        },
        {
            "name": "å§¿æ€æ£€æµ‹ (YOLOv8n-pose)",
            "path": "models/yolo/yolov8n-pose.pt",
            "output": "models/yolo/yolov8n-pose.engine",
        },
    ]

    # æ‰“å°æ¨¡å‹åˆ—è¡¨
    logger.info("å°†è½¬æ¢ä»¥ä¸‹æ¨¡å‹:")
    for i, model in enumerate(models, 1):
        logger.info(f"  {i}. {model['name']}")
        logger.info(f"     è¾“å…¥: {model['path']}")
        logger.info(f"     è¾“å‡º: {model['output']}")

    # è½¬æ¢æ¨¡å‹
    logger.info("\n3. å¼€å§‹è½¬æ¢æ¨¡å‹...")
    results = convert_all_models(
        models=models,
        imgsz=640,
        precision="fp16",  # ä½¿ç”¨FP16ç²¾åº¦ä»¥è·å¾—æœ€ä½³æ€§èƒ½
        workspace=4,  # 4GBå·¥ä½œç©ºé—´
        device=0,  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    )

    # æ‰“å°æ‘˜è¦
    print_summary(results)

    # è¿”å›é€€å‡ºç 
    success_count = sum(1 for success in results.values() if success)
    return 0 if success_count == len(results) else 1


if __name__ == "__main__":
    sys.exit(main())
