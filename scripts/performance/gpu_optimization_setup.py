#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆçš„GPUæ€§èƒ½ä¼˜åŒ–è„šæœ¬
Generated GPU Performance Optimization Script
"""

import os
import torch

def setup_gpu_optimization():
    """è®¾ç½®GPUæ€§èƒ½ä¼˜åŒ–"""
    print("ğŸš€ å¯ç”¨GPUæ€§èƒ½ä¼˜åŒ–...")

    # PyTorchåç«¯ä¼˜åŒ–
    if torch.cuda.is_available():
        print("âœ… CUDAå¯ç”¨ï¼Œå¯ç”¨GPUä¼˜åŒ–")
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()

        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB")

    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("âœ… MPSå¯ç”¨ï¼Œå¯ç”¨macOS GPUä¼˜åŒ–")

    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå¯ç”¨CPUä¼˜åŒ–")
        torch.set_num_threads(32)
        os.environ['OMP_NUM_THREADS'] = '32'
        os.environ['MKL_NUM_THREADS'] = '32'

def get_optimized_config():
    """è·å–ä¼˜åŒ–é…ç½®"""
    return {
        "device_strategy": "auto",
        "mixed_precision": True,
        "compile_model": True,
        "batch_size": 16,
        "num_workers": 8,
        "pin_memory": True,
        "non_blocking": True,
        "optimization_level": "O2",
        "device": "cuda",
        "cudnn_benchmark": True
}

if __name__ == "__main__":
    setup_gpu_optimization()
    config = get_optimized_config()
    print("\nğŸ“Š ä¼˜åŒ–é…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
