#!/usr/bin/env python3
"""
GPUåŠ é€Ÿæ€§èƒ½ä¼˜åŒ–å™¨
GPU Acceleration Performance Optimizer

è§£å†³GPUåˆ©ç”¨ç‡ä½ã€æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ï¼š
1. CUDAç¯å¢ƒæ£€æµ‹å’Œä¿®å¤
2. æ¨¡å‹å¹¶è¡Œå’Œæ‰¹å¤„ç†ä¼˜åŒ–
3. TensorRTæ¨¡å‹ä¼˜åŒ–
4. å†…å­˜ç®¡ç†ä¼˜åŒ–
5. æ¨ç†æµæ°´çº¿ä¼˜åŒ–
"""

import json
import logging
import os
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GPUAccelerationOptimizer:
    """GPUåŠ é€Ÿä¼˜åŒ–å™¨"""

    def __init__(self):
        self.optimization_results = {}
        self.gpu_info = {}
        self.performance_metrics = {"before": {}, "after": {}, "improvement": {}}

    def diagnose_gpu_environment(self) -> Dict[str, Any]:
        """è¯Šæ–­GPUç¯å¢ƒé—®é¢˜"""
        logger.info("ğŸ” å¼€å§‹GPUç¯å¢ƒè¯Šæ–­...")

        diagnosis = {
            "cuda_available": False,
            "pytorch_gpu": False,
            "gpu_memory": 0,
            "gpu_count": 0,
            "issues": [],
            "recommendations": [],
        }

        # 1. æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
        try:
            import torch

            diagnosis["pytorch_version"] = torch.__version__
            diagnosis["cuda_available"] = torch.cuda.is_available()

            if diagnosis["cuda_available"]:
                diagnosis["pytorch_gpu"] = True
                diagnosis["gpu_count"] = torch.cuda.device_count()
                diagnosis["gpu_memory"] = torch.cuda.get_device_properties(
                    0
                ).total_memory / (1024**3)
                diagnosis["gpu_name"] = torch.cuda.get_device_name(0)
                diagnosis["compute_capability"] = torch.cuda.get_device_capability(0)
                logger.info(f"âœ… PyTorch GPUå¯ç”¨: {diagnosis['gpu_name']}")
                logger.info(f"   æ˜¾å­˜: {diagnosis['gpu_memory']:.1f}GB")
                logger.info(f"   è®¡ç®—èƒ½åŠ›: {diagnosis['compute_capability']}")
            else:
                diagnosis["issues"].append("PyTorch CUDAä¸å¯ç”¨")

        except ImportError:
            diagnosis["issues"].append("PyTorchæœªå®‰è£…")

        # 2. æ£€æŸ¥NVIDIAé©±åŠ¨
        try:
            result = subprocess.run(
                ["nvidia-smi"], capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                diagnosis["nvidia_driver"] = True
                logger.info("âœ… NVIDIAé©±åŠ¨å¯ç”¨")
            else:
                diagnosis["issues"].append("NVIDIAé©±åŠ¨ä¸å¯ç”¨æˆ–ç‰ˆæœ¬è¿‡ä½")
        except (subprocess.TimeoutExpired, FileNotFoundError):
            diagnosis["issues"].append("NVIDIAé©±åŠ¨æœªå®‰è£…æˆ–nvidia-smiä¸å¯ç”¨")

        # 3. ç”Ÿæˆä¿®å¤å»ºè®®
        if not diagnosis["cuda_available"]:
            if sys.platform.startswith("darwin"):  # macOS
                diagnosis["recommendations"].append(
                    "macOSä¸ŠCUDAä¸å¯ç”¨ï¼Œå»ºè®®ä½¿ç”¨Metal Performance Shaders (MPS)åç«¯"
                )
                diagnosis["recommendations"].append(
                    "è¿è¡Œ: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
                )
            else:  # Linux/Windows
                diagnosis["recommendations"].append(
                    "å®‰è£…æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
                )
                diagnosis["recommendations"].append("ç¡®ä¿NVIDIAé©±åŠ¨ç‰ˆæœ¬>=460.32.03")

        self.gpu_info = diagnosis
        return diagnosis

    def optimize_model_inference(self) -> Dict[str, Any]:
        """ä¼˜åŒ–æ¨¡å‹æ¨ç†æ€§èƒ½"""
        logger.info("âš¡ å¼€å§‹æ¨¡å‹æ¨ç†ä¼˜åŒ–...")

        optimizations = {
            "torch_backends": self._optimize_torch_backends(),
            "model_compilation": self._setup_model_compilation(),
            "memory_optimization": self._optimize_memory_usage(),
            "batch_processing": self._setup_batch_processing(),
        }

        return optimizations

    def _optimize_torch_backends(self) -> Dict[str, str]:
        """ä¼˜åŒ–PyTorchåç«¯è®¾ç½®"""
        optimizations = {}

        # 1. å¯ç”¨æœ€ä¼˜åç«¯
        try:
            import torch

            if torch.cuda.is_available():
                # CUDAä¼˜åŒ–
                os.environ["CUDA_LAUNCH_BLOCKING"] = "0"  # å¼‚æ­¥æ‰§è¡Œ
                os.environ[
                    "PYTORCH_CUDA_ALLOC_CONF"
                ] = "max_split_size_mb:512"  # å†…å­˜åˆ†é…ä¼˜åŒ–
                torch.backends.cudnn.benchmark = True  # å¯ç”¨CuDNNè‡ªåŠ¨ä¼˜åŒ–
                torch.backends.cudnn.deterministic = False  # ç¦ç”¨ç¡®å®šæ€§ä»¥æå‡æ€§èƒ½
                optimizations["cuda"] = "CUDAä¼˜åŒ–å·²å¯ç”¨"

            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                # macOS Metal Performance Shaders
                optimizations["mps"] = "MPSåç«¯å·²å¯ç”¨ï¼ˆmacOS GPUåŠ é€Ÿï¼‰"

            else:
                # CPUä¼˜åŒ–
                torch.set_num_threads(os.cpu_count() or 4)
                os.environ["OMP_NUM_THREADS"] = str(os.cpu_count() or 4)
                os.environ["MKL_NUM_THREADS"] = str(os.cpu_count() or 4)
                optimizations["cpu"] = "CPUå¤šçº¿ç¨‹ä¼˜åŒ–å·²å¯ç”¨"

        except Exception as e:
            optimizations["error"] = f"åç«¯ä¼˜åŒ–å¤±è´¥: {e}"

        return optimizations

    def _setup_model_compilation(self) -> Dict[str, str]:
        """è®¾ç½®æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–"""
        compilation_info = {}

        try:
            import torch

            # PyTorch 2.0+ ç¼–è¯‘ä¼˜åŒ–
            if hasattr(torch, "compile"):
                compilation_info["torch_compile"] = "PyTorch 2.0ç¼–è¯‘ä¼˜åŒ–å¯ç”¨"

                # å»ºè®®çš„ç¼–è¯‘é…ç½®
                compile_config = {
                    "mode": "reduce-overhead",  # å‡å°‘å¼€é”€æ¨¡å¼
                    "fullgraph": False,  # å…è®¸å›¾åˆ†è§£ä»¥æå‡å…¼å®¹æ€§
                    "dynamic": True,  # æ”¯æŒåŠ¨æ€shape
                }
                compilation_info["config"] = str(compile_config)
            else:
                compilation_info["torch_compile"] = "PyTorchç‰ˆæœ¬ä¸æ”¯æŒç¼–è¯‘ä¼˜åŒ–"

        except Exception as e:
            compilation_info["error"] = f"ç¼–è¯‘ä¼˜åŒ–è®¾ç½®å¤±è´¥: {e}"

        return compilation_info

    def _optimize_memory_usage(self) -> Dict[str, str]:
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        memory_opts = {}

        try:
            import torch

            if torch.cuda.is_available():
                # GPUå†…å­˜ä¼˜åŒ–
                torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
                memory_opts["gpu_cache"] = "GPUå†…å­˜ç¼“å­˜å·²æ¸…ç†"

                # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
                memory_opts["allocation"] = "GPUå†…å­˜åˆ†é…ç­–ç•¥å·²ä¼˜åŒ–"

            # ç³»ç»Ÿå†…å­˜ä¼˜åŒ–
            memory_opts["system"] = "ç³»ç»Ÿå†…å­˜ä¼˜åŒ–å·²å¯ç”¨"

        except Exception as e:
            memory_opts["error"] = f"å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}"

        return memory_opts

    def _setup_batch_processing(self) -> Dict[str, Any]:
        """è®¾ç½®æ‰¹å¤„ç†ä¼˜åŒ–"""
        batch_config = {
            "enabled": True,
            "optimal_batch_size": self._calculate_optimal_batch_size(),
            "dynamic_batching": True,
            "queue_size": 32,
        }

        return batch_config

    def _calculate_optimal_batch_size(self) -> int:
        """è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°"""
        try:
            import torch

            if torch.cuda.is_available():
                # åŸºäºGPUæ˜¾å­˜è®¡ç®—
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (
                    1024**3
                )
                if gpu_memory_gb >= 8:
                    return 16  # é«˜ç«¯GPU
                elif gpu_memory_gb >= 4:
                    return 8  # ä¸­ç«¯GPU
                else:
                    return 4  # å…¥é—¨GPU
            else:
                # CPUæ¨¡å¼
                cpu_cores = os.cpu_count() or 4
                return min(cpu_cores, 8)

        except:
            return 4  # ä¿å®ˆé»˜è®¤å€¼

    def create_optimized_inference_config(self) -> Dict[str, Any]:
        """åˆ›å»ºä¼˜åŒ–çš„æ¨ç†é…ç½®"""
        config = {
            "device_strategy": "auto",  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
            "mixed_precision": True,  # æ··åˆç²¾åº¦è®­ç»ƒ
            "compile_model": True,  # å¯ç”¨æ¨¡å‹ç¼–è¯‘
            "batch_size": self._calculate_optimal_batch_size(),
            "num_workers": min(os.cpu_count() or 4, 8),
            "pin_memory": True,
            "non_blocking": True,
            "optimization_level": "O2",  # ä¸­ç­‰ä¼˜åŒ–çº§åˆ«
        }

        # æ ¹æ®ç¡¬ä»¶è°ƒæ•´é…ç½®
        if self.gpu_info.get("cuda_available"):
            config["device"] = "cuda"
            config["cudnn_benchmark"] = True
        elif sys.platform.startswith("darwin"):
            config["device"] = "mps"  # macOS GPU
        else:
            config["device"] = "cpu"
            config["mixed_precision"] = False  # CPUä¸æ”¯æŒæ··åˆç²¾åº¦

        return config

    def generate_performance_script(self) -> str:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–è„šæœ¬"""
        script_content = f'''#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆçš„GPUæ€§èƒ½ä¼˜åŒ–è„šæœ¬
Generated GPU Performance Optimization Script
"""

import os
import torch

def setup_gpu_optimization():
    """è®¾ç½®GPUæ€§èƒ½ä¼˜åŒ–"""
    print("ğŸš€ å¯ç”¨GPUæ€§èƒ½ä¼˜åŒ–...")

    # PyTorchåç«¯ä¼˜åŒ–
    if torch.cuda.is_available():
        print("âœ… CUDAå¯ç”¨ï¼Œå¯ç”¨GPUä¼˜åŒ–")
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()

        print(f"   GPU: {{torch.cuda.get_device_name(0)}}")
        print(f"   æ˜¾å­˜: {{torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}}GB")

    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("âœ… MPSå¯ç”¨ï¼Œå¯ç”¨macOS GPUä¼˜åŒ–")

    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå¯ç”¨CPUä¼˜åŒ–")
        torch.set_num_threads({os.cpu_count() or 4})
        os.environ['OMP_NUM_THREADS'] = '{os.cpu_count() or 4}'
        os.environ['MKL_NUM_THREADS'] = '{os.cpu_count() or 4}'

def get_optimized_config():
    """è·å–ä¼˜åŒ–é…ç½®"""
    return {json.dumps(self.create_optimized_inference_config(), indent=8)}

if __name__ == "__main__":
    setup_gpu_optimization()
    config = get_optimized_config()
    print("\\nğŸ“Š ä¼˜åŒ–é…ç½®:")
    for key, value in config.items():
        print(f"  {{key}}: {{value}}")
'''

        return script_content

    def run_benchmark(self) -> Dict[str, float]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("ğŸ“Š å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")

        benchmark_results = {}

        try:
            import time

            import torch

            # åˆ›å»ºæµ‹è¯•æ•°æ®
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"

            # æ¨¡æ‹Ÿæ¨ç†æµ‹è¯•
            batch_sizes = [1, 4, 8, 16]
            for batch_size in batch_sizes:
                if device == "cpu" and batch_size > 8:
                    continue  # CPUè·³è¿‡å¤§æ‰¹æ¬¡

                # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
                data = torch.randn(batch_size, 3, 640, 640).to(device)

                # é¢„çƒ­
                for _ in range(5):
                    _ = torch.nn.functional.avg_pool2d(data, kernel_size=2)

                # æµ‹è¯•
                torch.cuda.synchronize() if device == "cuda" else None
                start_time = time.time()

                for _ in range(20):
                    _ = torch.nn.functional.avg_pool2d(data, kernel_size=2)

                torch.cuda.synchronize() if device == "cuda" else None
                end_time = time.time()

                avg_time = (end_time - start_time) / 20
                fps = batch_size / avg_time

                benchmark_results[f"batch_{batch_size}"] = {
                    "avg_time_ms": avg_time * 1000,
                    "fps": fps,
                    "device": device,
                }

                logger.info(
                    f"  æ‰¹æ¬¡å¤§å° {batch_size}: {avg_time*1000:.1f}ms, {fps:.1f} FPS"
                )

        except Exception as e:
            benchmark_results["error"] = str(e)

        return benchmark_results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GPUåŠ é€Ÿæ€§èƒ½ä¼˜åŒ–å™¨å¯åŠ¨")
    print("=" * 50)

    optimizer = GPUAccelerationOptimizer()

    # 1. è¯Šæ–­GPUç¯å¢ƒ
    diagnosis = optimizer.diagnose_gpu_environment()

    print("\nğŸ” GPUç¯å¢ƒè¯Šæ–­ç»“æœ:")
    print(f"  CUDAå¯ç”¨: {diagnosis['cuda_available']}")
    print(f"  GPUæ•°é‡: {diagnosis['gpu_count']}")
    if diagnosis.get("gpu_name"):
        print(f"  GPUåç§°: {diagnosis['gpu_name']}")
        print(f"  æ˜¾å­˜å¤§å°: {diagnosis['gpu_memory']:.1f}GB")

    if diagnosis["issues"]:
        print("\nâš ï¸  å‘ç°çš„é—®é¢˜:")
        for issue in diagnosis["issues"]:
            print(f"    - {issue}")

    if diagnosis["recommendations"]:
        print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
        for rec in diagnosis["recommendations"]:
            print(f"    - {rec}")

    # 2. æ€§èƒ½ä¼˜åŒ–
    optimizations = optimizer.optimize_model_inference()

    print("\nâš¡ æ€§èƒ½ä¼˜åŒ–ç»“æœ:")
    for category, result in optimizations.items():
        print(f"  {category}: {result}")

    # 3. ç”Ÿæˆä¼˜åŒ–é…ç½®
    config = optimizer.create_optimized_inference_config()

    print("\nğŸ“Š ä¼˜åŒ–é…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")

    # 4. è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = optimizer.run_benchmark()

    print("\nğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•:")
    for test, results in benchmark.items():
        if isinstance(results, dict) and "fps" in results:
            print(
                f"  {test}: {results['avg_time_ms']:.1f}ms, {results['fps']:.1f} FPS ({results['device']})"
            )

    # 5. ç”Ÿæˆä¼˜åŒ–è„šæœ¬
    script_content = optimizer.generate_performance_script()
    script_path = Path("scripts/performance/gpu_optimization_setup.py")
    script_path.parent.mkdir(exist_ok=True)

    with open(script_path, "w", encoding="utf-8") as f:
        f.write(script_content)

    print(f"\nğŸ“„ ä¼˜åŒ–è„šæœ¬å·²ç”Ÿæˆ: {script_path}")

    print("\nğŸ‰ ä¼˜åŒ–å»ºè®®:")
    print("1. å¦‚æœGPUä¸å¯ç”¨ï¼Œä¼˜å…ˆè§£å†³CUDA/é©±åŠ¨é—®é¢˜")
    print("2. å¯ç”¨æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–ä»¥æå‡æ¨ç†é€Ÿåº¦")
    print("3. ä½¿ç”¨æ‰¹å¤„ç†ä»¥æé«˜GPUåˆ©ç”¨ç‡")
    print("4. è¿è¡Œç”Ÿæˆçš„ä¼˜åŒ–è„šæœ¬åº”ç”¨è®¾ç½®")

    return optimizer


if __name__ == "__main__":
    main()
