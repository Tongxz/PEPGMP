#!/usr/bin/env python3
"""
å‰ç«¯æ„å»ºä¼˜åŒ–å·¥å…·
Frontend Build Optimizer

ç”¨äºä¼˜åŒ–å‰ç«¯æ„å»ºè¿‡ç¨‹ï¼ŒåŒ…æ‹¬èµ„æºå‹ç¼©ã€åˆå¹¶ã€ç¼“å­˜ç­‰
"""

import gzip
import json
import logging
import mimetypes
import shutil
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


class FrontendBuildOptimizer:
    """å‰ç«¯æ„å»ºä¼˜åŒ–å™¨"""

    def __init__(
        self, frontend_dir: str = "frontend", output_dir: str = "frontend/dist"
    ):
        self.frontend_dir = Path(frontend_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ”¯æŒå‹ç¼©çš„æ–‡ä»¶ç±»å‹
        self.compressible_types = {
            "text/html",
            "text/css",
            "text/javascript",
            "application/javascript",
            "application/json",
            "text/plain",
            "text/xml",
            "application/xml",
        }

        # æ–‡ä»¶æ‰©å±•åæ˜ å°„
        self.extension_map = {
            ".html": "text/html",
            ".css": "text/css",
            ".js": "application/javascript",
            ".json": "application/json",
            ".txt": "text/plain",
            ".xml": "application/xml",
        }

    def optimize_html_files(self) -> Dict[str, Any]:
        """ä¼˜åŒ–HTMLæ–‡ä»¶"""
        logger.info("å¼€å§‹ä¼˜åŒ–HTMLæ–‡ä»¶...")

        html_files = list(self.frontend_dir.glob("*.html"))
        optimization_results = []

        for html_file in html_files:
            logger.info(f"ä¼˜åŒ–HTMLæ–‡ä»¶: {html_file.name}")

            # è¯»å–åŸå§‹æ–‡ä»¶
            with open(html_file, "r", encoding="utf-8") as f:
                content = f.read()

            # æ‰§è¡Œä¼˜åŒ–
            optimized_content = self._optimize_html_content(content)

            # ä¿å­˜ä¼˜åŒ–åçš„æ–‡ä»¶
            output_file = self.output_dir / html_file.name
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(optimized_content)

            # å‹ç¼©æ–‡ä»¶
            compressed_file = self._compress_file(output_file)

            # è®°å½•ä¼˜åŒ–ç»“æœ
            original_size = html_file.stat().st_size
            optimized_size = output_file.stat().st_size
            compressed_size = (
                compressed_file.stat().st_size if compressed_file else optimized_size
            )

            optimization_results.append(
                {
                    "file": html_file.name,
                    "original_size": original_size,
                    "optimized_size": optimized_size,
                    "compressed_size": compressed_size,
                    "compression_ratio": (1 - compressed_size / original_size) * 100
                    if original_size > 0
                    else 0,
                }
            )

        return {"total_files": len(html_files), "results": optimization_results}

    def optimize_css_files(self) -> Dict[str, Any]:
        """ä¼˜åŒ–CSSæ–‡ä»¶"""
        logger.info("å¼€å§‹ä¼˜åŒ–CSSæ–‡ä»¶...")

        css_files = list(self.frontend_dir.glob("*.css"))
        optimization_results = []

        for css_file in css_files:
            logger.info(f"ä¼˜åŒ–CSSæ–‡ä»¶: {css_file.name}")

            # è¯»å–åŸå§‹æ–‡ä»¶
            with open(css_file, "r", encoding="utf-8") as f:
                content = f.read()

            # æ‰§è¡ŒCSSä¼˜åŒ–
            optimized_content = self._optimize_css_content(content)

            # ä¿å­˜ä¼˜åŒ–åçš„æ–‡ä»¶
            output_file = self.output_dir / css_file.name
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(optimized_content)

            # å‹ç¼©æ–‡ä»¶
            compressed_file = self._compress_file(output_file)

            # è®°å½•ä¼˜åŒ–ç»“æœ
            original_size = css_file.stat().st_size
            optimized_size = output_file.stat().st_size
            compressed_size = (
                compressed_file.stat().st_size if compressed_file else optimized_size
            )

            optimization_results.append(
                {
                    "file": css_file.name,
                    "original_size": original_size,
                    "optimized_size": optimized_size,
                    "compressed_size": compressed_size,
                    "compression_ratio": (1 - compressed_size / original_size) * 100
                    if original_size > 0
                    else 0,
                }
            )

        return {"total_files": len(css_files), "results": optimization_results}

    def optimize_js_files(self) -> Dict[str, Any]:
        """ä¼˜åŒ–JavaScriptæ–‡ä»¶"""
        logger.info("å¼€å§‹ä¼˜åŒ–JavaScriptæ–‡ä»¶...")

        js_files = list(self.frontend_dir.glob("*.js"))
        optimization_results = []

        for js_file in js_files:
            logger.info(f"ä¼˜åŒ–JavaScriptæ–‡ä»¶: {js_file.name}")

            # è¯»å–åŸå§‹æ–‡ä»¶
            with open(js_file, "r", encoding="utf-8") as f:
                content = f.read()

            # æ‰§è¡ŒJavaScriptä¼˜åŒ–
            optimized_content = self._optimize_js_content(content)

            # ä¿å­˜ä¼˜åŒ–åçš„æ–‡ä»¶
            output_file = self.output_dir / js_file.name
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(optimized_content)

            # å‹ç¼©æ–‡ä»¶
            compressed_file = self._compress_file(output_file)

            # è®°å½•ä¼˜åŒ–ç»“æœ
            original_size = js_file.stat().st_size
            optimized_size = output_file.stat().st_size
            compressed_size = (
                compressed_file.stat().st_size if compressed_file else optimized_size
            )

            optimization_results.append(
                {
                    "file": js_file.name,
                    "original_size": original_size,
                    "optimized_size": optimized_size,
                    "compressed_size": compressed_size,
                    "compression_ratio": (1 - compressed_size / original_size) * 100
                    if original_size > 0
                    else 0,
                }
            )

        return {"total_files": len(js_files), "results": optimization_results}

    def _optimize_html_content(self, content: str) -> str:
        """ä¼˜åŒ–HTMLå†…å®¹"""
        import re

        # ç§»é™¤HTMLæ³¨é‡Šï¼ˆä¿ç•™æ¡ä»¶æ³¨é‡Šï¼‰
        content = re.sub(r"<!--(?!\[if|\s*\[if).*?-->", "", content, flags=re.DOTALL)

        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r"\s+", " ", content)

        # ç§»é™¤æ ‡ç­¾é—´çš„ç©ºç™½
        content = re.sub(r">\s+<", "><", content)

        # ä¼˜åŒ–scriptæ ‡ç­¾
        content = self._optimize_script_tags(content)

        # ä¼˜åŒ–linkæ ‡ç­¾
        content = self._optimize_link_tags(content)

        return content.strip()

    def _optimize_css_content(self, content: str) -> str:
        """ä¼˜åŒ–CSSå†…å®¹"""
        import re

        # ç§»é™¤CSSæ³¨é‡Š
        content = re.sub(r"/\*.*?\*/", "", content, flags=re.DOTALL)

        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r"\s+", " ", content)

        # ç§»é™¤åˆ†å·å‰çš„ç©ºç™½
        content = re.sub(r"\s*;\s*", ";", content)

        # ç§»é™¤å†’å·å‰åçš„ç©ºç™½
        content = re.sub(r"\s*:\s*", ":", content)

        # ç§»é™¤å¤§æ‹¬å·å‰åçš„ç©ºç™½
        content = re.sub(r"\s*{\s*", "{", content)
        content = re.sub(r"\s*}\s*", "}", content)

        # ç§»é™¤é€—å·åçš„ç©ºç™½
        content = re.sub(r",\s*", ",", content)

        # ç§»é™¤æœ«å°¾åˆ†å·
        content = re.sub(r";}", "}", content)

        return content.strip()

    def _optimize_js_content(self, content: str) -> str:
        """ä¼˜åŒ–JavaScriptå†…å®¹"""
        import re

        # ç§»é™¤å•è¡Œæ³¨é‡Š
        content = re.sub(r"//.*$", "", content, flags=re.MULTILINE)

        # ç§»é™¤å¤šè¡Œæ³¨é‡Š
        content = re.sub(r"/\*.*?\*/", "", content, flags=re.DOTALL)

        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r"\s+", " ", content)

        # ç§»é™¤åˆ†å·å‰çš„ç©ºç™½
        content = re.sub(r"\s*;\s*", ";", content)

        # ç§»é™¤é€—å·åçš„ç©ºç™½
        content = re.sub(r",\s*", ",", content)

        # ç§»é™¤å¤§æ‹¬å·å‰åçš„ç©ºç™½
        content = re.sub(r"\s*{\s*", "{", content)
        content = re.sub(r"\s*}\s*", "}", content)

        return content.strip()

    def _optimize_script_tags(self, content: str) -> str:
        """ä¼˜åŒ–scriptæ ‡ç­¾"""
        import re

        # ä¸ºæ²¡æœ‰async/deferçš„scriptæ ‡ç­¾æ·»åŠ defer
        def add_defer(match):
            script_content = match.group(0)
            if "async" not in script_content and "defer" not in script_content:
                script_content = script_content.replace("<script", "<script defer")
            return script_content

        content = re.sub(r"<script[^>]*>", add_defer, content)

        return content

    def _optimize_link_tags(self, content: str) -> str:
        """ä¼˜åŒ–linkæ ‡ç­¾"""
        import re

        # ä¸ºCSSé“¾æ¥æ·»åŠ preload
        def add_preload(match):
            link_content = match.group(0)
            if 'rel="stylesheet"' in link_content and "preload" not in link_content:
                # è¿™é‡Œå¯ä»¥æ·»åŠ preloadé€»è¾‘ï¼Œä½†éœ€è¦å°å¿ƒå¤„ç†
                pass
            return link_content

        content = re.sub(r"<link[^>]*>", add_preload, content)

        return content

    def _compress_file(self, file_path: Path) -> Optional[Path]:
        """å‹ç¼©æ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æ”¯æŒå‹ç¼©
            mime_type, _ = mimetypes.guess_type(str(file_path))
            if mime_type not in self.compressible_types:
                return None

            # åˆ›å»ºå‹ç¼©æ–‡ä»¶
            compressed_path = file_path.with_suffix(file_path.suffix + ".gz")

            with open(file_path, "rb") as f_in:
                with gzip.open(compressed_path, "wb") as f_out:
                    shutil.copyfileobj(f_in, f_out)

            logger.info(f"æ–‡ä»¶å·²å‹ç¼©: {compressed_path}")
            return compressed_path

        except Exception as e:
            logger.error(f"å‹ç¼©æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def generate_cache_manifest(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¼“å­˜æ¸…å•"""
        logger.info("ç”Ÿæˆç¼“å­˜æ¸…å•...")

        # æ”¶é›†æ‰€æœ‰é™æ€èµ„æº
        static_files = []

        # HTMLæ–‡ä»¶
        for html_file in self.frontend_dir.glob("*.html"):
            static_files.append(
                {
                    "url": f"/{html_file.name}",
                    "type": "html",
                    "size": html_file.stat().st_size,
                }
            )

        # CSSæ–‡ä»¶
        for css_file in self.frontend_dir.glob("*.css"):
            static_files.append(
                {
                    "url": f"/{css_file.name}",
                    "type": "css",
                    "size": css_file.stat().st_size,
                }
            )

        # JavaScriptæ–‡ä»¶
        for js_file in self.frontend_dir.glob("*.js"):
            static_files.append(
                {
                    "url": f"/{js_file.name}",
                    "type": "js",
                    "size": js_file.stat().st_size,
                }
            )

        # ç”Ÿæˆç¼“å­˜ç­–ç•¥
        cache_strategies = {
            "html": {"cache_control": "no-cache", "etag": True, "last_modified": True},
            "css": {
                "cache_control": "max-age=31536000",
                "etag": True,
                "last_modified": True,
            },
            "js": {
                "cache_control": "max-age=31536000",
                "etag": True,
                "last_modified": True,
            },
        }

        manifest = {
            "version": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "files": static_files,
            "cache_strategies": cache_strategies,
            "total_size": sum(f["size"] for f in static_files),
        }

        # ä¿å­˜æ¸…å•æ–‡ä»¶
        manifest_file = self.output_dir / "cache_manifest.json"
        with open(manifest_file, "w", encoding="utf-8") as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)

        logger.info(f"ç¼“å­˜æ¸…å•å·²ä¿å­˜åˆ°: {manifest_file}")
        return manifest

    def generate_service_worker(self) -> str:
        """ç”ŸæˆService Worker"""
        logger.info("ç”ŸæˆService Worker...")

        sw_content = """
// Service Worker for Frontend Performance Optimization
const CACHE_NAME = 'frontend-cache-v1';
const STATIC_CACHE = 'static-cache-v1';
const DYNAMIC_CACHE = 'dynamic-cache-v1';

// éœ€è¦ç¼“å­˜çš„é™æ€èµ„æº
const STATIC_ASSETS = [
    '/',
    '/index.html',
    '/common.css',
    '/theme.css',
    '/nav.css',
    '/app.js',
    '/nav.js'
];

// å®‰è£…äº‹ä»¶
self.addEventListener('install', event => {
    console.log('Service Worker installing...');
    event.waitUntil(
        caches.open(STATIC_CACHE)
            .then(cache => {
                console.log('Caching static assets...');
                return cache.addAll(STATIC_ASSETS);
            })
            .then(() => self.skipWaiting())
    );
});

// æ¿€æ´»äº‹ä»¶
self.addEventListener('activate', event => {
    console.log('Service Worker activating...');
    event.waitUntil(
        caches.keys().then(cacheNames => {
            return Promise.all(
                cacheNames.map(cacheName => {
                    if (cacheName !== STATIC_CACHE && cacheName !== DYNAMIC_CACHE) {
                        console.log('Deleting old cache:', cacheName);
                        return caches.delete(cacheName);
                    }
                })
            );
        }).then(() => self.clients.claim())
    );
});

// æ‹¦æˆªè¯·æ±‚
self.addEventListener('fetch', event => {
    const { request } = event;
    const url = new URL(request.url);

    // åªå¤„ç†åŒæºè¯·æ±‚
    if (url.origin !== location.origin) {
        return;
    }

    event.respondWith(
        caches.match(request)
            .then(response => {
                // å¦‚æœç¼“å­˜ä¸­æœ‰ï¼Œç›´æ¥è¿”å›
                if (response) {
                    return response;
                }

                // å¦åˆ™å‘èµ·ç½‘ç»œè¯·æ±‚
                return fetch(request)
                    .then(fetchResponse => {
                        // æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
                        if (!fetchResponse || fetchResponse.status !== 200 || fetchResponse.type !== 'basic') {
                            return fetchResponse;
                        }

                        // å…‹éš†å“åº”
                        const responseToCache = fetchResponse.clone();

                        // æ ¹æ®èµ„æºç±»å‹å†³å®šç¼“å­˜ç­–ç•¥
                        if (request.url.match(/\\.(css|js|png|jpg|jpeg|gif|svg)$/)) {
                            // é™æ€èµ„æºç¼“å­˜
                            caches.open(STATIC_CACHE)
                                .then(cache => {
                                    cache.put(request, responseToCache);
                                });
                        } else if (request.url.match(/\\.(html)$/)) {
                            // HTMLæ–‡ä»¶ç¼“å­˜
                            caches.open(DYNAMIC_CACHE)
                                .then(cache => {
                                    cache.put(request, responseToCache);
                                });
                        }

                        return fetchResponse;
                    })
                    .catch(() => {
                        // ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œå°è¯•è¿”å›ç¼“å­˜
                        if (request.destination === 'document') {
                            return caches.match('/index.html');
                        }
                    });
            })
    );
});

// æ¶ˆæ¯å¤„ç†
self.addEventListener('message', event => {
    if (event.data && event.data.type === 'SKIP_WAITING') {
        self.skipWaiting();
    }
});
"""

        # ä¿å­˜Service Workeræ–‡ä»¶
        sw_file = self.output_dir / "sw.js"
        with open(sw_file, "w", encoding="utf-8") as f:
            f.write(sw_content.strip())

        logger.info(f"Service Workerå·²ä¿å­˜åˆ°: {sw_file}")
        return str(sw_file)

    def generate_build_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ„å»ºæŠ¥å‘Š"""
        logger.info("ç”Ÿæˆæ„å»ºä¼˜åŒ–æŠ¥å‘Š...")

        # æ‰§è¡Œå„ç§ä¼˜åŒ–
        html_results = self.optimize_html_files()
        css_results = self.optimize_css_files()
        js_results = self.optimize_js_files()

        # ç”Ÿæˆç¼“å­˜æ¸…å•
        cache_manifest = self.generate_cache_manifest()

        # ç”ŸæˆService Worker
        sw_file = self.generate_service_worker()

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_original_size = 0
        total_optimized_size = 0
        total_compressed_size = 0

        for result in (
            html_results["results"] + css_results["results"] + js_results["results"]
        ):
            total_original_size += result["original_size"]
            total_optimized_size += result["optimized_size"]
            total_compressed_size += result["compressed_size"]

        overall_compression_ratio = (
            (1 - total_compressed_size / total_original_size) * 100
            if total_original_size > 0
            else 0
        )

        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_files": html_results["total_files"]
                + css_results["total_files"]
                + js_results["total_files"],
                "html_files": html_results["total_files"],
                "css_files": css_results["total_files"],
                "js_files": js_results["total_files"],
                "total_original_size": total_original_size,
                "total_optimized_size": total_optimized_size,
                "total_compressed_size": total_compressed_size,
                "overall_compression_ratio": overall_compression_ratio,
            },
            "html_optimization": html_results,
            "css_optimization": css_results,
            "js_optimization": js_results,
            "cache_manifest": cache_manifest,
            "service_worker": sw_file,
            "recommendations": self._generate_build_recommendations(
                overall_compression_ratio
            ),
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "build_report.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"æ„å»ºæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return report

    def _generate_build_recommendations(self, compression_ratio: float) -> List[str]:
        """ç”Ÿæˆæ„å»ºå»ºè®®"""
        recommendations = []

        if compression_ratio < 30:
            recommendations.append("å‹ç¼©ç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰æœªä¼˜åŒ–çš„èµ„æº")

        if compression_ratio > 70:
            recommendations.append("å‹ç¼©æ•ˆæœè‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘å¯ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–")

        recommendations.extend(
            [
                "å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ç”¨Gzipå‹ç¼©",
                "è€ƒè™‘ä½¿ç”¨CDNåŠ é€Ÿé™æ€èµ„æº",
                "å®šæœŸæ›´æ–°Service Workerç¼“å­˜ç­–ç•¥",
                "ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡å’Œæ€§èƒ½æŒ‡æ ‡",
            ]
        )

        return recommendations


def main():
    """ä¸»å‡½æ•°"""
    optimizer = FrontendBuildOptimizer()
    report = optimizer.generate_build_report()

    print("\n" + "=" * 60)
    print("å‰ç«¯æ„å»ºä¼˜åŒ–æŠ¥å‘Š")
    print("=" * 60)

    summary = report["summary"]
    print(f"\nğŸ“Š æ„å»ºæ‘˜è¦:")
    print(f"  æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
    print(f"  HTMLæ–‡ä»¶: {summary['html_files']}")
    print(f"  CSSæ–‡ä»¶: {summary['css_files']}")
    print(f"  JavaScriptæ–‡ä»¶: {summary['js_files']}")

    print(f"\nğŸ“¦ æ–‡ä»¶å¤§å°:")
    print(f"  åŸå§‹å¤§å°: {summary['total_original_size']:,} å­—èŠ‚")
    print(f"  ä¼˜åŒ–åå¤§å°: {summary['total_optimized_size']:,} å­—èŠ‚")
    print(f"  å‹ç¼©åå¤§å°: {summary['total_compressed_size']:,} å­—èŠ‚")
    print(f"  æ€»ä½“å‹ç¼©ç‡: {summary['overall_compression_ratio']:.1f}%")

    print(f"\nğŸš€ ä¼˜åŒ–å»ºè®®:")
    for recommendation in report["recommendations"]:
        print(f"  â€¢ {recommendation}")

    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: frontend/dist/build_report.json")
    print(f"ğŸ“„ ç¼“å­˜æ¸…å•å·²ä¿å­˜åˆ°: frontend/dist/cache_manifest.json")
    print(f"ğŸ“„ Service Workerå·²ä¿å­˜åˆ°: frontend/dist/sw.js")


if __name__ == "__main__":
    main()
